---
title: 3. Best Practices for Data Storage 
format: html
date-modified: last-modified
date-format: long
date: 2023-11-30
summary: In this lesson we discuss about how to organize your files and follow some naming recommendations.
---

:::{.callout-note title="Course Overview"}

⏰ **Time Estimation:** X minutes  
💬 **Learning Objectives:**    
    
1. Organize NGS data and external resources efficiently
2. Apply naming conventions for files and folders
3. Define rules for naming results and figures accurately
:::

So far, we have covered how to adhere to FAIR and Open Science standards, which primarily focus on data sharing post-project completion. However, effective data management is essential while actively working on the project. Organizing data folders, raw and processed data, analysis scripts and pipelines, and results ensures long-term project success. Without a clear structure, future access and understanding of data become challenging, even more for collaborators, leading to potential chaos down the line. 

:::{.callout-exercise}
- Have you ever had trouble finding data, results, figures, or specific scripts?
- Do you maintain the same structure across different projects?
- Have you ever discussed this topic with collaborators?
:::

# File structure and naming conventions
On the other hand, applying a consistent file structure and naming conventions to your files will help you to efficiently manage your data. Consider the following practices:

- **Folder structure**: establish a logical and intuitive folder structure that mirrors the organization of research projects and experimental data. Employ descriptive folder names for easy identification and access to specific data files.
    - **Subfolders**: enhance the organization using subfolders to further categorize data based on their contents, such as workflows, scripts, results, reports, etc.
- **File naming conventions**: implement a standardized file naming convention to maintain consistency and clarity. Use descriptive and informative names (e.g., specify data type: plots, results tables, etc.)

In this lesson we will see a practical example on how you could organize your own files and folders.

## Folder organization

Here we suggest the use of three main folders:

1. **Shared project data folders**:
- This shared directory is designated for storing unprocessed sequencing data files, with each subfolder representing a separate project.
- Each project folder contains raw data, corresponding metadata, and optionally pre-processed data like quality control reports and processed data.
    - Include the pipeline or workflow used for data processing, along with a metadata file.
- Data in these folders should be locked and set to **read-only** to prevent unauthorized ("unwanted") modifications.
2. **Individual project folders**:
- This directory typically belongs to the researcher conducting bioinformatics analyses and encompasses all essential files for a specific research project (data, scripts, software, workflows, results, etc.). 
- A project may utilize data from various assays or results obtained from other projects. It's important to avoid duplicating datasets; instead, link it from the original source to maintain data integrity and avoid redundancy.
3. **Resources and databases folders**: 
- This (commonly) shared directory contains common repositories or curated databases that facilitate research (genomics, clinical data, imaging data, and more!). For instance, in genomics, it includes genome references (fasta files), annotations (gtf files) for different species, and indexes for various alignment algorithms. 
- Each folder corresponds to a unique reference or database version, allowing for multiple references from the same organism or different species.
    - Ensure each contains the version of the reference and a metadata file. 
    - More subfolders can be created for different data formats.

:::{.callout-tip title="Verify the integrity of downloaded files!"}
Ensure that the person downloading the files employs checksums or cryptographic hash functions to verify the integrity and ascertain that files are neither corrupted nor tampered with.

- **MD5 Checksum**: Files with names ending in ".md5" contain MD5 checksums. For instance, "filename.txt.md5" holds the MD5 checksum of "filename.txt"."
:::

:::{.callout-definition}
# Database
A database is a structured repository for storing, managing, and retrieving information, forming the cornerstone of efficient data organization. 
:::

:::{.callout-tip title="Create shortcuts to public datasets and assays!"}
The use of symbolic links, also referred as softlinks, is a key practice in large labs where data might used for different purposes and by multiple people. 

- They act as pointers, containing the path to the location of the target files/directories. 
- They avoid duplication and they are flexible and lightweight (do not occupy much disk space). 
- They simplify directory structures.  
- Extra use case: create symbolic links to executable files and libraries!
:::

:::{.callout-exercise}
# Exercise: create a softlink link
Open your terminal and create a softlink using the following command. The first path is the target (directory or file) and the second one where the symbolic link will be created. 
```{.bash}
ln -s path/to/dataset/<ASSAY_ID> /path/to/user/<PROJECT_ID>/data/
```
Now, access the target file/directory through the symbolic link:

```{.bash}
ls /path/to/user/<PROJECT_ID>/data/
```
:::{.callout-hint}
Follow this example if need extra guidance (change paths!): 

1. Create the target/original file
```{.bash}
echo "This is the content of the original file." > /home/users/Documents/original_file.txt
```
2. Create the symbolic link
```{.bash}
ln -s /home/users/Documents/original_file.txt /home/users/Desktop/original_file.txt
```
3. Verify the symbolic link 
```{.bash}
ls -s /home/users/Desktop/original_file.txt
```
4. Access the file through the symbolic link:
```{.bash}
cat /home/users/Desktop/original_file.txt
```
The last command will display the contents of the original file. 
:::
:::

## 1. Navigating Shared Project Data

Let's focus on the shared folders containing experimental datasets generated in-house.

### Naming Shared Folders Effectively

Create a folder for all your NGS experiments, for instance named `Assay`. Each subfolder, denoted by a unique `Assay-ID`, should be named clearly and comprehensibly. Assay-ID comprises raw files, processed files, and the pipeline used to generate them. Raw files should remain unchanged, while modifications to processed files should be restricted post-preprocessing (e.g., after quality control) to prevent unintended alterations. Check the exercise for efficient naming of Assay-ID:

:::{.callout-exercise}
# Exercise: name your `Assay-ID`
- How would you ensure its name is unique and understood at a glance?

:::{.callout-hint}
Use an acronym (1) that describes type of NGS assay (RNAseq, ChIPseq, ATACseq) a keyword (2) that represents a unique element to that assay, and the date (3). 
```{.bash}
<Assay-ID>_<keyword>_YYYYMMDD
```
:::
:::

:::{.callout-note title="Other Assay ID code names" collapse="true"}

- `CHIP`: ChIP-seq
- `RNA`: RNA-seq
- `ATAC`: ATAC-seq
- `SCR`: scRNA-seq
- `PROT`: Mass Spectrometry Assay
- `CAT`: Cut&Tag
- `CAR`: Cut&Run
- `RIME`: Rapid Immunoprecipitation Mass spectrometry of Endogenous proteins
- ...
:::

Keep in mind that these folders might be (re)used in different individual projects over many years. 

### Optimizing Folder Structures

The provided folder structure is designed to be intuitive for NGS data. The description and metadata files aid in understanding the project's origin and structure, crucial for archiving and manuscript preparation. There is a section dedicated to databases in [lesson 4](./04_metadata.qmd). Let's explore the example and its folder contents:

```{.bash}
<data_type>_<keyword>_YYYYMMDD/
├── README.md 
├── CHECKSUMS
├── pipeline.md
├── processed
    ├── fastqc/
    ├── multiqc/
    ├── final_fastq/
└── raw
    ├── .fastq.gz 
    └── samplesheet.csv
```

- **README.md**: This file contains general information about the project or experiment, usually in markdown or plain text format. It includes details such as such as the origin of the raw NGS data (including sample information, laboratory protocols used, and the assay's objectives). Sometimes, it also outlines the basic directory structure and file naming conventions.
- **metadata.yml**: This serves as the metadata file for the project ([see this lesson](./04_metadata.qmd)).
- **pipeline.md**: This document describes the pipeline employed to process the raw data, along with the specific commands used to execute the pipeline. The specific format can very depending on the workflow system employed (e.g., bash, Snakemake, Nextflow, Jupyter Notebooks etc.) ([see this lesson](./06_pipelines.qmd)). Employing a standardized pipeline ensures a consistent file organization system (and the corresponding documentation)
- **processed_data**: folder with results of the preprocessing pipeline. The contents may vary depending on the pipeline utilized. For example, 
    -  **fastqc**: quality Control results of the raw fastq files.
    - **multiqc**: aggregated quality control results across all samples 
    - final_fastq: cleaned and processed files 
- **raw_data**: folder with the raw data.
    - *.fastq.gz* or other file formats (depending on the field or the experiment)
    - *samplesheet.csv*: metadata information for the samples. It may contain additional columns that will facilitate downstream analysis. This file is key if are planning to use nf-core pipelines.

## 2. Navigating Project folder

In the `Projects` folder, usually private to the individual performing the data analysis, each project has its own subfolder containing project information, data analysis scripts and pipelines, and results. It's advisable to maintain folders for individual projects, separate from shared data folders, as project-specific files typically aren't reused across multiple projects, and more than one dataset might be needed to answer a specific scientific question.

### Naming Project Folders Effectively
The Project folder  should have a unique, easily readable, distinguishable, and instantly understandable name.For instance, consider naming it using the main author's initials, a descriptive keyword, and the date:

```{.bash}
<author_initials>_<keyword>_YYYYMMDD
```

### Optimizing Folder Structures

Next, let's take a look at a possible folder structure and what kind of files you can find there.

```{.bash}
<author_initials>_<keyword>_YYYYMMDD
├── data (symbolic link)
│  └── raw
│  └── processed
│  └── external (third party resources)
├── docs
│  └── project_template.docx
├── notebooks or pipelines/
├── README.md
├── logs
├── tmp
├── environment
│  └── requirements.txt or environment.yml
├── scripts/src
├── reports
│  └── figures/
│  └── <file_name>.html
├── results
│  └── tables/
│  └── figures/
└── metadata.yml
```

- **data**: contains symlinks or shortcuts to where the data is (raw, processed, external, etc.), avoiding duplication and modification of original files.
- **docs**: folder containing word documents, slides or pdfs related to the project. It also contains your [Data Management Plan](./02_DMP.qmd).
- **notebooks or pipelines**: folder containing notebooks (Jupyter, R markdown, Quarto notebooks) or workflows (Snakemake or Nextflow) with the actual data analysis. Tip: labeled them numerically indicating the sequential order.
- **README.md**: detailed description of the project in markdown format.
- logs: log files.
- tmp: store temporary or intermediate files.
- **environment**: files for reproducing the analysis environment to reproduce the results, such as a Dockerfile, conda yaml file, or a text file ([See 6th lesson](./06_pipelines.qmd) for more tips on making your pipelines reproducible). It includes software, libraries/packages and dependencies (and their versions!). 
- **scripts**: folder containing helper scripts to run data analysis or source code
- **reports**: Generated analysis as HTML, PDF, LaTeX, etc. Great for sharing with colleagues and creating formal reports of the data analysis procedure.
    - *figures*: figures produced upon rendering notebooks. Tip: save the figures under a subfolder named after the notebook/pipeline that created them (you will appreciate this organization when you need to rerun analysis and know which script created each figure!). 
- **results**: results from the data analysis, such as tables and figures, etc. Tip: Create a subfolder named after the notebook or pipeline for storing the results generated by that specific notebook or pipeline.
- **metadata.yml**: metadata file describing the dataset, samples, etc. ([see this lesson](./04_metadata.qmd)).

:::{.callout-exercise}
# Exercise: Write your personal data structure
- Create your own data structure for one of the projects you are currently working on. Consider how it is similar to the example provided and how it differs. Make sure the data structure is easily understandable and navigable.
- What improvements or modifications could be made to enhance clarity and efficiency?
:::

## Template engine
Setting up folder structures manually for each new project can be time-consuming. Thankfully, tools like [Cookiecutter](https://github.com/cookiecutter/cookiecutter) offer a solution by allowing users to create project templates easily. These templates can ensure consistency across projects and save time. Additionally, using [cruft](https://github.com/cruft/cruft) alongside Cookiecutter can assist in maintaining older templates when updates are made (by synchronizing them with the latest version).

:::{.callout-note title="Cookiecutter templates"}
- Coockiecutter template for [Data science projects](https://github.com/drivendata/cookiecutter-data-science)
- Brickmanlab template for [NGS data](https://github.com/brickmanlab/ngs-template): similar to the folder structures in the examples above. You can download and modify it to suit your needs. 
:::

### Quick tutorial on cookiecutter
:::{.callout-caution title="Sandbox Tutorial"} 
**Learn how to create your own template [here](./practical_workshop.qmd).**

We offer workshops on practical RDM for NGS data. Keep an eye on the upcoming events on the [Sandbox website](https://hds-sandbox.github.io/news/news.html).
:::


## 3. Resources and databases folder 
 
Health databases are utilized for storing, organizing, and providing access to diverse health-related data, including genomic data, clinical records, imaging data, and more. These resources are regularly updated and released under different versions from various sources. To ensure data reproducibility, it's crucial to manage and specify the versions and sources of data within these databases. 

:::{.callout-note title="Example NGS: genomic resources" collapse="true"}
For example, preprocessing NGS data involves utilizing various genomic resources for tasks like aligning and annotating fastq files. Essential resources include reference genomes in FASTA format (e.g., human and mouse), indexed fasta files for alignment tools like STAR and Bowtie, and GTF or GFF files for quantifying reads into genomic regions. One of the latest human reference genome is GRCh38, however many past studies are based on GRCh37. 

How can you keep track of your resources? 
Name the folder using the version, or use a reference genome manager such as [refgenie](http://refgenie.databio.org/en/latest/). 

#### Refgenie
It manages storage, access, and transfer of reference genome resources. It provides command-line and Python interfaces to download pre-built reference genome "assets", like indexes used by bioinformatics tools. It can also build assets for custom genome assemblies. Refgenie provides programmatic access to a standard genome folder structure, so software can swap from one genome to another. Check this [tutorial](http://refgenie.databio.org/en/latest/tutorial/) to get started. 
:::

#### Manual download 

Best practices for downloading data from the source while ensuring the preservation of information about the version and other metadata includes:

- Organizing data structure: Create data structure that allows storing all versions in the same parent directory, and ensure that all lab members follow these practices.
- Documentation and metadata preservation: Before downloading, carefully review the documentation provided by the database. Download files containing the data version and any associated metadata.
- README.md: Record the version of the data in the README.md file.
- Checksums: Check for and use checksums provided by the database to verify the integrity of the downloaded data, ensuring that it hasn't been corrupted during transfer. Do the exercise below. 
- Verify File size: Check the file size provided by the source. It is not as secure as checksum verification but discrepancy could indicate corruption.
- Automated Processes: whenever possible, automate the download process to reduce the likelihood of errors and ensure consistency (e.g. use bash script or pipeline).

:::{.callout-note title="Optional: Exercise on CHECKSUMS" collapse="true"}
We recommend the use of md5sum to verify data integrity, specially if you are downloading large datasets. In this example, we use data from the [HLA FTP Directory](ftp://ftp.ebi.ac.uk/pub/databases/ipd/imgt/hla/). 

1. Install md5sum (from coreutils package)
```{.bash} 
#!/bin/bash
# On Ubuntu/Debian
apt-get install coreutils
# On macOS
brew install coreutils
```
2. Create a bash script to download the target files (named "dw_resources.sh" in the data structure).
```{.bash .code-overflow-wrap} 
#!/bin/bash
# Important: go through the README before downloading! Check if a checksums file is included. 

# 1. Create or change directory to the resources dir. 

# Check for checksums (e.g.: md5checksum.txt), download and modify it so that it only contains the checksums of the target files. The file will look like:
1a3d12e4e6cc089388d88e3509e41cb3  hla_gen.fasta
# Finally, save it: 
md5file="md5checksum.txt"

# Define the URL of the files to download
url="ftp://ftp.ebi.ac.uk/pub/databases/ipd/imgt/hla/hla_gen.fasta"
# 
filename=$(basename "$url")

# (Optional) Define a different filename to save the downloaded file (`wget -O $out_filename`)
# out_filename = "imgt_hla_gen.fasta"

# Download the file
wget $url && \
md5sum --status --check $md5file

```
3. Folder structure 
```{.bash}
genomic_resources/
├── specie1/
│  └── version/
│     ├── files.txt
│     └── indexes/
└── dw_resources.sh
```
4. Create an md5sum file and share it with collaborators before sharing the data. This allows others to check the integrity of the files. 

```{.bash}
md5sum <data>
```

:::{.callout-exercise}
Download a file using md5sums. Choose a file from your favorite dataset or select one from the HLA database (for quick testing, consider using a text file such as `Nomenclature_2009.txt`).
:::
:::

## Naming conventions

Consistent naming conventions play a crucial role in scientific research by enhancing organization and data retrieval. By adopting standardized naming conventions, researchers ensure that files, experiments, or datasets are labeled logically, facilitating easy location and comparison of similar data. For instance, in fields like genomics, uniform naming conventions for files associated with particular experiments or samples allows for swift identification and comparison of relevant data, streamlining the research process and contributing to the reproducibility of findings. Overall, promotes efficiency, collaboration, and the integrity of scientific work.

:::{.callout-tip title="General tips for file and folder naming"}

Remember to keep the folder structure simple.

- Keep it short and meaningful (use understandable abbreviation only, e.g., Cor for correlations or LFC for Log Fold Change)
- Consider including one of these elements: project name, category, descriptor, content, author...
    - Author-based: use initials
- Use alphanumeric characters: letters (A-Z) and numbers (0-9)
- Avoid special characters: ~!@#$%^&*()`[]{}"|
- **Date-based format**: use `YYYYMMDD` format (year/month/day format helps with sorting and listing files in chronological order)
- Use **underscores and hyphens** as delimiters and **avoid spaces**.
    - Not all search tools may work well with spaces (messy to indicate paths)
    - If the length is a concern, use capital letter to delimit words [camelCase](https://en.wikipedia.org/wiki/Camel_case).
- **Sequential numbering**: Use a two-‑digit format for single-digit numbers (0–9) to ensure correct numerical sequence order (for example, 01 and not 1)
- **Version control**: Indicate the version ("V") or revision ("R") as the last element, using the two--digit format (e.g., v01, v02)
- Write down your naming convention pattern and document it in the README file
:::

:::{.callout-exercise}
# Define your file name conventions
Avoid long and complicated names and ensure your file names are both informative and easy to manage: 

1. For saving a new plot, heatmap representing sample correlations 
2. When naming the file for the document containing the Research Data Management Course Objectives (Version 2, 2nd May 2024) from the University of Copenhagen
3. Consider the most common file types you work with, such as visualizations, tables, etc., and create logical and clear file names

:::{.callout-hint}
1. `heatmap_sampleCor_20240101.png`
2. `KU_RDM-objectives_20240502_v02.doc` or `KU_RDMObj_20240502_v02.doc`
:::


:::

<details>
  <summary><span style="color:#4b4646;font-weight:bold;font-size:x-large">Additional file naming conventions</summary>
  <p>

```{r}
#| echo: false
# Libraries
quiet <- function(x) { suppressMessages(suppressWarnings(x)) }
quiet(library(gt))
quiet(library(tidyverse))

# Load the data from CSV
data <- read_tsv("./assets/file_naming_convention.tsv",show_col_types = FALSE)

# Print the data as a table
data %>% gt() |> tab_options(table.font.size = 11)
```

  </p>
</details>

## Wrap up

In this lesson, we have learned some practical tips and examples about how to organize your data and bring some order to chaos! Complete the practical tutorial on using `cookiecutter` as a template engine to be able to create your own templates and reuse them as much as you need. 

### Sources

- UK Data Service: <https://ukdataservice.ac.uk/learning-hub/research-data-management/format-your-data/organising/>
- Oakland University: <https://library.oakland.edu/services/research-data/file-org.html>
- Cessda guidelines: <https://dmeg.cessda.eu/Data-Management-Expert-Guide/2.-Organise-Document/File-naming-and-folder-structure>.
- RDMkit Elixir Europe: <https://rdmkit.elixir-europe.org/data_organisation>