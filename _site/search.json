[
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "Alba Refoyo Martinez\n\n\n\n\n\nData Scientist, Copenhagen University\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJos√© Alejandro Romero Herrera\n\n\n\n\n\nPrincipal Bioinformatician, Lundbeck\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cards/AlbaMartinez.html",
    "href": "cards/AlbaMartinez.html",
    "title": "Alba Refoyo Martinez",
    "section": "",
    "text": "Alba is a Sandbox data scientist based at the University of Copenhagen. During her academic background as a PhD and Postdoc she has developed a solid expertise in large-scale genomics and pipelines development on computing clusters."
  },
  {
    "objectID": "develop/keywords.html",
    "href": "develop/keywords.html",
    "title": "Computational Research Data Management",
    "section": "",
    "text": "Keywords\nHere‚Äôs a lit of used keywords:\n[TAGS]\n\n\n\n\nCopyrightCC-BY-SA 4.0 license"
  },
  {
    "objectID": "develop/03_data_life_cycle.html",
    "href": "develop/03_data_life_cycle.html",
    "title": "Data Life Cycle",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n1. Understand what the Data Life Cycle is\n2. Understand what each of the phases of the Data Life Cycle.\n\n\nThe data life cycle is a comprehensive and systematic approach that encompasses the entire journey of research data, from its inception through its collection, processing, analysis, sharing, and preservation. Like a living organism, data goes through different stages in its life cycle, each with specific considerations and requirements. Understanding and managing the data life cycle is crucial for researchers to ensure data integrity, accessibility, and long-term usability. By following the data life cycle, researchers can effectively organize, document, and share their data, fostering transparency, reproducibility, and collaboration in scientific research.\n\n\nUCPH describes the data life cycle in 6 phases:\n\nPlan: In the planning stage, researchers define the objectives of their data collection and analysis, identify data requirements, and develop a data management plan. This plan outlines how data will be collected, stored, and shared, as well as ethical and legal considerations.\nCollect and Document: In this stage, researchers gather the data according to their plan. They document important details about the data, such as its source, collection methods, and any modifications made during data acquisition. Proper documentation ensures data quality and facilitates later use.\nProcess and Analyse: Data is processed and analyzed to draw meaningful insights and conclusions. Researchers use various methods, tools, and algorithms to extract valuable information from the collected data. During this phase, data is transformed, cleaned, and transformed into usable formats.\nStore and Secure: Storing data securely is essential to protect it from loss, unauthorized access, and corruption. Researchers must choose appropriate storage solutions and implement data security measures to safeguard sensitive or confidential information.\nShare: Sharing data is an important aspect of the data life cycle. Researchers are encouraged to share their data openly whenever possible, adhering to Open Science and FAIR principles. Data sharing fosters collaboration, increases the visibility of research, and enables data reuse by others.\nPreserve: Data preservation ensures that valuable research data is available and usable in the long term. Researchers should deposit data in trusted data repositories or archives, ensuring its ongoing accessibility and future use by the scientific community.\n\n\n\n\nResearch Data Life Cycle, University of Copenhagen RDM guidelines.\n\n\nLet‚Äôs take a look at each of the phases:\n\n\nIn the planning stage, researchers define the objectives of their data collection and analysis, identify data requirements, and develop a data management plan. This plan outlines how data will be collected, stored, and shared, as well as ethical and legal considerations.\nThe management of research data must be thoroughly considered before physical materials (samples, model organisms, reagents or media, etc) and digital data (bioinformatics data) are collected, observed, generated, created or reused. Data management plans (DMP) must be developed and documented, preferably in electronic format. DMPs should be updated when significant changes to the management of research data occur and (references to) the DMP should be stored with the corresponding research data as long as they exist.\n\n\n\n\n\n\nWarning\n\n\n\nThis is a reminder that this course focuses only on bioinformatics digital data, not physical samples or materials.\n\n\nThe DMP should be discussed with project collaborators, research managers and supervisors (if any), ensuring that agreements are reached regarding responsibilities for different research data management activities during and after research projects.\n\n\n\n\n\n\nTip\n\n\n\nWe will check about how to write a DMP for NGS data in the 5th lesson.\n\n\n\n\n\nIn this stage, researchers gather the data according to their plan. They document important details about the data, such as its source, collection methods, and any modifications made during data acquisition. Proper documentation ensures data quality and facilitates later use.\nResearch data should be collected and processed in line with best practice in the research discipline. For example, there will be important differences between a metagenomics project and a clinical trial involving human samples! Research projects should be documented in a way that allows them to be repeated by others. Among other things, this includes clearly and accurately describing project methodology and any equipment, software or code used. This includes workflows for data preprocessing and how you will structure and organize your files.\n\n\n\n\n\n\nTip\n\n\n\nWe will see about how to structure your files and folders in the 6th lesson.\n\n\nIn addition, research data should be described using appropriate metadata to facilitate searching for, the identification of, and the interpretation of the research data. Metadata should be linked to the research data as long as they exist, unless legislation or agreements state otherwise.\n\n\n\n\n\n\nTip\n\n\n\nWe will see about what kind of metadata you can use for your NGS data in the 7th lesson\n\n\n\n\n\nThis, in the data life cycle for NGS data, is a critical phase that involves transforming raw sequencing data into meaningful biological insights. During this stage, researchers apply computational methods and bioinformatics tools to extract valuable information from the vast amount of sequencing data generated in NGS experiments.\nThroughout this phase, researchers should adhere to good coding practices, ensuring well-documented and reproducible analyses. Code notebooks and version control tools, such as Git, help maintain transparency and facilitate the sharing of methods and results with the scientific community.\nTo streamline and standardize the data analysis process, researchers often employ workflows and pipelines. Workflows automate the execution of multiple analysis steps, enhancing efficiency and consistency while promoting reproducibility. The NGS research community benefits from collaborative efforts, such as the nf-core community, which provides pre-established and validated Nextflow-based pipelines for various NGS applications. Leveraging community-developed pipelines ensures adherence to best practices and accelerates the pace of research through shared expertise.\n\n\n\n\n\n\nTip\n\n\n\nWe will see more about version control in lesson 9\n\n\n\n\n\nStoring data securely is essential to protect it from loss, unauthorized access, and corruption. Researchers must choose appropriate storage solutions and implement data security measures to safeguard sensitive or confidential information.\nResearch data must be classified at the start of a research project on the basis of the level of sensitivity and the impact to the University if data are disclosed, altered or destroyed without authorisation. Risks to data security and of data loss should be assessed in relation to the data classification. This includes evaluating:\n\nPhysical and digital access to research data\nRisks associated with data management procedures\nBackup requirements and backup procedures\nExternal and internal threats to data confidentiality, integrity and accessibility\nFinancial, regulatory and technical consequences of working with data, data storage and data preservation\n\n\n\n\n\n\n\nWarning\n\n\n\nThis step is very specific to the setup used in your environment. Maybe you are using a department server with storage, or you use a personal drive, or cloud solutions such as AWS. Thus, we cannot include in this course a comprehensive guideline on this matter.\n\n\n\n\n\n\n\n\nAbout GPDR and protected data\n\n\n\nThis course does not talk in detail about protected NGS data, such as human or patient samples. For more information about GPDR protected data, check out our course!\n\n\n\n\n\nSharing data is an important aspect of the data life cycle. Researchers are encouraged to share their data openly whenever possible, adhering to Open Science and FAIR principles. Data sharing fosters collaboration, increases the visibility of research, and enables data reuse by others.\nLegislation or agreements may preclude research data sharing or impose conditions for sharing. Before sharing research data, the relevant approvals need to be obtained and, if necessary, the appropriate agreements set up to allow data and material sharing.\nBy default, research data should be made openly available after project end, as a minimum for data sets underlying research publications. Concerns relating to intellectual property rights, personal data protection, information security as well as commercial and national interests and legislation must be taken into account in accordance with the principle of ‚Äòas open as possible, as closed as necessary‚Äô. If the research data cannot be made available, sharing the metadata associated with the research data should be considered. The FAIR principles (for findable, accessible, interoperable and reusable research objects) should be followed as much as possible when preparing digital data sets that can be shared. This includes as a minimum:\n\nProviding open access to data (Open Data) by depositing data in a data repository, or by providing access to information on whether, when, how, and to what extent data can be accessed if data sets cannot be made openly available.\nAs much as possible using persistent identifiers (PID) and metadata (such as descriptive keywords) that help locate the data set.\nCommunicating terms and conditions for data reuse by others, for example by attaching a data licence.\nProviding the information necessary to understand how data sets were created and structured, and for what purpose.\n\n\n\n\n\n\n\nTip\n\n\n\nWe will talk about FAIR and OS principles in the next lesson\n\n\n\n\n\nData preservation ensures that valuable research data is available and usable in the long term. Researchers should deposit data in trusted data repositories or archives, ensuring its ongoing accessibility and future use by the scientific community.\nAppropriate arrangements for the long-term preservation of digital data, physical material and associated metadata must be made, adhering to legislation and/or agreements. This should include:\n\nDeciding which research data will be preserved. As a minimum, data sets underlying published research results must be preserved so that any objections or criticisms can be addressed.\nDeciding how long research data will be preserved. Data sets underlying research publications should be retained for at least five years after project completion or date of publication, whichever comes last.\nChoosing a format and location in which research data should be preserved, and deciding what metadata should be associated with the preserved data and material.\nDeleting/destructing research data if legislation or agreements exclude preservation, or when researchers and their managers determine that preservation is not required (for example when research data can easily be reproduced) or not possible (for example when research data are too costly to store or when material quality will deteriorate over time).\nAssigning a person, persons or role(s) responsible for the research data after project end. Responsibilities include safeguarding the long-term integrity of data sets.\nDetermining rights, for example, of access to and use of preserved data sets.\n\nIn addition, your institution may oblige you to preserve and keep a copy of your data at their premises. They will usually explain any requirement at their own data management department. For example, the UCPH mandates that a copy of data sets and associated metadata must remain at UCPH after project end and/or when employment with the University ceases, in a way in which they are accessible to research managers and understandable for research managers and peers, unless legislation or agreements determine otherwise.\n\n\n\n\n\n\nTip\n\n\n\nWe will check about which repositories you can use to preserve your NGS data in the 10th lesson\n\n\n\n\n\n\nIn this lesson we have learned about the Research Data Life Cycle, from the conception of a project and collection of data until the end of the project and preservation of the data. These concepts were taken from the UCPH data management guidelines and put them in the context of NGS, but are perfectly usable in other universities and applications.",
    "crumbs": [
      "Course material",
      "Data Life Cycle"
    ]
  },
  {
    "objectID": "develop/03_data_life_cycle.html#data-life-cycle-phases",
    "href": "develop/03_data_life_cycle.html#data-life-cycle-phases",
    "title": "Data Life Cycle",
    "section": "",
    "text": "UCPH describes the data life cycle in 6 phases:\n\nPlan: In the planning stage, researchers define the objectives of their data collection and analysis, identify data requirements, and develop a data management plan. This plan outlines how data will be collected, stored, and shared, as well as ethical and legal considerations.\nCollect and Document: In this stage, researchers gather the data according to their plan. They document important details about the data, such as its source, collection methods, and any modifications made during data acquisition. Proper documentation ensures data quality and facilitates later use.\nProcess and Analyse: Data is processed and analyzed to draw meaningful insights and conclusions. Researchers use various methods, tools, and algorithms to extract valuable information from the collected data. During this phase, data is transformed, cleaned, and transformed into usable formats.\nStore and Secure: Storing data securely is essential to protect it from loss, unauthorized access, and corruption. Researchers must choose appropriate storage solutions and implement data security measures to safeguard sensitive or confidential information.\nShare: Sharing data is an important aspect of the data life cycle. Researchers are encouraged to share their data openly whenever possible, adhering to Open Science and FAIR principles. Data sharing fosters collaboration, increases the visibility of research, and enables data reuse by others.\nPreserve: Data preservation ensures that valuable research data is available and usable in the long term. Researchers should deposit data in trusted data repositories or archives, ensuring its ongoing accessibility and future use by the scientific community.\n\n\n\n\nResearch Data Life Cycle, University of Copenhagen RDM guidelines.\n\n\nLet‚Äôs take a look at each of the phases:\n\n\nIn the planning stage, researchers define the objectives of their data collection and analysis, identify data requirements, and develop a data management plan. This plan outlines how data will be collected, stored, and shared, as well as ethical and legal considerations.\nThe management of research data must be thoroughly considered before physical materials (samples, model organisms, reagents or media, etc) and digital data (bioinformatics data) are collected, observed, generated, created or reused. Data management plans (DMP) must be developed and documented, preferably in electronic format. DMPs should be updated when significant changes to the management of research data occur and (references to) the DMP should be stored with the corresponding research data as long as they exist.\n\n\n\n\n\n\nWarning\n\n\n\nThis is a reminder that this course focuses only on bioinformatics digital data, not physical samples or materials.\n\n\nThe DMP should be discussed with project collaborators, research managers and supervisors (if any), ensuring that agreements are reached regarding responsibilities for different research data management activities during and after research projects.\n\n\n\n\n\n\nTip\n\n\n\nWe will check about how to write a DMP for NGS data in the 5th lesson.\n\n\n\n\n\nIn this stage, researchers gather the data according to their plan. They document important details about the data, such as its source, collection methods, and any modifications made during data acquisition. Proper documentation ensures data quality and facilitates later use.\nResearch data should be collected and processed in line with best practice in the research discipline. For example, there will be important differences between a metagenomics project and a clinical trial involving human samples! Research projects should be documented in a way that allows them to be repeated by others. Among other things, this includes clearly and accurately describing project methodology and any equipment, software or code used. This includes workflows for data preprocessing and how you will structure and organize your files.\n\n\n\n\n\n\nTip\n\n\n\nWe will see about how to structure your files and folders in the 6th lesson.\n\n\nIn addition, research data should be described using appropriate metadata to facilitate searching for, the identification of, and the interpretation of the research data. Metadata should be linked to the research data as long as they exist, unless legislation or agreements state otherwise.\n\n\n\n\n\n\nTip\n\n\n\nWe will see about what kind of metadata you can use for your NGS data in the 7th lesson\n\n\n\n\n\nThis, in the data life cycle for NGS data, is a critical phase that involves transforming raw sequencing data into meaningful biological insights. During this stage, researchers apply computational methods and bioinformatics tools to extract valuable information from the vast amount of sequencing data generated in NGS experiments.\nThroughout this phase, researchers should adhere to good coding practices, ensuring well-documented and reproducible analyses. Code notebooks and version control tools, such as Git, help maintain transparency and facilitate the sharing of methods and results with the scientific community.\nTo streamline and standardize the data analysis process, researchers often employ workflows and pipelines. Workflows automate the execution of multiple analysis steps, enhancing efficiency and consistency while promoting reproducibility. The NGS research community benefits from collaborative efforts, such as the nf-core community, which provides pre-established and validated Nextflow-based pipelines for various NGS applications. Leveraging community-developed pipelines ensures adherence to best practices and accelerates the pace of research through shared expertise.\n\n\n\n\n\n\nTip\n\n\n\nWe will see more about version control in lesson 9\n\n\n\n\n\nStoring data securely is essential to protect it from loss, unauthorized access, and corruption. Researchers must choose appropriate storage solutions and implement data security measures to safeguard sensitive or confidential information.\nResearch data must be classified at the start of a research project on the basis of the level of sensitivity and the impact to the University if data are disclosed, altered or destroyed without authorisation. Risks to data security and of data loss should be assessed in relation to the data classification. This includes evaluating:\n\nPhysical and digital access to research data\nRisks associated with data management procedures\nBackup requirements and backup procedures\nExternal and internal threats to data confidentiality, integrity and accessibility\nFinancial, regulatory and technical consequences of working with data, data storage and data preservation\n\n\n\n\n\n\n\nWarning\n\n\n\nThis step is very specific to the setup used in your environment. Maybe you are using a department server with storage, or you use a personal drive, or cloud solutions such as AWS. Thus, we cannot include in this course a comprehensive guideline on this matter.\n\n\n\n\n\n\n\n\nAbout GPDR and protected data\n\n\n\nThis course does not talk in detail about protected NGS data, such as human or patient samples. For more information about GPDR protected data, check out our course!\n\n\n\n\n\nSharing data is an important aspect of the data life cycle. Researchers are encouraged to share their data openly whenever possible, adhering to Open Science and FAIR principles. Data sharing fosters collaboration, increases the visibility of research, and enables data reuse by others.\nLegislation or agreements may preclude research data sharing or impose conditions for sharing. Before sharing research data, the relevant approvals need to be obtained and, if necessary, the appropriate agreements set up to allow data and material sharing.\nBy default, research data should be made openly available after project end, as a minimum for data sets underlying research publications. Concerns relating to intellectual property rights, personal data protection, information security as well as commercial and national interests and legislation must be taken into account in accordance with the principle of ‚Äòas open as possible, as closed as necessary‚Äô. If the research data cannot be made available, sharing the metadata associated with the research data should be considered. The FAIR principles (for findable, accessible, interoperable and reusable research objects) should be followed as much as possible when preparing digital data sets that can be shared. This includes as a minimum:\n\nProviding open access to data (Open Data) by depositing data in a data repository, or by providing access to information on whether, when, how, and to what extent data can be accessed if data sets cannot be made openly available.\nAs much as possible using persistent identifiers (PID) and metadata (such as descriptive keywords) that help locate the data set.\nCommunicating terms and conditions for data reuse by others, for example by attaching a data licence.\nProviding the information necessary to understand how data sets were created and structured, and for what purpose.\n\n\n\n\n\n\n\nTip\n\n\n\nWe will talk about FAIR and OS principles in the next lesson\n\n\n\n\n\nData preservation ensures that valuable research data is available and usable in the long term. Researchers should deposit data in trusted data repositories or archives, ensuring its ongoing accessibility and future use by the scientific community.\nAppropriate arrangements for the long-term preservation of digital data, physical material and associated metadata must be made, adhering to legislation and/or agreements. This should include:\n\nDeciding which research data will be preserved. As a minimum, data sets underlying published research results must be preserved so that any objections or criticisms can be addressed.\nDeciding how long research data will be preserved. Data sets underlying research publications should be retained for at least five years after project completion or date of publication, whichever comes last.\nChoosing a format and location in which research data should be preserved, and deciding what metadata should be associated with the preserved data and material.\nDeleting/destructing research data if legislation or agreements exclude preservation, or when researchers and their managers determine that preservation is not required (for example when research data can easily be reproduced) or not possible (for example when research data are too costly to store or when material quality will deteriorate over time).\nAssigning a person, persons or role(s) responsible for the research data after project end. Responsibilities include safeguarding the long-term integrity of data sets.\nDetermining rights, for example, of access to and use of preserved data sets.\n\nIn addition, your institution may oblige you to preserve and keep a copy of your data at their premises. They will usually explain any requirement at their own data management department. For example, the UCPH mandates that a copy of data sets and associated metadata must remain at UCPH after project end and/or when employment with the University ceases, in a way in which they are accessible to research managers and understandable for research managers and peers, unless legislation or agreements determine otherwise.\n\n\n\n\n\n\nTip\n\n\n\nWe will check about which repositories you can use to preserve your NGS data in the 10th lesson",
    "crumbs": [
      "Course material",
      "Data Life Cycle"
    ]
  },
  {
    "objectID": "develop/03_data_life_cycle.html#wrap-up",
    "href": "develop/03_data_life_cycle.html#wrap-up",
    "title": "Data Life Cycle",
    "section": "",
    "text": "In this lesson we have learned about the Research Data Life Cycle, from the conception of a project and collection of data until the end of the project and preservation of the data. These concepts were taken from the UCPH data management guidelines and put them in the context of NGS, but are perfectly usable in other universities and applications.",
    "crumbs": [
      "Course material",
      "Data Life Cycle"
    ]
  },
  {
    "objectID": "develop/practical_workshop.html",
    "href": "develop/practical_workshop.html",
    "title": "Practical material",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nOrganize and structure your data and data analysis with Cookiecutter templates\nDefine metadata fields and collect metadata when creating a Cookiecutter folder\nEstablish naming conventions for your data\nCreate a catalog of your data\nUse GitHub repositories of your data analysis and display them as GitHub Pages\nArchive GitHub repositories on Zenodo\nThis practical version covers practical aspects of RDM applied to biodata. The exercises provided here aim to help you organize and structure your datasets and data analyses. You‚Äôll learn how to manage your experimental metadata effectively and safely version control and archive your data analyses using GitHub repositories and Zenodo. Through these guided exercises and step-by-step instructions, we hope you will acquire essential skills for managing and sharing your research data efficiently, thereby enhancing the reproducibility and impact of your work."
  },
  {
    "objectID": "develop/practical_workshop.html#organize-and-structure-your-ngs-data-and-data-analysis",
    "href": "develop/practical_workshop.html#organize-and-structure-your-ngs-data-and-data-analysis",
    "title": "Practical material",
    "section": "1. Organize and structure your NGS data and data analysis",
    "text": "1. Organize and structure your NGS data and data analysis\nApplying a consistent file structure and naming conventions to your files will help you to efficiently manage your data. We will divide your NGS data and data analyses into two different types of folders:\n\nAssay folders: These folders contain the raw and processed NGS datasets, as well as the pipeline/workflow used to generate the processed data, provenance of the raw data, and quality control reports of the data. This data should be locked and read-only to prevent unwanted modifications. This also applies to external resources that you download.\nProject folders: These folders contain all the necessary files for a specific research project. A project may use several assays or results from other projects. The assay data should not be copied or duplicated, but linked from the source.\n\nProjects and Assays are separated from each other because a project may use one or more assays to answer a scientific question, and assays may be reused several times in different projects. This could be, for example, all the data analysis related to a publication (an RNAseq and a ChIPseq experiment), or a comparison between a previous ATACseq experiment (which was used for a older project) with a new laboratory protocol.\nYou could also create Genomic resources folders things such as genome references (fasta files) and annotations (gtf files) for different species, as well as indexes for different alignment algorithms. If you want to know more, feel free to check the relevant full lesson\nThis will help you to keep your data tidied up, especially if you are working in a big lab where assays may be used for different purposes and by different people!\n\nAssay folder\nFor each NGS experiment, there should be an Assay folder that will contain all experimental datasets, that is, an Assay (raw files and pipeline processed files). Raw files should not be modified at all, but you should probably lock modifications to the final results once you are done with preprocessing the data. This will help you prevent unwanted modifications to the data. Each Assay subfolder should be named in a way that is unique, easily readable, distinguishable, and understood at a glance. For example, you could name an NGS assay using an acronym for the type of NGS assay (RNAseq, ChIPseq, ATACseq), a keyword that represents a unique descriptive element of that assay, and the date. Like this:\n&lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD\nFor example CHIP_Oct4_20230101 is a ChIPseq assay made on 1st January 2023 with the keyword Oct4, so it is easily identifiable by the eye. Next, let‚Äôs take a look at a possible folder structure and what kind of files you can find there.\nCHIP_Oct4_20230101/\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ metadata.yml\n‚îú‚îÄ‚îÄ pipeline.md\n‚îú‚îÄ‚îÄ processed\n‚îî‚îÄ‚îÄ raw\n   ‚îú‚îÄ‚îÄ .fastq.gz\n   ‚îî‚îÄ‚îÄ samplesheet.csv\n\nREADME.md: Long description of the assay in markdown format. It should contain provenance of the raw NGS data (samples, laboratory protocols used, the aim of the assay, etc)\nmetadata.yml: metadata file for the assay describing different keys and important information regarding that assay (see this lesson).\npipeline.md: description of the pipeline used to process raw data, as well as the commands used to run the pipeline.\nprocessed: folder with results of the preprocessing pipeline. Contents depend on the pipeline used.\nraw: folder with the raw data.\n\n.fastq.gz:In the case of NGS assays, there should be fastq files.\nsamplesheet.csv: file that contains metadata information for the samples. This file is used to run the nf-core pipelines. You can also add extra columns with info regarding the experimental variables and batches so it can be used for downstream analysis as well.\n\n\n\n\nProject folder\nOn the other hand, we have the other type of folder called Projects. In this folder, you will save a subfolder for each project that you (or your lab) work on. Each Project subfolder will contain project information and all the data analysis notebooks and scripts used in that project.\nAs like for an Assay folder, the Project folder should be named in a way that is unique, easily readable, distinguishable, and understood at a glance. For example, you could name it after the main author‚Äôs initials, a keyword that represents a unique descriptive element of that assay, and the date:\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\nFor example, JARH_Oct4_20230101, is a project about the gene Oct4 owned by Jose Alejandro Romero Herrera, created on the 1st of January of 2023.\nNext, let‚Äôs take a look at a possible folder structure and what kind of files you can find there.\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n‚îú‚îÄ‚îÄ data\n‚îÇ  ‚îî‚îÄ‚îÄ &lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD/\n‚îú‚îÄ‚îÄ documents\n‚îÇ  ‚îî‚îÄ‚îÄ Non-sensitive_NGS_research_project_template.docx\n‚îú‚îÄ‚îÄ notebooks\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis.rmd\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ reports\n‚îÇ  ‚îú‚îÄ‚îÄ figures\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis/\n‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ heatmap_sampleCor_20230102.png\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis.html\n‚îú‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ results\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis/\n‚îÇ      ‚îî‚îÄ‚îÄ DEA_treat-control_LFC1_p01.tsv\n‚îú‚îÄ‚îÄ scripts\n‚îî‚îÄ‚îÄ metadata.yml\n\ndata: a folder that contains symlinks or shortcuts to where the data is, avoiding copying and modification of original files.\ndocuments: a folder containing Word documents, slides, or PDFs related to the project, such as explanations of the data or project, papers, etc. It also contains your Data Management Plan.\n\nNon-sensitive_NGS_research_project_template.docx. This is a pre-filled Data Management Plan based on the Horizon Europe guidelines.\n\nnotebooks: a folder containing Jupyter, R markdown, or Quarto notebooks with the actual data analysis.\nREADME.md: detailed description of the project in markdown format.\nreports: notebooks rendered as HTML/docx/pdf versions, ideal for sharing with colleagues and also as a formal report of the data analysis procedure.\n\nfigures: figures produced upon rendering notebooks. The figures will be saved under a subfolder named after the notebook that created them. This is for provenance purposes so we know which notebook created which figures.\n\nrequirements.txt: file explaining what software and libraries/packages and their versions are necessary to reproduce the code.\nresults: results from the data analysis, such as tables with differentially expressed genes, enrichment results, etc.\nscripts: folder containing helper scripts needed to run data analysis or reproduce the work of the folder\ndescription.yml: a short description of the project.\nmetadata.yml: metadata file for the assay describing different keys (see this lesson).\n\n\n\nTemplate engine\nIt is very easy to create a folder template using cookiecutter. Cookiecutter is a command-line utility that creates projects from cookiecutters (that is, a template), e.g.¬†creating a Python package project from a Python package project template. Here you can find an example of a cookiecutter folder template-directed to NGS data, where we have applied the structures explained in the previous sections. You are very welcome to adapt it or modify it to your needs!\n\nQuick tutorial on cookiecutter\nCreating a Cookiecutter template from scratch involves defining a folder structure, creating a cookiecutter.json file, and specifying the placeholders (keywords) that will be replaced during project generation. Let‚Äôs walk through the process step by step:\n\nStep 1: Create a Folder Template\nStart by creating a folder with the structure you want for your template. For example, let‚Äôs create a simple Python project template:\nmy_template/\n|-- {{cookiecutter.project_name}}\n|   |-- main.py\n|-- tests\n|   |-- test_{{cookiecutter.project_name}}.py\n|-- README.md\nIn this example, {cookiecutter.project_name} is a placeholder that will be replaced with the actual project name when the template is used.\n\n\nStep 2: Create cookiecutter.json\nIn the root of your template folder, create a file named cookiecutter.json. This file will define the variables (keywords) that users will be prompted to fill in. For our Python project template, it might look like this:\n{\n  \"project_name\": \"MyProject\",\n  \"author_name\": \"Your Name\",\n  \"description\": \"A short description of your project\"\n}\nThese are the questions users will be asked when generating a project based on your template. The values provided here will be used to replace the corresponding placeholders in the template files.\nIn addition to replacing placeholders in files and directory names, Cookiecutter can also automatically fill in information within the contents of text files. This can be useful for providing default configurations or templates for code files. Let‚Äôs extend our previous example to include a placeholder inside a text file:\nFirst, modify the my_template/main.py file to include a placeholder inside its contents:\n# main.py\n\ndef hello():\n    print(\"Hello, {{cookiecutter.project_name}}!\")\nNow, the {cookiecutter.project_name} placeholder is inside the main.py file. When you run Cookiecutter, it will automatically replace the placeholders not only in file and directory names but also within the contents of text files. After running Cookiecutter, your generated main.py file might look like this:\n# main.py\n\ndef hello():\n    print(\"Hello, MyProject!\")  # Assuming \"MyProject\" was entered as the project_name\n\n\nStep 3: Use Cookiecutter\nNow that your template is set up, you can use Cookiecutter to generate a project based on it. Open a terminal and run:\ncookiecutter path/to/your/template\nCookiecutter will prompt you to fill in the values for project_name, author_name, and description. After you provide these values, Cookiecutter will replace the placeholders in your template files with the entered values.\n\n\nStep 4: Explore the Generated Project\nOnce the generation process is complete, navigate to the directory where Cookiecutter created the new project. You will see a project structure with the placeholders replaced by the values you provided.\n\n\n\n\n\n\nExercise 1: Create your own template\n\n\n\n\n\n\n\nUsing Cookiecutter, create your own templates for your folders. You do not need to copy exactly our suggestions, adjust your template to your own needs!\nRequirements:\nUsing Cookiecutter, create your own templates for your folders. You do not need to copy exactly our suggestions, adjust your template to your own needs! In order to create your cookiecutter template, you will need to install Python, cookiecutter, Git, and a GitHub account. If you do not have Git and a GitHub account, we suggest you do one as soon as possible. We will take a deeper look at Git and GitHub in the version control lesson.\nWe have prepared already two simple Cookiecutter templates in GitHub repositories.\nAssay\n\nFirst, fork our Assay folder template from the GitHub page into your own account/organization. \nThen, use git clone &lt;your URL to the template&gt; to put it on your computer.\nModify the contents of the repository so that it matches the Assay example above. You are welcome to make changes as you please!\nModify the cookiecutter.json file so that it will include the Assay name template\nGit add, commit, and push your changes\nTest your folder by using cookiecutter &lt;URL to your GitHub repository for \"assay-template&gt;\n\nProject\n\nFirst, fork our Project folder template from the GitHub page into your own account/organization. \nThen, use git clone &lt;your URL to the template&gt; to put it on your computer.\nModify the contents of the repository so that it matches the Project example above. You are welcome to make changes as you please!\nModify the cookiecutter.json file so that it will include the Project name template\nGit add, commit, and push your changes\nTest your folder by using cookiecutter &lt;URL to your GitHub repository for \"project-template&gt;"
  },
  {
    "objectID": "develop/practical_workshop.html#metadata-and-naming-conventions",
    "href": "develop/practical_workshop.html#metadata-and-naming-conventions",
    "title": "Workshop 202X",
    "section": "",
    "text": "Metadata is the behind-the-scenes information that makes sense of data and gives context and structure. For NGS data, metadata includes information such as when and where the data was collected, what it represents, and how it was processed. Let‚Äôs check what kind of relevant metadata is available for NGS data and how to capture it in your Assay or Project folders. Both of these folders contain a metadata.yml file and a README.md file. In this section, we will check what kind of information you should collect in each of these files.\n\n\n\n\n\n\nMetadata and controlled vocabularies\n\n\n\nIn order for metadata to be most useful, you should try to use controlled vocabularies for all your fields. For example, tissue could be described with the UBERON ontologies, species using the NCBI taxonomy, diseases using the Mondo database, etc. Unfortunately, implementing a systematic way of using these vocabularies is rather complex and outside the scope of this workshop, but you are very welcome to try to implement them on your own!\n\n\n\n\nThe README.md file is a markdown file that allows you to write a long description of the data placed in a folder. Since it is a markdown file, you are able to write in rich text format (bold, italic, include links, etc) what is inside the folder, why it was created/collected, how and when. If it is an Assay folder, you could include the laboratory protocol used to generate the samples, images explaining the experiment design, a summary of the results of the experiment and any sort of comments that would help to understand the context of the experiment. On the other hand, a ‚ÄòProject‚Äô README file may contain a description of the project, what are its aims, why is it important, what ‚ÄòAssays‚Äô is it using, how to interpret the code notebooks, a summary of the results and, again, any sort of comments that would help to understand the project.\nHere is an example of a README file for a Project folder:\n# NGS Analysis Project: Exploring Gene Expression in Human Tissues\n\n## Aims\n\nThis project aims to investigate gene expression patterns across various human tissues using Next Generation Sequencing (NGS) data. By analyzing the transcriptomes of different tissues, we seek to uncover tissue-specific gene expression profiles and identify potential markers associated with specific biological functions or diseases.\n\n## Why It's Important\n\nUnderstanding tissue-specific gene expression is crucial for deciphering the molecular basis of health and disease. Identifying genes that are uniquely expressed in certain tissues can provide insights into tissue function, development, and potential therapeutic targets. This project contributes to our broader understanding of human biology and has implications for personalized medicine and disease research.\n\n## Datasets\n\nWe have used internal datasets with IDs: RNA_humanSkin_20201030, RNA_humanBrain_20210102, RNA_humanLung_20220304.\n\nIn addition, we utilized publicly available NGS datasets from the GTEx (Genotype-Tissue Expression) project, which provides comprehensive RNA-seq data across multiple human tissues. These datasets offer a wealth of information on gene expression levels and isoform variations across diverse tissues, making them ideal for our analysis.\n\n## Summary of Results\n\nOur analysis revealed distinct gene expression patterns among different human tissues. We identified tissue-specific genes enriched in brain tissues, highlighting their potential roles in neurodevelopment and function. Additionally, we found a set of genes that exhibit consistent expression across a range of tissues, suggesting their fundamental importance in basic cellular processes.\n\nFurthermore, our differential expression analysis unveiled significant changes in gene expression between healthy and diseased tissues, shedding light on potential molecular factors underlying various diseases. Overall, this project underscores the power of NGS data in unraveling intricate gene expression networks and their implications for human health.\n\n---\n\nFor more details, refer to our [Jupyter Notebook](link-to-jupyter-notebook.ipynb) for the complete analysis pipeline and code.\n\n\n\nThe metadata file is a yml file, which is a text document that contains data formatted using a human-readable data format for data serialization.\n\n\n\nyaml file example\n\n\n\n\n\nThere is a ton of information you can collect regarding an NGS assay or a project. Some information fields are very general, such as author or date, while others are specific to the Assay or Project folder. Below, we will take a look at minimal information you should collect in each of the folders.\n\n\nHere you can find a list of suggestions for general metadata fields that can be used for both assays and project folders:\n\nTitle: A brief yet informative name for the dataset.\nAuthor(s): The individual(s) or organization responsible for creating the dataset. You can use your ORCID\nDate Created: The date when the dataset was originally generated or compiled. Use YYYY-MM-DD format!\nDescription: A short narrative explaining the content, purpose, and context.\nKeywords: A set of descriptive terms or phrases that capture the folder‚Äôs main topics and attributes.\nVersion: The version number or identifier for the folder, useful for tracking changes.\nLicense: The type of license or terms of use associated with the dataset/project.\n\n\n\n\nHere you will find a table with possible metadata fields that you can use to annotate and track your Assay folders:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nassay_ID\nIdentifier for the assay that is at least unique within the project\n&lt;Assay-ID\\&gt;_&lt;keyword\\&gt;_YYYYMMDD\nNA\nCHIP_Oct4_20200101\n\n\nassay_type\nThe type of experiment performed, eg ATAC-seq or seqFISH\nNA\nontology field- e.g. EFO or OBI\nChIPseq\n\n\nassay_subtype\nMore specific type or assay like bulk nascent RNAseq or single cell ATACseq\nNA\nontology field- e.g. EFO or OBI\nbulk ChIPseq\n\n\nowner\nOwner of the assay (who made the experiment?).\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nJose Romero\n\n\nplatform\nThe type of instrument used to perform the assay, eg Illumina HiSeq 4000 or Fluidigm C1 microfluidics platform\nNA\nontology field- e.g. EFO or OBI\nIllumina\n\n\nextraction_method\nTechnique used to extract the nucleic acid from the cell\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nlibrary_method\nTechnique used to amplify a cDNA library\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nexternal_accessions\nAccession numbers from external resources to which assay or protocol information was submitted\nNA\neg protocols.io, AE, GEO accession number, etc\nGSEXXXXX\n\n\nkeyword\nKeyword for easy identification\nwordWord\ncamelCase\nOct4ChIP\n\n\ndate\nDate of assay creation\nYYYYMMDD\nNA\n20200101\n\n\nnsamples\nNumber of samples analyzed in this assay\n&lt;integer\\&gt;\nNA\n9\n\n\nis_paired\nPaired fastq files or not\n&lt;single OR paired\\&gt;\nNA\nsingle\n\n\npipeline\nPipeline used to process data and version\nNA\nNA\nnf-core/chipseq -r 1.0\n\n\nstrandedness\nThe strandedness of the cDNA library\n&lt;+ OR - OR *\\&gt;\nNA\n*\n\n\nprocessed_by\nWho processed the data\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nSarah Lundregan\n\n\norganism\nOrganism origin\n&lt;Genus species\\&gt;\nTaxonomy name\nMus musculus\n\n\norigin\nIs internal or external (from a public resources) data\n&lt;internal OR external\\&gt;\nNA\ninternal\n\n\npath\nPath to files\n&lt;/path/to/file\\&gt;\nNA\nNA\n\n\nshort_desc\nShort description of the assay\nplain text\nNA\nOct4 ChIP after pERK activation\n\n\nELN_ID\nID of the experiment/assay in your Electronic Lab Notebook software, like labguru or benchling\nplain text\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\nHere you will find a table with possible metadata fields that you can use to annotate and track your Project folders:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nproject\nProject ID\n&lt;surname\\&gt;_et_al_2023\nNA\nproks_et_al_2023\n\n\nauthor\nOwner of the project\n&lt;First name\\&gt; &lt;Surname\\&gt;\nNA\nMartin Proks\n\n\ndate\nDate of creation\nYYYYMMDD\nNA\n20230101\n\n\ndescription\nShort description of the project\nPlain text\nNA\nThis is a project describing the effect of Oct4 perturbation after pERK activation\n\n\n\n\n\n\n\n\n\n\n\n\nThe information provided in this lesson is not at all exhaustive. There might be many more fields and controlled vocabularies that could be useful for your NGS data. We recommend that you take a look at the following sources for more information!\n\nTranscriptomics metadata standards and fields\nBionty: Biological ontologies for data scientists.\n\n\n\n\n\n\n\nExercise 2: modify the metadata.yml files in your cookiecutter templates\n\n\n\n\n\n\n\nWe have seen some examples of metadata for NGS data. It is time now to customize your cookiecutter templates and modify the metadata.yml files so that they fit your needs!\n\nThink about what kind of metadata you would like to include.\nModify the cookiecutter.json file so that when you create a new folder template, all the metadata is filled accordingly.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\n\n\ncookiecutter_json_example\n\n\n\n\n\n\n\n\nModify the metadata.yml file so that it includes the metadata recorded by the cookiecutter.json file.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\n\n\ncookiecutter_json_example\n\n\n\n\n\n\n\n\nModify the README.md file so that it includes the short description recorded by the cookiecutter.json file.\nGit add, commit and push the changes of your template.\nTest your folders by using the command cookiecutter &lt;URL to your cookiecutter repository in GitHub&gt;"
  },
  {
    "objectID": "develop/practical_workshop.html#naming-conventions",
    "href": "develop/practical_workshop.html#naming-conventions",
    "title": "Practical material",
    "section": "3. Naming conventions",
    "text": "3. Naming conventions\nAs discussed in lesson 3, consistent naming conventions are key for interpreting, comparing, and reproducing findings in scientific research. Standardized naming helps organize and retrieve data or results, allowing researchers to locate and compare similar types of data within or across large datasets.\n\n\n\n\n\n\nExercise 3: Define your file name conventions\n\n\n\n\n\n\n\nAvoid long and complicated names and ensure your file names are both informative and easy to manage:\n\nFor saving a new plot, a heatmap representing sample correlations\nWhen naming the file for the document containing the Research Data Management Course Objectives (Version 2, 2nd May 2024) from the University of Copenhagen\nConsider the most common file types you work with, such as visualizations, figures, tables, etc., and create logical and clear file names\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\nheatmap_sampleCor_20240101.png\nKU_RDM-objectives_20240502_v02.doc or KU_RDMObj_20240502_v02.doc"
  },
  {
    "objectID": "develop/practical_workshop.html#create-a-catalog-of-your-assays-folder",
    "href": "develop/practical_workshop.html#create-a-catalog-of-your-assays-folder",
    "title": "Practical material",
    "section": "4. Create a catalog of your assays folder",
    "text": "4. Create a catalog of your assays folder\nThe next step is to collect all the NGS datasets that you have created in the manner explained above. Since your folders all should contain the metadata.yml file in the same place with the same metadata, it should be very easy to iteratively go through all the folders and merge all the metadata.yml files into a one single table. This table can be then browsed easily with Microsoft Excel, for example. If you are interested in making a Shiny app or Python Panel tool to interactively browse the catalog, check out this lesson.\n\n\n\n\n\n\nExercise 4: create a metadata.tsv catalog\n\n\n\n\n\n\n\nWe will make a small script in R (or you can make one with python) that recursively goes through all the folders inside a input path (like your Assays folder), fetch all the metadata.yml files and merge them. Finally, it will write a tsv file as an output.\n\nCreate a folder call Assays\nUnder that folder, make three new Assay folders from your cookiecutter template\nRun the script below with R (or create your own with python). Modify the folder_path variable so it matches the path tot the folder Assays. The table will be written under the same folder_path.\nVisualize your Assays table with Excel\n\n\nlibrary(yaml)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Function to recursively fetch metadata.yml files\nget_metadata &lt;- function(folder_path) {\n    file_list &lt;- list.files(path = folder_path, pattern = \"metadata\\\\.yml$\", recursive = TRUE, full.names = TRUE)\n    metadata_list &lt;- lapply(file_list, yaml::yaml.load_file)\n    return(metadata_list)\n    }\n\n# Specify the folder path\n    folder_path &lt;- \"/path/to/your/folder\"\n\n    # Fetch metadata from the specified folder\n    metadata &lt;- get_metadata(folder_path)\n\n    # Convert metadata to a data frame\n    metadata_df &lt;- data.frame(matrix(unlist(metadata), ncol = length(metadata), byrow = TRUE))\n    colnames(metadata_df) &lt;- names(metadata[[1]])\n\n    # Save the data frame as a TSV file\n    output_file &lt;- paste0(\"database_\", format(Sys.Date(), \"%Y%m%d\"), \".tsv\")\n    write.table(metadata_df, file = output_file, sep = \"\\t\", quote = FALSE, row.names = FALSE)\n\n    # Print confirmation message\n    cat(\"Database saved as\", output_file, \"\\n\")"
  },
  {
    "objectID": "develop/practical_workshop.html#version-control-of-your-data-analysis-using-git-and-github",
    "href": "develop/practical_workshop.html#version-control-of-your-data-analysis-using-git-and-github",
    "title": "Practical material",
    "section": "5. Version control of your data analysis using Git and GitHub",
    "text": "5. Version control of your data analysis using Git and GitHub\nVersion control involves systematically tracking changes to a project over time, offering a structured way to document revisions and understand the progression of your work. In research data management and data analytics, it plays a critical role and provides numerous benefits.\nGit is a distributed version control system that helps developers and researchers efficiently manage project history, collaborate seamlessly, and maintain data integrity. On the other hand, GitHub is a web-based platform that builds on Git‚Äôs functionality by providing a centralized, collaborative hub for hosting Git repositories. It offers several key functionalities, such as tracking issues, security features to safeguard your repos, and GitHub Pages that allow you to create websites to showcase your projects.\n\n\n\n\n\n\nCreate a GitHub organization for your lab or department\n\n\n\nGitHub users can create organizations, allowing groups to collaborate or create repositories under the same organization umbrella. You can create an educational organization on Github for free, by setting up a Github account for your lab.\nFollow these instructions to create a GitHub organization.\nOnce you‚Äôve established your GitHub organization, be sure to create your repositories within the organization‚Äôs space rather than under your personal user account. This keeps your projects centralized and accessible to the entire group. Best practices for managing an organization on GitHub include setting clear access permissions, regularly reviewing roles and memberships, and organizing repositories effectively to keep your projects structured and easy to navigate.\n\n\n\nSetting up a GitHub repository for your project folder\nVersion controlling your data analysis folders becomes straightforward once you‚Äôve established your Cookiecutter templates. After you‚Äôve created several folder structures and metadata using your Cookiecutter template, you can manage version control by either converting those folders into Git repositories or copying a folder into an existing Git repository. Both approaches are explained in Lesson 5.\n\n\n\n\n\n\nExercise 6: initialize a repository from an existing folder:\n\n\n\n\n\n\n\n\nInitialize the repository: Begin by running the command git init in your project directory. This command sets up a new Git repository in the current directory and is executed only once, even for collaborative projects. See (git init) for more details.\nCreate a remote repository: Once the local repository is initialized, create an empty new repository on GitHub (website or Github Desktop).\nConnect the remote repository: Add the GitHub repository URL to your local repository using the command git remote add origin &lt;URL&gt;. This associates the remote repository with the name ‚Äúorigin.‚Äù\nCommit changes: If you have files you want to add to your repository, stage them using git add ., then create a commit to save a snapshot of your changes with git commit -m \"add local folder\".\nPush to GitHub: To synchronize your local repository with the remote repository and establish a tracking relationship, push your commits to the GitHub repository using git push -u origin main.\n\n\n\n\n\n\n\n\n\n\n\n\nTips to write good commit messages\n\n\n\nIf you would like to know more about Git commits and the best way to make clear Git messages, check out this post!\n\n\n\n\nGitHub Pages\nOnce you have created your repository (and put it in GitHub), you have now the opportunity to add your data analysis reports that you created, in either Jupyter Notebooks, R Markdown files, or HTML reports, in a GitHub Page website. Creating a GitHub page is very simple, and we really recommend that you follow the nice tutorial that GitHub has put for you. Nonetheless, we will see the main steps in the exercise below.\nThere are many different ways to create your web pages. We recommend using Mkdocs and Mkdocs materials as a framework to create a nice webpage simply. The folder templates that we used as an example in the previous exercise already contain everything you need to start a webpage. Nonetheless, you will need to understand the basics of MkDocs and MkDocs materials to design a webpage to your liking. MkDocs is a static webpage generator that is very easy to use, while MkDocs materials is an extension of the tool that gives you many more options to customize your website. Check out their web pages to get started!\n\n\n\n\n\n\nExercise 5: make a project folder and publish a data analysis webpage\n\n\n\n\n\n\n\n\nConfigure your main GitHub Page and its repo\nThe first step is to set up the main GitHub Page site and the repository that will host it. This is very simple, as you will only need to follow these steps. In a Markdown document, outline the primary objectives of the organization and provide an overview of ongoing research projects. After you have created the organization/usernamegithub.io, it is time to configure your Project repository webpage using MkDocs!\nStart a new project from Cookiecutter or use one from the previous exercise.\nIf you use a Project repo from the first exercise, go to the next paragraph. Using Cookiecutter, create a new data analysis project. Remember to fill up your metadata and description files! After you have created the folder, it would be best to initialize a Git repo following the instructions from the previous section.\nNext, link your data of interest (or create a small fake dataset) and make an example of a data analysis notebook/report (this could be just a scatter plot of a random matrix of values). Depending on your setup, you might be using Jupyter Notebooks or Rmarkdowns. The extensions that we have installed using pip allow you to directly add a Jupyter Notebook file to the mkdocs.yml navigation section. On the other hand, if you are using Rmarkdown, you will have to knit your document into either an HTML page or a GitHub document.\nFor the purposes of this exercise, we have already included a basic index.md markdown file that can serve as the intro page of your repo, and a jupyter_example.ipynb with some code in it. You are welcome to modify them further to test them out!\nUse MkDocs to create your webpage\nWhen you are happy with your files and are ready to publish them, make sure to add, commit, and push the changes to the remote. Then, build up your webpage using MkDocs and the mkdocs gh-deploy command from the same directory where the mkdocs.yml file is. For example, if your mkdocs.yml for your Project folder is in /Users/JARH/Projects/project1_JARH_20231010/mkdocs.yml, do cd /Users/JARH/Projects/project1_JARH_20231010/ and then mkdocs gh-deploy. This requires a couple of changes in your GitHub organization settings.\nRemember to make sure that your markdowns, images, reports, etc., are included in the docs folder and properly set up in the navigation section of your mkdocs.yml file.\nFinally, we only need to set up the GitHub Project repo settings.\nPublishing your GitHub Page\nGo to your GitHub repo settings and configure the Page section. Since you are using the mkdocs gh-deploy command to publish your site in the gh-pages branch (as explained the the mkdocs documentation), we need to change where GitHub is fetching the website. You will need to configure the settings of this repository in GitHub so that the Page is taken from the gh-pages branch and the root folder.\n\n\n\nGitHub Pages setup\n\n\n\nBranch should be gh-pages\nFolder should be root\n\nAfter a couple of minutes, your webpage should be ready! You should be able to see your webpage through the link provided in the Page section!\n\nNow it is also possible to include this repository webpage in your main webpage organizationgithub.io by including the link of the repo website (https://organizationgithub.io/repo-name) in the navigation section of the mkdocs.yml file in the main organizationgithub.io repo."
  },
  {
    "objectID": "develop/practical_workshop.html#archive-github-repositories-on-zenodo",
    "href": "develop/practical_workshop.html#archive-github-repositories-on-zenodo",
    "title": "Practical material",
    "section": "6. Archive GitHub repositories on Zenodo",
    "text": "6. Archive GitHub repositories on Zenodo\nArchives are specialized digital platforms that provide secure storage, curation, and dissemination of scientific data. They play a crucial role in the research community by acting as trusted repositories for preserving valuable datasets. With standardized formats and thorough curation processes, they ensure the long-term accessibility and citability of research findings. Researchers globally rely on these repositories to share, discover, and validate scientific information, promoting transparency, collaboration, and knowledge growth across various fields.\nIn the next practical exercise, you will archive your Project folder, which contains data analyses (software, code and pipelines), in a repository such as Zenodo. This can be done by linking your Zenodo account to your GitHub account.\n\n\n\n\n\n\nArchiving data‚Ä¶\n\n\n\nData should be deposited in a domain-specific archive. If you want to know more about these archives, check out this lesson.\n\n\n\nZenodo\nZenodo is an open-access digital repository that supports the archiving of scientific research outputs, including datasets, papers, software, and multimedia files. Affiliated with CERN and backed by the European Commission, Zenodo promotes transparency, collaboration, and the advancement of knowledge globally. Researchers can easily upload, share, and preserve their data on its user-friendly platform. Each deposit receives a unique DOI for citability and long-term accessibility. Zenodo also offers robust metadata options and allows linking your GitHub account to archive a specific release of your GitHub repository directly to Zenodo. This integration streamlines the process of preserving a snapshot of your project‚Äôs progress.\n\n\n\n\n\n\nExercise 6: Archive a Project GitHub repo in Zenodo\n\n\n\n\n\n\n\n\nIn order to archive your GitHub repos in Zenodo, link your Zenodo and GitHub accounts\nOnce your accounts are linked, go to your Zenodo GitHub account settings and turn on the GitHub repository you want to archive. \nCreating a Zenodo archive is now as simple as making a release in your GitHub repository. Remember to create a proper tag and specify the version.\nNOTE: If you make a release before enabling the GitHub repository in Zenodo, it will not appear in Zenodo! \nZenodo will automatically detect the release and it should appear on your Zenodo upload page: My dashboard &gt; Uploads.\nThis archive is assigned a unique Digital Object Identifier (DOI), making it a citable reference for your work. \n\nBefore submitting your work in a journal, make sure to link your data analysis repository to Zenodo, get a DOI, and cite it in your manuscript!"
  },
  {
    "objectID": "develop/practical_workshop.html#wrap-up",
    "href": "develop/practical_workshop.html#wrap-up",
    "title": "Practical material",
    "section": "Wrap up",
    "text": "Wrap up\nIn this small workshop, we have learned how to improve the FAIRability of your data, as well as organize and structure it in a way that will be much more useful in the future. These advantages do not serve yourself only, but your teammates, group leader, and the general scientific population! We hope that you found this workshop useful. If you would like to leave us some comments or suggestions, feel free to contact us."
  },
  {
    "objectID": "develop/07_metadata.html",
    "href": "develop/07_metadata.html",
    "title": "Metadata for NGS data",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n1. Learn what is metadata\n2. Learn what metadata is associated to NGS data 3. Learn sources for controlled vocabularies for NGS data\n\n\nMetadata is the behind-the-scenes information that makes sense of data. It‚Äôs the extra layer of details that help you understand what the data is all about. Imagine you have a book ‚Äì the words on the pages are the data, while the title, author, and table of contents are the metadata. In a similar way, metadata gives data context and structure. For NGS data, metadata includes information such as when and where the data was collected, what it represents, and how it was processed. In this section, we will explore what kind of relevant metadata is available for NGS data, and how to capture it in your Assays or Project folders.\n\n\n From ontotext.com\nImagine you have a batch of DNA sequences from different people. The raw sequences are like jigsaw puzzle pieces, and metadata is a cheat sheet that tells you which pieces fit where. It could include details like when and where the samples were collected, the lab procedures used, who did created them, and even the equipment involved, providing context and making sense of the who, what, when, where, and why.\n\n\n\n\n\n\nDefinition of metadata\n\n\n\nMetadata refers to data that provides information about other data. It describes various aspects of the data, such as its origin, structure, format, and context. Metadata is typically used to facilitate the organization, management, and interpretation of data, making it easier to locate, access, and understand. In essence, metadata adds valuable context and attributes to the primary data, enhancing its usability and ensuring efficient data management.\"\n\n\nLet‚Äôs think of an example that shows why metadata is extremelly important. Imagine you‚Äôre in a big lab with a plethora of different datasets all saved under generic folder names. Without metadata, it would be like searching for a needle in a haystack. You‚Äôd have folders labeled ‚ÄòExperiment123,‚Äô ‚ÄòDataBatch42,‚Äô and so on, but zero clues about what‚Äôs inside. With metadata, ‚ÄòExperiment123‚Äô is not just that anymore, but ‚ÄòDNA Sequencing of Human Cells, March 2023‚Äô. Providing relevant metadata converts data chaos into an organized repository, turning data exploration, interpretation, and insight extraction into a much easier journey. This is true not only for yourself or your lab, but to other researchers that might want to reuse your data. Collecting metadata from the very beginning of the research project will help tremendously to alleviate future efforts to understand the data. It can also help you make an organised collection of data, so that you do not lose any information regarding the data, or that you do not repeat an unnecessary experiment that someone else has done it before, but you could not find. That is, it helps you save money and time!\n\n\n\n\n\n\nBenefits of collecting proper metadata\n\n\n\n1. **Data Context and Interpretation**: Metadata provides crucial context to NGS data, offering insights into the experimental conditions, sample origins, and processing methods. This context is vital for understanding data variations, drawing accurate conclusions, and interpreting results correctly.\n2. **Data Discovery and Access**: With metadata, researchers can easily locate and access specific NGS datasets within large repositories. Details like sample identifiers, experimental parameters, and timestamps help researchers quickly identify relevant data for their analyses.\n3. **Reproducibility and Collaboration**: Metadata ensures that NGS experiments can be replicated and validated by others. By sharing comprehensive metadata, researchers enable colleagues to reproduce analyses, compare results, and collaborate effectively, bolstering the integrity of scientific findings.\n4. **Quality Control and Validation**: Metadata supports data quality assessment by allowing researchers to track the origin and handling of NGS data. It enables the identification of potential errors or biases, helping researchers validate the accuracy and reliability of their analyses.\n5. **Long-Term Data Preservation**: Properly documented metadata is essential for preserving NGS data over time. As research evolves, metadata ensures that future generations can understand and utilize archived NGS datasets, ensuring the continued impact of scientific discoveries.\n\n\n\n\n\nIn our previous lesson, we learnt about how to organize your data into different types of folders: Assays and Projects. Both of these folders contain a metadata.yml file and a README.md file. In this section, we will check what kind of information you should collect in each of these files.\n\n\nThe README.md file is a markdown file that allows you to write a long description of the data placed in a folder. Since it is a markdown file, you are able to write in rich text format (bold, italic, include links, etc) what is inside the folder, why it was created/collected, how and when. If it is an Assay folder, you could include the laboratory protocol used to generate the samples, images explaining the experiment design, a summary of the results of the experiment and any sort of comments that would help to understand the context of the experiment. On the other hand, a ‚ÄòProject‚Äô README file may contain a description of the project, what are its aims, why is it important, what ‚ÄòAssays‚Äô is it using, how to interpret the code notebooks, a summary of the results and, again, any sort of comments that would help to understand the project.\nHere is an example of a README file for a `Project`` folder:\n# NGS Analysis Project: Exploring Gene Expression in Human Tissues\n\n## Aims\n\nThis project aims to investigate gene expression patterns across various human tissues using Next Generation Sequencing (NGS) data. By analyzing the transcriptomes of different tissues, we seek to uncover tissue-specific gene expression profiles and identify potential markers associated with specific biological functions or diseases.\n\n## Why It's Important\n\nUnderstanding tissue-specific gene expression is crucial for deciphering the molecular basis of health and disease. Identifying genes that are uniquely expressed in certain tissues can provide insights into tissue function, development, and potential therapeutic targets. This project contributes to our broader understanding of human biology and has implications for personalized medicine and disease research.\n\n## Datasets\n\nWe have used internal datasets with IDs: RNA_humanSkin_20201030, RNA_humanBrain_20210102, RNA_humanLung_20220304.\n\nIn addition, we utilized publicly available NGS datasets from the GTEx (Genotype-Tissue Expression) project, which provides comprehensive RNA-seq data across multiple human tissues. These datasets offer a wealth of information on gene expression levels and isoform variations across diverse tissues, making them ideal for our analysis.\n\n## Summary of Results\n\nOur analysis revealed distinct gene expression patterns among different human tissues. We identified tissue-specific genes enriched in brain tissues, highlighting their potential roles in neurodevelopment and function. Additionally, we found a set of genes that exhibit consistent expression across a range of tissues, suggesting their fundamental importance in basic cellular processes.\n\nFurthermore, our differential expression analysis unveiled significant changes in gene expression between healthy and diseased tissues, shedding light on potential molecular factors underlying various diseases. Overall, this project underscores the power of NGS data in unraveling intricate gene expression networks and their implications for human health.\n\n---\n\nFor more details, refer to our [Jupyter Notebook](link-to-jupyter-notebook.ipynb) for the complete analysis pipeline and code.\n\n\n\nThe metadata file is a yml file, which is a text document that contains data formatted using a human-readable data format for data serialization. It is not mandatory that you use yml format, any other structured file format will work too, such as json files. We recommend using yml format because it is easily readable for humans, so non-coding people will have an easier time checking, writing or modifying the file if they need to.\n\n\n\nyaml file example\n\n\n\n\n\n\nThere is a ton of information you can collect regarding an NGS assay or a project. Some information fields are very general, such as author or date, while others are specific to the Assay or Project folder. Below, we will take a look at minimal information you should collect in each of the folders. But before that, we need to talk about controlled vocabularies/ontologies.\nImagine this scenario: a researcher in genomics us excited to explore various NGS datasets from different human tissue samples to study gene expression patterns. However, she encounters a significant hurdle: the tissue names used in the datasets are inconsistent and lack standardized terms. Some datasets refer to ‚Äúbrain,‚Äù while others use ‚Äúcerebral cortex‚Äù or ‚Äúcortical tissue.‚Äù This lack of controlled vocabularies for tissue names complicates her data integration efforts, requiring her to spend additional time curating and mapping tissue labels to establish meaningful cross-dataset comparisons.\nSo, what happened here? If the original creators of the NGS datasets had adopted a standardized vocabulary for tissue names, Dr.¬†Smith could have seamlessly integrated the data without the need for extensive curation. By employing a widely accepted tissue ontology, like the Uberon Ontology, dataset contributors could have consistently used predefined terms, such as ‚Äúbrain‚Äù or ‚Äúcerebral cortex.‚Äù This practice would have not only simplified data integration but also facilitated accurate cross-dataset comparisons and enabled more reliable downstream analyses.\nIn the context of NGS data, ontologies and controlled vocabularies play a pivotal role in clarifying and categorizing many concepts and information about your data. More examples are: the Gene Ontology (GO), which provides a shared vocabulary to describe gene functions, molecular processes, and cellular components, enhancing the consistency and comparability of NGS results, and Ensembl gene IDs, which are used to uniquely represent individual genes and provide a standardized way of referencing and accessing gene-related information across various species. By leveraging ontologies, researchers ensure that metadata fields capture specific details consistently across experiments, from sample sources and protocols to experimental conditions.\n\n\n\n\n\n\nDefinition of ontology\n\n\n\nAn ontology is a formal representation of knowledge that encompasses concepts, their attributes, and the relationships between them within a particular domain or subject area. It serves as a structured framework for organizing and categorizing information, facilitating the interpretation, sharing, and integration of knowledge across diverse applications and disciplines. Ontologies employ standardized vocabularies and define the semantics of terms, enabling effective communication and reasoning among humans and computer systems. They play a pivotal role in knowledge representation, data integration, and semantic interoperability, contributing to enhanced understanding, collaboration, and analysis within complex domains.\n\n\nThis standardization not only enhances data discoverability and interoperability but also empowers robust data analysis, accelerates knowledge sharing, and enables meaningful cross-study comparisons. In essence, ontologies serve as the universal translators of the scientific language, fostering a harmonious symphony of data interpretation and collaboration.\n\n\nHere you can find a list of suggestions for general metadata fields that can be used for both assays and project folders:\n\nTitle: A brief yet informative name for the dataset.\nAuthor(s): The individual(s) or organization responsible for creating the dataset. You can use your ORCID\nDate Created: The date when the dataset was originally generated or compiled. Use YYYY-MM-DD format!\nDate Modified: The date when the dataset was last updated or modified. Use YYYY-MM-DD format!\nObject ID: The project or assay ID for tracking and reference purposes.\nDescription: A short narrative explaining the content, purpose, and context.\nKeywords: A set of descriptive terms or phrases that capture the folder‚Äôs main topics and attributes.\nEthical and Legal Considerations: Information about ethical approvals, consent, and any legal restrictions.\nVersion: The version number or identifier for the folder, useful for tracking changes.\nRelated Publications: Links or references to any scientific publications associated with the folder. Try to add here the DOI!\nFunding Source: Details about the funding agency or source that supported the research and data generation.\nLicense: The type of license or terms of use associated with the dataset/project.\nContact Information: Contact details for individuals who can provide further information about the dataset/project.\n\n\n\n\nHere you will find a table with possible metadata fields that you can use to annotate and track your Assay folders:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nassay_ID\nIdentifier for the assay that is at least unique within the project\n&lt;Assay-ID\\&gt;_&lt;keyword\\&gt;_YYYYMMDD\nNA\nCHIP_Oct4_20200101\n\n\nassay_type\nThe type of experiment performed, eg ATAC-seq or seqFISH\nNA\nontology field- e.g. EFO or OBI\nChIPseq\n\n\nassay_subtype\nMore specific type or assay like bulk nascent RNAseq or single cell ATACseq\nNA\nontology field- e.g. EFO or OBI\nbulk ChIPseq\n\n\nowner\nOwner of the assay (who made the experiment?).\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nJose Romero\n\n\nplatform\nThe type of instrument used to perform the assay, eg Illumina HiSeq 4000 or Fluidigm C1 microfluidics platform\nNA\nontology field- e.g. EFO or OBI\nIllumina\n\n\nextraction_method\nTechnique used to extract the nucleic acid from the cell\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nlibrary_method\nTechnique used to amplify a cDNA library\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nexternal_accessions\nAccession numbers from external resources to which assay or protocol information was submitted\nNA\neg protocols.io, AE, GEO accession number, etc\nGSEXXXXX\n\n\nkeyword\nKeyword for easy identification\nwordWord\ncamelCase\nOct4ChIP\n\n\ndate\nDate of assay creation\nYYYYMMDD\nNA\n20200101\n\n\nnsamples\nNumber of samples analyzed in this assay\n&lt;integer\\&gt;\nNA\n9\n\n\nis_paired\nPaired fastq files or not\n&lt;single OR paired\\&gt;\nNA\nsingle\n\n\npipeline\nPipeline used to process data and version\nNA\nNA\nnf-core/chipseq -r 1.0\n\n\nstrandedness\nThe strandedness of the cDNA library\n&lt;+ OR - OR *\\&gt;\nNA\n*\n\n\nprocessed_by\nWho processed the data\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nSarah Lundregan\n\n\norganism\nOrganism origin\n&lt;Genus species\\&gt;\nTaxonomy name\nMus musculus\n\n\norigin\nIs internal or external (from a public resources) data\n&lt;internal OR external\\&gt;\nNA\ninternal\n\n\npath\nPath to files\n&lt;/path/to/file\\&gt;\nNA\nNA\n\n\nshort_desc\nShort description of the assay\nplain text\nNA\nOct4 ChIP after pERK activation\n\n\nELN_ID\nID of the experiment/assay in your Electronic Lab Notebook software, like labguru or benchling\nplain text\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\nThere is some information that will be specific to your samples. For example, which samples are treated, which are control, which tissue do they come from, which cell type, etc. In this case, it would be beneficial if you include all this information in the samplesheet.csv that describes the fastq files. Here is a list of possible metadata fields that you can use:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nsample\nName of the sample\nNA\nNA\ncontrol_rep1, treat_rep1\n\n\nfastq_1\nPath to fastq file 1\nNA\nNA\nAEG588A1_S1_L002_R1_001.fastq.gz\n\n\nfastq_2\nPath to paired fastq file, if it is a paired experiment\nNA\nNA\nAEG588A1_S1_L002_R2_001.fastq.gz\n\n\nstrandedness\nThe strandedness of the cDNA library\n&lt;unstranded OR forward OR reverse \\&gt;\nNA\nunstranded\n\n\ncondition\nVariable of interest of the experiment, such as \"control\", \"treatment\", etc\nwordWord\ncamelCase\ncontrol, treat1, treat2\n\n\ncell_type\nThe cell type(s) known or selected to be present in the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\ntissue\nThe tissue from which the sample was taken\nNA\nUberon\nNA\n\n\nsex\nThe biological/genetic sex of the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\ncell_line\nCell line of the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\norganism\nOrganism origin of the sample\n&lt;Genus species&gt;\nTaxonomy\nMus musculus\n\n\nreplicate\nReplicate number\n&lt;integer\\&gt;\nNA\n1\n\n\nbatch\nBatch information\nwordWord\ncamelCase\n1\n\n\ndisease\nAny diseases that may affect the sample\nNA\nDisease Ontology or MONDO\nNA\n\n\ndevelopmental_stage\nThe developmental stage of the sample\nNA\nNA\nNA\n\n\nsample_type\nThe type of the collected specimen, eg tissue biopsy, blood draw or throat swab\nNA\nNA\nNA\n\n\nstrain\nStrain of the species from which the sample was collected, if applicable\nNA\nontology field - e.g. NCBITaxonomy\nNA\n\n\ngenetic variation\nAny relevant genetic differences from the specimen or sample to the expected genomic information for this species, eg abnormal chromosome counts, major translocations or indels\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\nHere you will find a table with possible metadata fields that you can use to annotate and track your Project folders:\nIn development.\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nproject\nProject ID\n&lt;surname\\&gt;_et_al_2023\nNA\nproks_et_al_2023\n\n\nauthor\nOwner of the project\n&lt;First name\\&gt; &lt;Surname\\&gt;\nNA\nMartin Proks\n\n\ndate\nDate of creation\nYYYYMMDD\nNA\n20230101\n\n\ndescription\nShort description of the project\nPlain text\nNA\nThis is a project describing the effect of Oct4 perturbation after pERK activation\n\n\n\n\n\n\n\n\n\n\n\n\nThe information provided in this lesson is not at all exhaustive. There might be many more fields and controlled vocabularies that could be useful for your NGS data. We recommend that you take a look at the following sources for more information!\n\nTranscriptomics metadata standards and fields\nBionty: Biological ontologies for data scientists.\n\n::: {.callout-tip appearance=‚Äúsimple‚Äù} # ‚ÄúExercise: modify the metadata files in your cookiecutter templates‚Äù We have seen some examples of metadata for NGS data. It is time now to customize your cookiecutter templates and modify the metadata.yml files so that they fit your needs!\n1. Think about what kind of metadata you would like to include.\n2. Modify the `cookiecutter.json` file so that when you create a new folder template, all the metadata is filled accordingly.\n![cookiecutter_json_example](./images/cookiecutter_json.png)\n3. Modify the `metadata.yml` file so that it includes the metadata recorded by the `cookiecutter.json` file.\n![assay_metadata_example](./images/assay_metadata.png)\n4. Modify the `README.md` file so that it includes the short description recorded by the `cookiecutter.json` file.\n5. Git add, commit and push the changes of your template.\n6. Test your folders by using the command `cookiecutter &lt;URL to your cookiecutter repository in GitHub&gt;`\n\n\n\nIn this lesson we have learned about what metadata should be attached to your data in order to be reusable and understood in the future. Not only that, the metadata provided can help to process your samples adequately, and could even be useful for metadata studies. We have also briefly introduced some controlled vocabularies and sources for different fields, such as Disease Ontologies, Cell type ontologies, Organisms, etc. However, since this course is not aimed at enforcing the use of these vocabularies (that would be very complex), their implementation and use is up to you! In the next lesson we will learn how we can collect all the metadata information in each of the folders in order to make a database of assays and projects, allowing you to browse all your data so that they are always at hand!"
  },
  {
    "objectID": "develop/07_metadata.html#what-is-metadata-and-why-it-is-important",
    "href": "develop/07_metadata.html#what-is-metadata-and-why-it-is-important",
    "title": "Metadata for NGS data",
    "section": "",
    "text": "From ontotext.com\nImagine you have a batch of DNA sequences from different people. The raw sequences are like jigsaw puzzle pieces, and metadata is a cheat sheet that tells you which pieces fit where. It could include details like when and where the samples were collected, the lab procedures used, who did created them, and even the equipment involved, providing context and making sense of the who, what, when, where, and why.\n\n\n\n\n\n\nDefinition of metadata\n\n\n\nMetadata refers to data that provides information about other data. It describes various aspects of the data, such as its origin, structure, format, and context. Metadata is typically used to facilitate the organization, management, and interpretation of data, making it easier to locate, access, and understand. In essence, metadata adds valuable context and attributes to the primary data, enhancing its usability and ensuring efficient data management.\"\n\n\nLet‚Äôs think of an example that shows why metadata is extremelly important. Imagine you‚Äôre in a big lab with a plethora of different datasets all saved under generic folder names. Without metadata, it would be like searching for a needle in a haystack. You‚Äôd have folders labeled ‚ÄòExperiment123,‚Äô ‚ÄòDataBatch42,‚Äô and so on, but zero clues about what‚Äôs inside. With metadata, ‚ÄòExperiment123‚Äô is not just that anymore, but ‚ÄòDNA Sequencing of Human Cells, March 2023‚Äô. Providing relevant metadata converts data chaos into an organized repository, turning data exploration, interpretation, and insight extraction into a much easier journey. This is true not only for yourself or your lab, but to other researchers that might want to reuse your data. Collecting metadata from the very beginning of the research project will help tremendously to alleviate future efforts to understand the data. It can also help you make an organised collection of data, so that you do not lose any information regarding the data, or that you do not repeat an unnecessary experiment that someone else has done it before, but you could not find. That is, it helps you save money and time!\n\n\n\n\n\n\nBenefits of collecting proper metadata\n\n\n\n1. **Data Context and Interpretation**: Metadata provides crucial context to NGS data, offering insights into the experimental conditions, sample origins, and processing methods. This context is vital for understanding data variations, drawing accurate conclusions, and interpreting results correctly.\n2. **Data Discovery and Access**: With metadata, researchers can easily locate and access specific NGS datasets within large repositories. Details like sample identifiers, experimental parameters, and timestamps help researchers quickly identify relevant data for their analyses.\n3. **Reproducibility and Collaboration**: Metadata ensures that NGS experiments can be replicated and validated by others. By sharing comprehensive metadata, researchers enable colleagues to reproduce analyses, compare results, and collaborate effectively, bolstering the integrity of scientific findings.\n4. **Quality Control and Validation**: Metadata supports data quality assessment by allowing researchers to track the origin and handling of NGS data. It enables the identification of potential errors or biases, helping researchers validate the accuracy and reliability of their analyses.\n5. **Long-Term Data Preservation**: Properly documented metadata is essential for preserving NGS data over time. As research evolves, metadata ensures that future generations can understand and utilize archived NGS datasets, ensuring the continued impact of scientific discoveries."
  },
  {
    "objectID": "develop/07_metadata.html#how-to-collect-metadata-in-your-folders",
    "href": "develop/07_metadata.html#how-to-collect-metadata-in-your-folders",
    "title": "Metadata for NGS data",
    "section": "",
    "text": "In our previous lesson, we learnt about how to organize your data into different types of folders: Assays and Projects. Both of these folders contain a metadata.yml file and a README.md file. In this section, we will check what kind of information you should collect in each of these files.\n\n\nThe README.md file is a markdown file that allows you to write a long description of the data placed in a folder. Since it is a markdown file, you are able to write in rich text format (bold, italic, include links, etc) what is inside the folder, why it was created/collected, how and when. If it is an Assay folder, you could include the laboratory protocol used to generate the samples, images explaining the experiment design, a summary of the results of the experiment and any sort of comments that would help to understand the context of the experiment. On the other hand, a ‚ÄòProject‚Äô README file may contain a description of the project, what are its aims, why is it important, what ‚ÄòAssays‚Äô is it using, how to interpret the code notebooks, a summary of the results and, again, any sort of comments that would help to understand the project.\nHere is an example of a README file for a `Project`` folder:\n# NGS Analysis Project: Exploring Gene Expression in Human Tissues\n\n## Aims\n\nThis project aims to investigate gene expression patterns across various human tissues using Next Generation Sequencing (NGS) data. By analyzing the transcriptomes of different tissues, we seek to uncover tissue-specific gene expression profiles and identify potential markers associated with specific biological functions or diseases.\n\n## Why It's Important\n\nUnderstanding tissue-specific gene expression is crucial for deciphering the molecular basis of health and disease. Identifying genes that are uniquely expressed in certain tissues can provide insights into tissue function, development, and potential therapeutic targets. This project contributes to our broader understanding of human biology and has implications for personalized medicine and disease research.\n\n## Datasets\n\nWe have used internal datasets with IDs: RNA_humanSkin_20201030, RNA_humanBrain_20210102, RNA_humanLung_20220304.\n\nIn addition, we utilized publicly available NGS datasets from the GTEx (Genotype-Tissue Expression) project, which provides comprehensive RNA-seq data across multiple human tissues. These datasets offer a wealth of information on gene expression levels and isoform variations across diverse tissues, making them ideal for our analysis.\n\n## Summary of Results\n\nOur analysis revealed distinct gene expression patterns among different human tissues. We identified tissue-specific genes enriched in brain tissues, highlighting their potential roles in neurodevelopment and function. Additionally, we found a set of genes that exhibit consistent expression across a range of tissues, suggesting their fundamental importance in basic cellular processes.\n\nFurthermore, our differential expression analysis unveiled significant changes in gene expression between healthy and diseased tissues, shedding light on potential molecular factors underlying various diseases. Overall, this project underscores the power of NGS data in unraveling intricate gene expression networks and their implications for human health.\n\n---\n\nFor more details, refer to our [Jupyter Notebook](link-to-jupyter-notebook.ipynb) for the complete analysis pipeline and code.\n\n\n\nThe metadata file is a yml file, which is a text document that contains data formatted using a human-readable data format for data serialization. It is not mandatory that you use yml format, any other structured file format will work too, such as json files. We recommend using yml format because it is easily readable for humans, so non-coding people will have an easier time checking, writing or modifying the file if they need to.\n\n\n\nyaml file example"
  },
  {
    "objectID": "develop/07_metadata.html#metadata-fields",
    "href": "develop/07_metadata.html#metadata-fields",
    "title": "Metadata for NGS data",
    "section": "",
    "text": "There is a ton of information you can collect regarding an NGS assay or a project. Some information fields are very general, such as author or date, while others are specific to the Assay or Project folder. Below, we will take a look at minimal information you should collect in each of the folders. But before that, we need to talk about controlled vocabularies/ontologies.\nImagine this scenario: a researcher in genomics us excited to explore various NGS datasets from different human tissue samples to study gene expression patterns. However, she encounters a significant hurdle: the tissue names used in the datasets are inconsistent and lack standardized terms. Some datasets refer to ‚Äúbrain,‚Äù while others use ‚Äúcerebral cortex‚Äù or ‚Äúcortical tissue.‚Äù This lack of controlled vocabularies for tissue names complicates her data integration efforts, requiring her to spend additional time curating and mapping tissue labels to establish meaningful cross-dataset comparisons.\nSo, what happened here? If the original creators of the NGS datasets had adopted a standardized vocabulary for tissue names, Dr.¬†Smith could have seamlessly integrated the data without the need for extensive curation. By employing a widely accepted tissue ontology, like the Uberon Ontology, dataset contributors could have consistently used predefined terms, such as ‚Äúbrain‚Äù or ‚Äúcerebral cortex.‚Äù This practice would have not only simplified data integration but also facilitated accurate cross-dataset comparisons and enabled more reliable downstream analyses.\nIn the context of NGS data, ontologies and controlled vocabularies play a pivotal role in clarifying and categorizing many concepts and information about your data. More examples are: the Gene Ontology (GO), which provides a shared vocabulary to describe gene functions, molecular processes, and cellular components, enhancing the consistency and comparability of NGS results, and Ensembl gene IDs, which are used to uniquely represent individual genes and provide a standardized way of referencing and accessing gene-related information across various species. By leveraging ontologies, researchers ensure that metadata fields capture specific details consistently across experiments, from sample sources and protocols to experimental conditions.\n\n\n\n\n\n\nDefinition of ontology\n\n\n\nAn ontology is a formal representation of knowledge that encompasses concepts, their attributes, and the relationships between them within a particular domain or subject area. It serves as a structured framework for organizing and categorizing information, facilitating the interpretation, sharing, and integration of knowledge across diverse applications and disciplines. Ontologies employ standardized vocabularies and define the semantics of terms, enabling effective communication and reasoning among humans and computer systems. They play a pivotal role in knowledge representation, data integration, and semantic interoperability, contributing to enhanced understanding, collaboration, and analysis within complex domains.\n\n\nThis standardization not only enhances data discoverability and interoperability but also empowers robust data analysis, accelerates knowledge sharing, and enables meaningful cross-study comparisons. In essence, ontologies serve as the universal translators of the scientific language, fostering a harmonious symphony of data interpretation and collaboration.\n\n\nHere you can find a list of suggestions for general metadata fields that can be used for both assays and project folders:\n\nTitle: A brief yet informative name for the dataset.\nAuthor(s): The individual(s) or organization responsible for creating the dataset. You can use your ORCID\nDate Created: The date when the dataset was originally generated or compiled. Use YYYY-MM-DD format!\nDate Modified: The date when the dataset was last updated or modified. Use YYYY-MM-DD format!\nObject ID: The project or assay ID for tracking and reference purposes.\nDescription: A short narrative explaining the content, purpose, and context.\nKeywords: A set of descriptive terms or phrases that capture the folder‚Äôs main topics and attributes.\nEthical and Legal Considerations: Information about ethical approvals, consent, and any legal restrictions.\nVersion: The version number or identifier for the folder, useful for tracking changes.\nRelated Publications: Links or references to any scientific publications associated with the folder. Try to add here the DOI!\nFunding Source: Details about the funding agency or source that supported the research and data generation.\nLicense: The type of license or terms of use associated with the dataset/project.\nContact Information: Contact details for individuals who can provide further information about the dataset/project.\n\n\n\n\nHere you will find a table with possible metadata fields that you can use to annotate and track your Assay folders:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nassay_ID\nIdentifier for the assay that is at least unique within the project\n&lt;Assay-ID\\&gt;_&lt;keyword\\&gt;_YYYYMMDD\nNA\nCHIP_Oct4_20200101\n\n\nassay_type\nThe type of experiment performed, eg ATAC-seq or seqFISH\nNA\nontology field- e.g. EFO or OBI\nChIPseq\n\n\nassay_subtype\nMore specific type or assay like bulk nascent RNAseq or single cell ATACseq\nNA\nontology field- e.g. EFO or OBI\nbulk ChIPseq\n\n\nowner\nOwner of the assay (who made the experiment?).\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nJose Romero\n\n\nplatform\nThe type of instrument used to perform the assay, eg Illumina HiSeq 4000 or Fluidigm C1 microfluidics platform\nNA\nontology field- e.g. EFO or OBI\nIllumina\n\n\nextraction_method\nTechnique used to extract the nucleic acid from the cell\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nlibrary_method\nTechnique used to amplify a cDNA library\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nexternal_accessions\nAccession numbers from external resources to which assay or protocol information was submitted\nNA\neg protocols.io, AE, GEO accession number, etc\nGSEXXXXX\n\n\nkeyword\nKeyword for easy identification\nwordWord\ncamelCase\nOct4ChIP\n\n\ndate\nDate of assay creation\nYYYYMMDD\nNA\n20200101\n\n\nnsamples\nNumber of samples analyzed in this assay\n&lt;integer\\&gt;\nNA\n9\n\n\nis_paired\nPaired fastq files or not\n&lt;single OR paired\\&gt;\nNA\nsingle\n\n\npipeline\nPipeline used to process data and version\nNA\nNA\nnf-core/chipseq -r 1.0\n\n\nstrandedness\nThe strandedness of the cDNA library\n&lt;+ OR - OR *\\&gt;\nNA\n*\n\n\nprocessed_by\nWho processed the data\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nSarah Lundregan\n\n\norganism\nOrganism origin\n&lt;Genus species\\&gt;\nTaxonomy name\nMus musculus\n\n\norigin\nIs internal or external (from a public resources) data\n&lt;internal OR external\\&gt;\nNA\ninternal\n\n\npath\nPath to files\n&lt;/path/to/file\\&gt;\nNA\nNA\n\n\nshort_desc\nShort description of the assay\nplain text\nNA\nOct4 ChIP after pERK activation\n\n\nELN_ID\nID of the experiment/assay in your Electronic Lab Notebook software, like labguru or benchling\nplain text\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\nThere is some information that will be specific to your samples. For example, which samples are treated, which are control, which tissue do they come from, which cell type, etc. In this case, it would be beneficial if you include all this information in the samplesheet.csv that describes the fastq files. Here is a list of possible metadata fields that you can use:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nsample\nName of the sample\nNA\nNA\ncontrol_rep1, treat_rep1\n\n\nfastq_1\nPath to fastq file 1\nNA\nNA\nAEG588A1_S1_L002_R1_001.fastq.gz\n\n\nfastq_2\nPath to paired fastq file, if it is a paired experiment\nNA\nNA\nAEG588A1_S1_L002_R2_001.fastq.gz\n\n\nstrandedness\nThe strandedness of the cDNA library\n&lt;unstranded OR forward OR reverse \\&gt;\nNA\nunstranded\n\n\ncondition\nVariable of interest of the experiment, such as \"control\", \"treatment\", etc\nwordWord\ncamelCase\ncontrol, treat1, treat2\n\n\ncell_type\nThe cell type(s) known or selected to be present in the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\ntissue\nThe tissue from which the sample was taken\nNA\nUberon\nNA\n\n\nsex\nThe biological/genetic sex of the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\ncell_line\nCell line of the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\norganism\nOrganism origin of the sample\n&lt;Genus species&gt;\nTaxonomy\nMus musculus\n\n\nreplicate\nReplicate number\n&lt;integer\\&gt;\nNA\n1\n\n\nbatch\nBatch information\nwordWord\ncamelCase\n1\n\n\ndisease\nAny diseases that may affect the sample\nNA\nDisease Ontology or MONDO\nNA\n\n\ndevelopmental_stage\nThe developmental stage of the sample\nNA\nNA\nNA\n\n\nsample_type\nThe type of the collected specimen, eg tissue biopsy, blood draw or throat swab\nNA\nNA\nNA\n\n\nstrain\nStrain of the species from which the sample was collected, if applicable\nNA\nontology field - e.g. NCBITaxonomy\nNA\n\n\ngenetic variation\nAny relevant genetic differences from the specimen or sample to the expected genomic information for this species, eg abnormal chromosome counts, major translocations or indels\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\nHere you will find a table with possible metadata fields that you can use to annotate and track your Project folders:\nIn development.\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nproject\nProject ID\n&lt;surname\\&gt;_et_al_2023\nNA\nproks_et_al_2023\n\n\nauthor\nOwner of the project\n&lt;First name\\&gt; &lt;Surname\\&gt;\nNA\nMartin Proks\n\n\ndate\nDate of creation\nYYYYMMDD\nNA\n20230101\n\n\ndescription\nShort description of the project\nPlain text\nNA\nThis is a project describing the effect of Oct4 perturbation after pERK activation"
  },
  {
    "objectID": "develop/07_metadata.html#more-info",
    "href": "develop/07_metadata.html#more-info",
    "title": "Metadata for NGS data",
    "section": "",
    "text": "The information provided in this lesson is not at all exhaustive. There might be many more fields and controlled vocabularies that could be useful for your NGS data. We recommend that you take a look at the following sources for more information!\n\nTranscriptomics metadata standards and fields\nBionty: Biological ontologies for data scientists.\n\n::: {.callout-tip appearance=‚Äúsimple‚Äù} # ‚ÄúExercise: modify the metadata files in your cookiecutter templates‚Äù We have seen some examples of metadata for NGS data. It is time now to customize your cookiecutter templates and modify the metadata.yml files so that they fit your needs!\n1. Think about what kind of metadata you would like to include.\n2. Modify the `cookiecutter.json` file so that when you create a new folder template, all the metadata is filled accordingly.\n![cookiecutter_json_example](./images/cookiecutter_json.png)\n3. Modify the `metadata.yml` file so that it includes the metadata recorded by the `cookiecutter.json` file.\n![assay_metadata_example](./images/assay_metadata.png)\n4. Modify the `README.md` file so that it includes the short description recorded by the `cookiecutter.json` file.\n5. Git add, commit and push the changes of your template.\n6. Test your folders by using the command `cookiecutter &lt;URL to your cookiecutter repository in GitHub&gt;`"
  },
  {
    "objectID": "develop/07_metadata.html#wrap-up",
    "href": "develop/07_metadata.html#wrap-up",
    "title": "Metadata for NGS data",
    "section": "",
    "text": "In this lesson we have learned about what metadata should be attached to your data in order to be reusable and understood in the future. Not only that, the metadata provided can help to process your samples adequately, and could even be useful for metadata studies. We have also briefly introduced some controlled vocabularies and sources for different fields, such as Disease Ontologies, Cell type ontologies, Organisms, etc. However, since this course is not aimed at enforcing the use of these vocabularies (that would be very complex), their implementation and use is up to you! In the next lesson we will learn how we can collect all the metadata information in each of the folders in order to make a database of assays and projects, allowing you to browse all your data so that they are always at hand!"
  },
  {
    "objectID": "develop/contributors.html",
    "href": "develop/contributors.html",
    "title": "Practical material",
    "section": "",
    "text": "Alba Refoyo Martinez :custom-orcid::simple-github:\nJose Alejandro Romero Herrera :custom-orcid: :simple-github:\n\n\n\n\nCopyrightCC-BY-SA 4.0 license"
  },
  {
    "objectID": "develop/contributors.html#credit-table",
    "href": "develop/contributors.html#credit-table",
    "title": "RDM for NGS",
    "section": "CRediT table",
    "text": "CRediT table\n\n\n\nCRediT role\nInitials\n\n\n\n\nConceptualization\nJARH\n\n\nData curation\nJARH\n\n\nFormal Analysis\nJARH\n\n\nFunding acquisition\nJARH\n\n\nInvestigation\nJARH\n\n\nMethodology\nJARH\n\n\nProject administration\nJARH\n\n\nResources\nJARH\n\n\nSoftware\nJARH\n\n\nSupervision\nJARH\n\n\nValidation\nJARH\n\n\nVisualization\nJARH\n\n\nWriting - original draft\nJARH\n\n\nWriting - review & editing\nJARH"
  },
  {
    "objectID": "develop/08_database.html",
    "href": "develop/08_database.html",
    "title": "Creating a data catalog",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n1. Learn how to create a database based on your metadata files 2. Create and use a catalogue browser to explore your data files\n\n\n\n\nSQLite is a lightweight and self-contained relational database management system known for its simplicity and efficiency. Unlike traditional databases, SQLite doesn‚Äôt require a separate server to operate, making it a convenient choice for smaller-scale applications and scenarios where resource usage needs to be minimal, like our case! It excels in tasks that involve storing and retrieving structured data, offering a reliable solution for managing information such as the metadata collected in our experiments.\nAll the information saved in the metadata.yml files can be recorded in a SQLite database with a script like this:\n!!! success ‚ÄúWhy create a SQLite database instead of a tsv file?‚Äù\nCreating a SQLite database offers several benefits over saving data in a tabular format:\n\n1. **Efficient Querying**: SQLite databases are optimized for querying and retrieving data. Complex queries and filtering operations are faster and more efficient, making it easier to extract specific information from the dataset.\n2. **Structured Organization**: Databases provide a structured and organized way to store data. Tables, relationships, and indices ensure that data remains well-organized and easily accessible.\n3. **Data Integrity**: SQLite databases enforce data integrity through constraints and validations. This helps prevent errors and inconsistencies in the data, ensuring high-quality and reliable information.\n4. **Concurrency and Multi-User Support**: SQLite databases support concurrent read access from multiple users, ensuring that data remains accessible while maintaining data integrity.\n5. **Scalability**: As datasets grow, SQLite databases continue to perform efficiently. They can handle larger datasets without significant degradation in performance.\n6. **Modularity and Portability**: Databases are self-contained and modular, meaning that a single database file can contain multiple tables, simplifying data distribution.\n7. **Security and Access Control**: SQLite databases offer security features, such as password protection and encryption, to safeguard sensitive data. Access to specific tables or data can be controlled through user roles and permissions.\n8. **Indexing**: SQLite databases support indexing, which significantly speeds up data retrieval based on specific columns. This is especially advantageous for large datasets.\n9. **Data Relationships**: Databases allow the establishment of relationships between tables, facilitating the storage of complex and interconnected data. This could be very relevant if you would like to connect information about projects, assays and samples.\n\n\n\nDesigning a catalog browser for your NGS database can be accomplished using tools like Rshiny or a Panel python app. These frameworks offer user-friendly interfaces for navigating and interacting with your database, providing features for dynamic search, filtering, and visualization. They allow for an accessible and efficient way to explore the contents of your database, which will be great for your teammates and your group leader.\nBelow we will see how to create a simple browser tool using Rshiny from both a tsv file and a SQLite database.\n\n\nlibrary(shiny)\nlibrary(DT)\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"TSV File Viewer\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Choose a TSV file\", accept = c(\".tsv\"))\n    ),\n    \n    mainPanel(\n      DTOutput(\"table\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output) {\n  \n  data &lt;- reactive({\n    req(input$file)\n    read.delim(input$file$datapath, sep = \"\\t\")\n  })\n  \n  output$table &lt;- renderDT({\n    datatable(data())\n  })\n}\n\n# Run the app\nshinyApp(ui, server)\n\n\n\nlibrary(shiny)\nlibrary(DBI)\nlibrary(DT)\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"SQLite Database Viewer\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"db_file\", \"Choose an SQLite Database\", accept = c(\".sqlite\")),\n      textInput(\"table_name\", \"Enter Table Name:\", value = \"\"),\n      actionButton(\"load_button\", \"Load Table\")\n    ),\n    \n    mainPanel(\n      DTOutput(\"table\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output, session) {\n  \n  con &lt;- reactive({\n    if (!is.null(input$db_file)) {\n      dbConnect(SQLite(), input$db_file$datapath)\n    }\n  })\n  \n  data &lt;- reactive({\n    req(input$load_button &gt; 0, input$table_name, con())\n    query &lt;- glue::glue_sql(\"SELECT * FROM {dbQuoteIdentifier(con(), input$table_name)}\")\n    dbGetQuery(con(), query)\n  })\n  \n  output$table &lt;- renderDT({\n    datatable(data())\n  })\n  \n  observeEvent(input$load_button, {\n    output$table &lt;- renderDT({\n      datatable(data())\n    })\n  })\n  \n  # Disconnect from the database when app closes\n  observe({\n    on.exit(dbDisconnect(con()), add = TRUE)\n  })\n}\n\n# Run the app\nshinyApp(ui, server)\n\n\n\nBelow we have added an example of a SQLite database catalog created by the Brickman Lab at the Center for Stem Cell Medicine. Simple, but effective! As an additional feature, the Brickman lab has added the option to open the metadata.yml file whenever you click on a data row, allowing you to see the full extent of the metadata for that Assay.\n\n\nVideo\ntype:video\n\n\n\n\n\n\nIf you would like to make a more complex catalog browser, here are some ideas!\n\nAdd a tab or an option where you can create an Assay or Project folder interactively from there as well as fill up metadata .\nModify and correct existing entries\nVisualize an analysed single cell RNAseq dataset by opening Cirrocumulus session.\n\nNonetheless, implementing these ideas are beyond the scope of this course, it would be very complicated to implement!\n!!! question ‚ÄúExercise: create a catalog of your NGS folders‚Äù\n1. Create a folder call `Assays`\n2. Under that folder, make three new `Assay` folders from your cookiecutter template\n3. Run the script above with R to create a database on a tsv file (or create your own with python). Modify the `folder_path` variable so it matches the path tot the folder `Assays`. The table will be written under the same `folder_path`.\n4. Visualize your `Assays` table with Excel\n\n\n\nThis lesson has been a very practical one. We have seen how useful your metadata can be! By collecting all the metadata in your folders, you are able to create a catalog that can be interactively explored using a Rshiny or Python panel tool. This allows you, your workmates and your group leader to keep track of all the experiments performed at the lab, making sure they will not be lost or forgotten. You will be able to see who made them, when and other interesting features that might help you understand the data at hand. In the next lesson, we will learn how to use version control tools in order to keep track of your data analyses and results using Git and GitHub."
  },
  {
    "objectID": "develop/08_database.html#what-is-a-database",
    "href": "develop/08_database.html#what-is-a-database",
    "title": "Creating a data catalog",
    "section": "",
    "text": "A database is a structured repository that stores, manages, and retrieves information. Databases are the basis of efficient data organization, ensuring data integrity and accessibility. You could store your database in something as simple as a table in tabular format (like excel, tsv, csv), although it would be more appropriate to use a DataBase Management System (DBMS) like SQLite. A DBMS would allow you to store data efficiently, more securely and be able to make fast queries of data. That said, it is very possible that you do not need all these features, and a browsable table would be enough! It all depends on the complexity and number of experiments and folders you have. We will take a look at both possibilities, always trying to make it as simple as possible.\n\n\nSince all our metadata files are structured in the same way, it would be very simple to construct a table by recursively going through your main Assays or Projects folder with an R or python script, and saving it as database_YYYYMMDD.tsv. We would recommend to use tabs instead of commas as your separator, due to the fact that some fields (like short description, or lists of keywords, for example) may contain commas.\nFor example, consider this Assays folder:\nAssays/\n‚îú‚îÄ‚îÄ CHIP_oct4_20200101/\n‚îú‚îÄ‚îÄ RNA_oct4_20200101/\n‚îú‚îÄ‚îÄ CHIP_med1_20190204/\n‚îú‚îÄ‚îÄ SCR_humanSkin_20210302/\n‚îî‚îÄ‚îÄ SCR_humanBrain_20220610/\nYou can use an R script like this one to create a tsv table:\nlibrary(yaml)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Function to recursively fetch metadata.yml files\nget_metadata &lt;- function(folder_path) {\n  file_list &lt;- list.files(path = folder_path, pattern = \"metadata\\\\.yml$\", recursive = TRUE, full.names = TRUE)\n  metadata_list &lt;- lapply(file_list, yaml::yaml.load_file)\n  return(metadata_list)\n}\n\n# Specify the folder path\nfolder_path &lt;- \"/path/to/your/folder\"\n\n# Fetch metadata from the specified folder\nmetadata &lt;- get_metadata(folder_path)\n\n# Convert metadata to a data frame\nmetadata_df &lt;- data.frame(matrix(unlist(metadata), ncol = length(metadata), byrow = TRUE))\ncolnames(metadata_df) &lt;- names(metadata[[1]])\n\n# Save the data frame as a TSV file\noutput_file &lt;- paste0(\"database_\", format(Sys.Date(), \"%Y%m%d\"), \".tsv\")\nwrite.table(metadata_df, file = output_file, sep = \"\\t\", quote = FALSE, row.names = FALSE)\n\n# Print confirmation message\ncat(\"Database saved as\", output_file, \"\\n\")\nYour database table should look like this:\n{{ read_table(‚Äò./assets/database_example_20230810.tsv‚Äô) }}\nUsing a tabular table is a very simple way to create a catalog of all your projects and assays, making sure your lab can keep track of all the experiments ever performed.\n\n\n\nSQLite is a lightweight and self-contained relational database management system known for its simplicity and efficiency. Unlike traditional databases, SQLite doesn‚Äôt require a separate server to operate, making it a convenient choice for smaller-scale applications and scenarios where resource usage needs to be minimal, like our case! It excels in tasks that involve storing and retrieving structured data, offering a reliable solution for managing information such as the metadata collected in our experiments.\nAll the information saved in the metadata.yml files can be recorded in a SQLite database with a script like this:\nlibrary(yaml)\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Function to recursively fetch metadata.yml files\nget_metadata &lt;- function(folder_path) {\n  file_list &lt;- list.files(path = folder_path, pattern = \"metadata\\\\.yml$\", recursive = TRUE, full.names = TRUE)\n  metadata_list &lt;- lapply(file_list, yaml::yaml.load_file)\n  return(metadata_list)\n}\n\n# Specify the folder path\nfolder_path &lt;- \"/path/to/your/folder\"\n\n# Fetch metadata from the specified folder\nmetadata &lt;- get_metadata(folder_path)\n\n# Convert metadata to a data frame\nmetadata_df &lt;- data.frame(matrix(unlist(metadata), ncol = length(metadata), byrow = TRUE))\ncolnames(metadata_df) &lt;- names(metadata[[1]])\n\n# Create an SQLite database and insert data\ndb_file &lt;- paste0(\"database_\", format(Sys.Date(), \"%Y%m%d\"), \".sqlite\")\ncon &lt;- dbConnect(SQLite(), db_file)\n\ndbWriteTable(con, \"metadata\", metadata_df, row.names = FALSE)\n\n# Print confirmation message\ncat(\"Database saved as\", db_file, \"\\n\")\n\n# Close the database connection\ndbDisconnect(con)\n!!! success ‚ÄúWhy create a SQLite database instead of a tsv file?‚Äù\nCreating a SQLite database offers several benefits over saving data in a tabular format:\n\n1. **Efficient Querying**: SQLite databases are optimized for querying and retrieving data. Complex queries and filtering operations are faster and more efficient, making it easier to extract specific information from the dataset.\n2. **Structured Organization**: Databases provide a structured and organized way to store data. Tables, relationships, and indices ensure that data remains well-organized and easily accessible.\n3. **Data Integrity**: SQLite databases enforce data integrity through constraints and validations. This helps prevent errors and inconsistencies in the data, ensuring high-quality and reliable information.\n4. **Concurrency and Multi-User Support**: SQLite databases support concurrent read access from multiple users, ensuring that data remains accessible while maintaining data integrity.\n5. **Scalability**: As datasets grow, SQLite databases continue to perform efficiently. They can handle larger datasets without significant degradation in performance.\n6. **Modularity and Portability**: Databases are self-contained and modular, meaning that a single database file can contain multiple tables, simplifying data distribution.\n7. **Security and Access Control**: SQLite databases offer security features, such as password protection and encryption, to safeguard sensitive data. Access to specific tables or data can be controlled through user roles and permissions.\n8. **Indexing**: SQLite databases support indexing, which significantly speeds up data retrieval based on specific columns. This is especially advantageous for large datasets.\n9. **Data Relationships**: Databases allow the establishment of relationships between tables, facilitating the storage of complex and interconnected data. This could be very relevant if you would like to connect information about projects, assays and samples.",
    "crumbs": [
      "Course material",
      "Creating a data catalog"
    ]
  },
  {
    "objectID": "develop/08_database.html#making-a-catalog-browser",
    "href": "develop/08_database.html#making-a-catalog-browser",
    "title": "Creating a data catalog",
    "section": "",
    "text": "Designing a catalog browser for your NGS database can be accomplished using tools like Rshiny or a Panel python app. These frameworks offer user-friendly interfaces for navigating and interacting with your database, providing features for dynamic search, filtering, and visualization. They allow for an accessible and efficient way to explore the contents of your database, which will be great for your teammates and your group leader.\nBelow we will see how to create a simple browser tool using Rshiny from both a tsv file and a SQLite database.\n\n\nlibrary(shiny)\nlibrary(DT)\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"TSV File Viewer\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Choose a TSV file\", accept = c(\".tsv\"))\n    ),\n    \n    mainPanel(\n      DTOutput(\"table\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output) {\n  \n  data &lt;- reactive({\n    req(input$file)\n    read.delim(input$file$datapath, sep = \"\\t\")\n  })\n  \n  output$table &lt;- renderDT({\n    datatable(data())\n  })\n}\n\n# Run the app\nshinyApp(ui, server)\n\n\n\nlibrary(shiny)\nlibrary(DBI)\nlibrary(DT)\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"SQLite Database Viewer\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"db_file\", \"Choose an SQLite Database\", accept = c(\".sqlite\")),\n      textInput(\"table_name\", \"Enter Table Name:\", value = \"\"),\n      actionButton(\"load_button\", \"Load Table\")\n    ),\n    \n    mainPanel(\n      DTOutput(\"table\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output, session) {\n  \n  con &lt;- reactive({\n    if (!is.null(input$db_file)) {\n      dbConnect(SQLite(), input$db_file$datapath)\n    }\n  })\n  \n  data &lt;- reactive({\n    req(input$load_button &gt; 0, input$table_name, con())\n    query &lt;- glue::glue_sql(\"SELECT * FROM {dbQuoteIdentifier(con(), input$table_name)}\")\n    dbGetQuery(con(), query)\n  })\n  \n  output$table &lt;- renderDT({\n    datatable(data())\n  })\n  \n  observeEvent(input$load_button, {\n    output$table &lt;- renderDT({\n      datatable(data())\n    })\n  })\n  \n  # Disconnect from the database when app closes\n  observe({\n    on.exit(dbDisconnect(con()), add = TRUE)\n  })\n}\n\n# Run the app\nshinyApp(ui, server)\n\n\n\nBelow we have added an example of a SQLite database catalog created by the Brickman Lab at the Center for Stem Cell Medicine. Simple, but effective! As an additional feature, the Brickman lab has added the option to open the metadata.yml file whenever you click on a data row, allowing you to see the full extent of the metadata for that Assay.\n\n\nVideo\ntype:video"
  },
  {
    "objectID": "develop/08_database.html#future-ideas",
    "href": "develop/08_database.html#future-ideas",
    "title": "Creating a data catalog",
    "section": "",
    "text": "If you would like to make a more complex catalog browser, here are some ideas!\n\nAdd a tab or an option where you can create an Assay or Project folder interactively from there as well as fill up metadata .\nModify and correct existing entries\nVisualize an analysed single cell RNAseq dataset by opening Cirrocumulus session.\n\nNonetheless, implementing these ideas are beyond the scope of this course, it would be very complicated to implement!\n!!! question ‚ÄúExercise: create a catalog of your NGS folders‚Äù\n1. Create a folder call `Assays`\n2. Under that folder, make three new `Assay` folders from your cookiecutter template\n3. Run the script above with R to create a database on a tsv file (or create your own with python). Modify the `folder_path` variable so it matches the path tot the folder `Assays`. The table will be written under the same `folder_path`.\n4. Visualize your `Assays` table with Excel"
  },
  {
    "objectID": "develop/08_database.html#wrap-up",
    "href": "develop/08_database.html#wrap-up",
    "title": "Creating a data catalog",
    "section": "",
    "text": "This lesson has been a very practical one. We have seen how useful your metadata can be! By collecting all the metadata in your folders, you are able to create a catalog that can be interactively explored using a Rshiny or Python panel tool. This allows you, your workmates and your group leader to keep track of all the experiments performed at the lab, making sure they will not be lost or forgotten. You will be able to see who made them, when and other interesting features that might help you understand the data at hand. In the next lesson, we will learn how to use version control tools in order to keep track of your data analyses and results using Git and GitHub."
  },
  {
    "objectID": "develop/09_version_control.html",
    "href": "develop/09_version_control.html",
    "title": "Data analysis version control",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n1. Learn what is version control and why it is important 2. Introduce you to Git and Github repositories 3. Make your own repositories 4. Make a Github page to show off your data analysis reports\n\n\nIn this lesson, we will explore the concept of version control and its role in modern research and development workflows. We‚Äôll introduce Git, a widely adopted version control system that allows you to systematically track changes in your work. Additionally, we will check GitHub, a collaborative platform for hosting Git repositories, enabling you to share your work with others and increase the visibility of your projects. By the end of this lesson, you‚Äôll have a introductory understanding of version control, the ability to create Git repositories, and the skills to build a GitHub page to showcase your data analysis.\n\n\nVersion control is a systematic approach to tracking changes made to a project over time. It provides a structured means of documenting alterations, allowing you to revisit and understand the evolution of your work. In research data management and data analytics, version control is very important and gives you a lot of advantages.\n!!! success ‚ÄúAdvantages of using version control‚Äù\n1. **Document Progress**: Version control keeps a detailed history of changes, making it easier to understand how a project has developed, what modifications were made, and by whom.\n2. **Ensure Data Integrity**: It safeguards data by preventing accidental overwrites or deletions. Each change is tracked, enabling easy recovery in case of errors.\n3. **Facilitate Collaboration**: In collaborative research, version control enables multiple team members to work simultaneously on a project without conflicts. Changes can be merged seamlessly.\n4. **Reproducibility**: It enhances reproducibility by preserving the exact state of your project at any point in time. This is invaluable for validating research findings and data analysis.\n5. **Branching and Experimentation**: Version control allows for branching, where you can create alternative versions of your project for experimentation without affecting the main branch.\n6. **Global Accessibility**: Platforms like GitHub provide global visibility, allowing researchers to share their work, receive feedback, and contribute to open science.\n\n\n\n!!! warning\nIn this section we will only talk briefly about what is Git and Github. Explaining how git works is beyond the scope of this course. If you want to know more, please check out our [course](https://heads.ku.dk/course/git_github/)! You can also check [GitHub documentation](https://docs.github.com/get-started), which cover all the basics to work with Git and GitHub.\n\n\nGit is a distributed version control system that enables developers and researchers to efficiently manage their project‚Äôs history, collaborate seamlessly, and ensure data integrity. At its core, Git operates through the following principles and mechanisms:\n\nLocal Repository: Each user maintains a local repository on their computer, allowing them to work on a project independently. This local repository stores the complete project history.\nSnapshots, Not Files: Git does not track individual file changes but rather captures snapshots of the entire project at different points in time. This ‚Äòsnapshot‚Äô approach ensures data consistency.\nCommits: Users create ‚Äòcommits,‚Äô which are snapshots of the project at a specific moment. Commits record changes made to files, along with a commit message explaining the modifications.\nBranching: Git supports branching, enabling users to create separate lines of development. Branches are useful for experimenting with new features or fixing bugs without affecting the main project.\nMerging: Changes made in one branch can be merged into another, allowing for the incorporation of new features or bug fixes back into the main project. Git ensures a smooth merging process.\nDistributed Architecture: Git‚Äôs distributed nature means that each user‚Äôs local repository is a complete copy of the project, including its entire history. This enables offline work and ensures data redundancy.\nRemote Repositories: Git allows users to connect and synchronize their local repositories with remote repositories hosted on platforms like GitHub. Remote repositories facilitate collaboration and provide a central hub for project sharing.\nPush and Pull: Users ‚Äòpush‚Äô their local changes to a remote repository to share with others. Conversely, they ‚Äòpull‚Äô changes made by others into their local repository to stay up-to-date.\nConflict Resolution: In cases of conflicting changes, Git provides tools to resolve conflicts manually, ensuring that data integrity is maintained during collaboration.\nVersioning and Tagging: Git offers versioning and tagging capabilities, allowing users to mark specific points in history, such as major releases or significant milestones.\n\n\n\n\nOn the other hand, GitHub is a web-based platform that enhances Git‚Äôs capabilities by providing a collaborative and centralized hub for hosting Git repositories. It offers several key functionalities, such as tracking issues, security features to safeguard your repos, and GitHub Pages that allows you to create websites to showcase your projects. We will see more about Github Pages in the section below. In addition, you can set your repository as private until you are ready to publish your work!\nIf you do not have an account in Github already, we recommend you to do one now! There are other alternatives to GitHub, such as BitBucket and GitLab, Some features might be different, so we will stick to Github for the rest of the lesson!\n\n\n\n\nWe will show you two ways to make your Project folder into a Git repository\n!!! warning ‚ÄúWhat about my Assay folder?‚Äù\nAn assay folder contains very big files that are not suitable for version control, at least in GitHub. We recommend that you deposit the data into a domain specific archive such as [GEO](https://www.ncbi.nlm.nih.gov/geo/) or [ArrayExpress/Annotare](https://www.ebi.ac.uk/fg/annotare/login/). Even [Zenodo](https://zenodo.org/) would be a better option in this case. We will look at them in the [next session](./10_repos.qmd).\n\nRemember to link the original dataset with the data analysis repository!\n\n\nUsing git init is probably the right choice for you if you have created a Project folder using the cookiecutter template we saw in the previous lesson.\nThe git init command should only be run once, even if other collaborators share will the project!\n\nFirst, initialize the repository (git init) and make at least one commit (git add and git commit).\nOnce you have initialized the repository, create a remote repository in GitHub\nThen, add the remote URL to your local git repository with git remote add origin \\&lt;URL\\&gt;. This stores the remote URL under a more human-friendly name, origin.\nShape your history into at least one commit by using git add to stage the existing files, and git commit to make the snapshot.\nOnce you have at least one commit, you can push to the remote and set up the tracking relationship for good with git push -u origin master.\n\n\n\n\nIf the repository already exists on a remote, you would choose to git clone and not git init. On the other hand, if you create a remote repository first with the intent of moving your project to it later, you may have a few other steps to follow. If there are no commits in the remote repository, you can follow the steps above for git init. If there are commits and files in the remote repository but you would still like it to contain your project files, git clone that repository. Then, move the project‚Äôs files into that cloned repository. git add, git commit, and git push to create a history that makes sense for the beginning of your project. Then, your team can interact with the repository without git init again.\n!!! tip ‚ÄúTips to write good commit messages‚Äù\nIf you would like to know more about Git commits and the best way to make clear git messages, check out [this post](https://www.conventionalcommits.org/en/v1.0.0/)!\n\n\n\n\nOnce you have created your repository (and put it in GitHub), you have now the opportunity to add your data analysis reports that you created, in either Jupyter Notebooks, Rmarkdowns or html reports, in a GitHub Page website. Creating a GitHub page is very simple, and we really recommend that you follow the nice tutorial that GitHub as put for you.\nThere are many different ways to create your webpages. We recommend using Mkdocs and Mkdocs materials as a framework to create a nice webpage in a simple manner. The folder templates that we used as an example in lesson 06 already contain everything you need to start a webpage. Nonetheless, you will need to understand the basics of MkDocs and MkDocs materials to design a webpage to your liking. MkDocs is a static webpage generator that is very easy to use, while MkDocs materials is an extension of the tool that gives you many more options to customize your website. Check out their webpages to get started!\n\n\n\nIn this section we will showcase a full example about how to setup Git, MkDocs and a github account so you can do it yourself!\n\n\nThere are several tools and softwares that you need to install. First you will need pip and install the following packages using pip in the command line:\npip install cookiecutter # cookiecutter to create folder templates\npip install cruft # cruft is used to version control your templates\npip install mkdocs # mkdocs to create your webpages\npip install mkdocs-material # mkdocs extension to customize your templates\npip install mkdocs-video # mkdocs extension to add videos or embed internet videos, like youtube, to your webpages\npip install mkdocs-bibtex # mkdocs extension to add references in your text from a bib file\npip install neoteroi-mkdocs # mkdocs extension to create author cards\npip install mkdocs-minify-plugin # mkdocs extension to minimize the html code created by mkdocs\npip install mkdocs-git-revision-date-localized-plugin  # mkdocs extension to show \"last updated\" date of your webpage\npip install mkdocs-jupyter # mkdocs extension to include jupyter notebooks without needing to convert them\npip install mkdocs-table-reader-plugin # mkdocs extension to embed tabular format files like tsv or csv\nLastly and very importantly, install Git from their webpage.\n\n\n\nGo to Github and create a new user.\n\n\n\nGitHub allows users to create organizations and teams that will collaborate together or create repositories under the same umbrella organization. If you would like to create an educational organization in GitHub, you can do so for free! For example, you could create a GitHub account for your lab.\nIn order to create a GitHub organization, follow these instructions\nAfter you have created the GitHub organization, make sure that you create your repositories under the organization space and not your own user!\n\n\n\nThe next step is to set up the main GitHub Page site and the repository that will host it. This is very simple, as you will only need to follow these steps.\nAfter you have created the organizationgithub.io, it is time to configure your webpage using MkDocs!\n\n\nFollow the steps on the MkDocs documentation to get started on your webpage! You can use a simple markdown file describing your organization (your lab or department), its main goals and missions and maybe a couple of images showcasing your research.\nWhen you are happy with your webpage and are ready too publish it, make sure to add, commit and push the changes to the remote! Instead of using the basic setup that GitHub offers, we recommend that you build up your webpage using MkDocs and the mkdocs gh-deploy command! This requires a couple of changes in your GitHub organization settings.\n\n\n\nGo to your GitHub organization settings and configure the Page section. Since you are using the mkdocs gh-deploy command to publish your site in the gh-pages branch (as explained the the mkdocs documentation), we need to change where GitHub is fetching the website from:\n\n\n\nGitHub Pages setup\n\n\n\nBranch should be gh-pages\nFolder should be root\n\nAfter a couple of minutes, your webpage should be ready!\n\n\n\n\nYour GitHub organization account and webpage is ready! Now it is time to create a cookiecutter template for your folders using what you learned in this lesson.\n\n\n\nUsing cookiecutter, create a new data analysis project. Remember to fill up your metadata and description files! After you have created the folder, it would be best to initialize a Git repo following the instructions from the previous section.\nNext, link your data of interest and make an example of data analysis notebook/report. Depending on your setup, you might be using Jupyter Notebooks or Rmarkdowns. The extensions that we have installed using pip allows you to directly add a Jupyter Notebook file to the mkdocs.yml navigation section. On the other hand, if you are using Rmarkdown, you will have to knit your document into either an html page or a github document.\n\n\n\nRemember to make sure that your markdowns, images, reports, etc., are included in the docs folder and properly set up in the navigation section of your mkdocs.yml file.\nGit add, commit and push your changes. Then, run mkdocs gh-deploy. You will still need to configure the settings of this repositories in GitHub, so that the Page is taken from the gh-pages branch and the root folder. You should be able to see your webpage through the link provided in the Page section!\nNow it is also possible to include this repository webpage in your main webpage organizationgithub.io by including the link of the repo website (https://organizationgithub.io/repo-name) in the navigation section of the mkdocs.yml file in the main organizationgithub.io repo.\n!!! question ‚ÄúExercise 5: make a project folder and publish a data analysis webpage‚Äù\n1. Configure your main GitHub Page and its repo\n\nThe first step is to set up the main GitHub Page site and the repository that will host it. This is very simple, as you will only need to follow [these steps](https://pages.github.com/).\nAfter you have created the *organization/username*github.io, it is time to configure your `Project` repository webpage using MkDocs!\n\n2. Start a new project from cookiecutter or use one from the previous exercise.\n\nIf you use a `Project` repo from the first exercise, go to the next paragraph. Using cookiecutter, create a new data analysis project. Remember to fill up your metadata and description files! After you have created the folder, it would be best to initialize a Git repo following the instructions from the [previous section](#creating-a-git-repo-online-and-copying-your-project-folder).\n\nNext, link your data of interest (or create a small fake dataset) and make an example of data analysis notebook/report (this could be just a scatter plot of a random matrix of values). Depending on your setup, you might be using Jupyter Notebooks or Rmarkdowns. The extensions that we have installed using `pip` allows you to directly add a Jupyter Notebook file to the `mkdocs.yml` navigation section. On the other hand, if you are using Rmarkdown, you will have to knit your document into either an html page or a github document.\n\nFor the purposes of this exercise, we have already included a basic `index.md` markdown file that can serve as the intro page of your repo, and a `jupyter_example.ipynb` with some code in it. You are welcome to modify them further to test them out!\n\n3. Use MkDocs to create your webpage\n\nWhen you are happy with your files and are ready too publish them, make sure to add, commit and push the changes to the remote. Then, build up your webpage using MkDocs and the [`mkdocs gh-deploy`](https://www.mkdocs.org/user-guide/deploying-your-docs/) command from the same directory where the `mkdocs.yml` file is. For example, if your `mkdocs.yml` for your `Project` folder is in `/Users/JARH/Projects/project1_JARH_20231010/mkdocs.yml`, do `cd /Users/JARH/Projects/project1_JARH_20231010/` and then `mkdocs gh-deploy`.\n\nFinally, we only need to set up the GitHub `Project` repo settings.\n\n4. Publishing your GitHub Page\n\nGo to your GitHub repo settings and configure the Page section. Since you are using the `mkdocs gh-deploy` command to publish your site in the `gh-pages` branch (as explained the the mkdocs documentation), we need to change where GitHub is fetching the website from:\n\n![GitHub Pages setup](./images/git_pages.png)\n\n- Branch should be `gh-pages`\n- Folder should be `root`\n\nAfter a couple of minutes, your webpage should be ready!\n\n\n\n\nIn this lesson we have learned about version control and how we can use Git and GitHub to create data analyses repositories from your Project folders. We have also seen how one can create a GitHub organization and use GitHub Pages to display your data analyses scripts and notebooks for the general public! In the next lesson we will learn about where to archive the raw NGS data (and its metadata), as well as archiving your GitHub repositories.",
    "crumbs": [
      "Course material",
      "Data analysis version control"
    ]
  },
  {
    "objectID": "develop/09_version_control.html#version-control",
    "href": "develop/09_version_control.html#version-control",
    "title": "Data analysis version control",
    "section": "",
    "text": "Version control is a systematic approach to tracking changes made to a project over time. It provides a structured means of documenting alterations, allowing you to revisit and understand the evolution of your work. In research data management and data analytics, version control is very important and gives you a lot of advantages.\n!!! success ‚ÄúAdvantages of using version control‚Äù\n1. **Document Progress**: Version control keeps a detailed history of changes, making it easier to understand how a project has developed, what modifications were made, and by whom.\n2. **Ensure Data Integrity**: It safeguards data by preventing accidental overwrites or deletions. Each change is tracked, enabling easy recovery in case of errors.\n3. **Facilitate Collaboration**: In collaborative research, version control enables multiple team members to work simultaneously on a project without conflicts. Changes can be merged seamlessly.\n4. **Reproducibility**: It enhances reproducibility by preserving the exact state of your project at any point in time. This is invaluable for validating research findings and data analysis.\n5. **Branching and Experimentation**: Version control allows for branching, where you can create alternative versions of your project for experimentation without affecting the main branch.\n6. **Global Accessibility**: Platforms like GitHub provide global visibility, allowing researchers to share their work, receive feedback, and contribute to open science.",
    "crumbs": [
      "Course material",
      "Data analysis version control"
    ]
  },
  {
    "objectID": "develop/09_version_control.html#git-and-github",
    "href": "develop/09_version_control.html#git-and-github",
    "title": "Data analysis version control",
    "section": "",
    "text": "!!! warning\nIn this section we will only talk briefly about what is Git and Github. Explaining how git works is beyond the scope of this course. If you want to know more, please check out our [course](https://heads.ku.dk/course/git_github/)! You can also check [GitHub documentation](https://docs.github.com/get-started), which cover all the basics to work with Git and GitHub.\n\n\nGit is a distributed version control system that enables developers and researchers to efficiently manage their project‚Äôs history, collaborate seamlessly, and ensure data integrity. At its core, Git operates through the following principles and mechanisms:\n\nLocal Repository: Each user maintains a local repository on their computer, allowing them to work on a project independently. This local repository stores the complete project history.\nSnapshots, Not Files: Git does not track individual file changes but rather captures snapshots of the entire project at different points in time. This ‚Äòsnapshot‚Äô approach ensures data consistency.\nCommits: Users create ‚Äòcommits,‚Äô which are snapshots of the project at a specific moment. Commits record changes made to files, along with a commit message explaining the modifications.\nBranching: Git supports branching, enabling users to create separate lines of development. Branches are useful for experimenting with new features or fixing bugs without affecting the main project.\nMerging: Changes made in one branch can be merged into another, allowing for the incorporation of new features or bug fixes back into the main project. Git ensures a smooth merging process.\nDistributed Architecture: Git‚Äôs distributed nature means that each user‚Äôs local repository is a complete copy of the project, including its entire history. This enables offline work and ensures data redundancy.\nRemote Repositories: Git allows users to connect and synchronize their local repositories with remote repositories hosted on platforms like GitHub. Remote repositories facilitate collaboration and provide a central hub for project sharing.\nPush and Pull: Users ‚Äòpush‚Äô their local changes to a remote repository to share with others. Conversely, they ‚Äòpull‚Äô changes made by others into their local repository to stay up-to-date.\nConflict Resolution: In cases of conflicting changes, Git provides tools to resolve conflicts manually, ensuring that data integrity is maintained during collaboration.\nVersioning and Tagging: Git offers versioning and tagging capabilities, allowing users to mark specific points in history, such as major releases or significant milestones.\n\n\n\n\nOn the other hand, GitHub is a web-based platform that enhances Git‚Äôs capabilities by providing a collaborative and centralized hub for hosting Git repositories. It offers several key functionalities, such as tracking issues, security features to safeguard your repos, and GitHub Pages that allows you to create websites to showcase your projects. We will see more about Github Pages in the section below. In addition, you can set your repository as private until you are ready to publish your work!\nIf you do not have an account in Github already, we recommend you to do one now! There are other alternatives to GitHub, such as BitBucket and GitLab, Some features might be different, so we will stick to Github for the rest of the lesson!",
    "crumbs": [
      "Course material",
      "Data analysis version control"
    ]
  },
  {
    "objectID": "develop/09_version_control.html#creating-a-repo-from-your-project-folder",
    "href": "develop/09_version_control.html#creating-a-repo-from-your-project-folder",
    "title": "Data analysis version control",
    "section": "",
    "text": "We will show you two ways to make your Project folder into a Git repository\n!!! warning ‚ÄúWhat about my Assay folder?‚Äù\nAn assay folder contains very big files that are not suitable for version control, at least in GitHub. We recommend that you deposit the data into a domain specific archive such as [GEO](https://www.ncbi.nlm.nih.gov/geo/) or [ArrayExpress/Annotare](https://www.ebi.ac.uk/fg/annotare/login/). Even [Zenodo](https://zenodo.org/) would be a better option in this case. We will look at them in the [next session](./10_repos.qmd).\n\nRemember to link the original dataset with the data analysis repository!\n\n\nUsing git init is probably the right choice for you if you have created a Project folder using the cookiecutter template we saw in the previous lesson.\nThe git init command should only be run once, even if other collaborators share will the project!\n\nFirst, initialize the repository (git init) and make at least one commit (git add and git commit).\nOnce you have initialized the repository, create a remote repository in GitHub\nThen, add the remote URL to your local git repository with git remote add origin \\&lt;URL\\&gt;. This stores the remote URL under a more human-friendly name, origin.\nShape your history into at least one commit by using git add to stage the existing files, and git commit to make the snapshot.\nOnce you have at least one commit, you can push to the remote and set up the tracking relationship for good with git push -u origin master.\n\n\n\n\nIf the repository already exists on a remote, you would choose to git clone and not git init. On the other hand, if you create a remote repository first with the intent of moving your project to it later, you may have a few other steps to follow. If there are no commits in the remote repository, you can follow the steps above for git init. If there are commits and files in the remote repository but you would still like it to contain your project files, git clone that repository. Then, move the project‚Äôs files into that cloned repository. git add, git commit, and git push to create a history that makes sense for the beginning of your project. Then, your team can interact with the repository without git init again.\n!!! tip ‚ÄúTips to write good commit messages‚Äù\nIf you would like to know more about Git commits and the best way to make clear git messages, check out [this post](https://www.conventionalcommits.org/en/v1.0.0/)!",
    "crumbs": [
      "Course material",
      "Data analysis version control"
    ]
  },
  {
    "objectID": "develop/09_version_control.html#github-pages",
    "href": "develop/09_version_control.html#github-pages",
    "title": "Data analysis version control",
    "section": "",
    "text": "Once you have created your repository (and put it in GitHub), you have now the opportunity to add your data analysis reports that you created, in either Jupyter Notebooks, Rmarkdowns or html reports, in a GitHub Page website. Creating a GitHub page is very simple, and we really recommend that you follow the nice tutorial that GitHub as put for you.\nThere are many different ways to create your webpages. We recommend using Mkdocs and Mkdocs materials as a framework to create a nice webpage in a simple manner. The folder templates that we used as an example in lesson 06 already contain everything you need to start a webpage. Nonetheless, you will need to understand the basics of MkDocs and MkDocs materials to design a webpage to your liking. MkDocs is a static webpage generator that is very easy to use, while MkDocs materials is an extension of the tool that gives you many more options to customize your website. Check out their webpages to get started!",
    "crumbs": [
      "Course material",
      "Data analysis version control"
    ]
  },
  {
    "objectID": "develop/09_version_control.html#a-full-setup-example",
    "href": "develop/09_version_control.html#a-full-setup-example",
    "title": "Data analysis version control",
    "section": "",
    "text": "In this section we will showcase a full example about how to setup Git, MkDocs and a github account so you can do it yourself!\n\n\nThere are several tools and softwares that you need to install. First you will need pip and install the following packages using pip in the command line:\npip install cookiecutter # cookiecutter to create folder templates\npip install cruft # cruft is used to version control your templates\npip install mkdocs # mkdocs to create your webpages\npip install mkdocs-material # mkdocs extension to customize your templates\npip install mkdocs-video # mkdocs extension to add videos or embed internet videos, like youtube, to your webpages\npip install mkdocs-bibtex # mkdocs extension to add references in your text from a bib file\npip install neoteroi-mkdocs # mkdocs extension to create author cards\npip install mkdocs-minify-plugin # mkdocs extension to minimize the html code created by mkdocs\npip install mkdocs-git-revision-date-localized-plugin  # mkdocs extension to show \"last updated\" date of your webpage\npip install mkdocs-jupyter # mkdocs extension to include jupyter notebooks without needing to convert them\npip install mkdocs-table-reader-plugin # mkdocs extension to embed tabular format files like tsv or csv\nLastly and very importantly, install Git from their webpage.\n\n\n\nGo to Github and create a new user.\n\n\n\nGitHub allows users to create organizations and teams that will collaborate together or create repositories under the same umbrella organization. If you would like to create an educational organization in GitHub, you can do so for free! For example, you could create a GitHub account for your lab.\nIn order to create a GitHub organization, follow these instructions\nAfter you have created the GitHub organization, make sure that you create your repositories under the organization space and not your own user!\n\n\n\nThe next step is to set up the main GitHub Page site and the repository that will host it. This is very simple, as you will only need to follow these steps.\nAfter you have created the organizationgithub.io, it is time to configure your webpage using MkDocs!\n\n\nFollow the steps on the MkDocs documentation to get started on your webpage! You can use a simple markdown file describing your organization (your lab or department), its main goals and missions and maybe a couple of images showcasing your research.\nWhen you are happy with your webpage and are ready too publish it, make sure to add, commit and push the changes to the remote! Instead of using the basic setup that GitHub offers, we recommend that you build up your webpage using MkDocs and the mkdocs gh-deploy command! This requires a couple of changes in your GitHub organization settings.\n\n\n\nGo to your GitHub organization settings and configure the Page section. Since you are using the mkdocs gh-deploy command to publish your site in the gh-pages branch (as explained the the mkdocs documentation), we need to change where GitHub is fetching the website from:\n\n\n\nGitHub Pages setup\n\n\n\nBranch should be gh-pages\nFolder should be root\n\nAfter a couple of minutes, your webpage should be ready!\n\n\n\n\nYour GitHub organization account and webpage is ready! Now it is time to create a cookiecutter template for your folders using what you learned in this lesson.\n\n\n\nUsing cookiecutter, create a new data analysis project. Remember to fill up your metadata and description files! After you have created the folder, it would be best to initialize a Git repo following the instructions from the previous section.\nNext, link your data of interest and make an example of data analysis notebook/report. Depending on your setup, you might be using Jupyter Notebooks or Rmarkdowns. The extensions that we have installed using pip allows you to directly add a Jupyter Notebook file to the mkdocs.yml navigation section. On the other hand, if you are using Rmarkdown, you will have to knit your document into either an html page or a github document.\n\n\n\nRemember to make sure that your markdowns, images, reports, etc., are included in the docs folder and properly set up in the navigation section of your mkdocs.yml file.\nGit add, commit and push your changes. Then, run mkdocs gh-deploy. You will still need to configure the settings of this repositories in GitHub, so that the Page is taken from the gh-pages branch and the root folder. You should be able to see your webpage through the link provided in the Page section!\nNow it is also possible to include this repository webpage in your main webpage organizationgithub.io by including the link of the repo website (https://organizationgithub.io/repo-name) in the navigation section of the mkdocs.yml file in the main organizationgithub.io repo.\n!!! question ‚ÄúExercise 5: make a project folder and publish a data analysis webpage‚Äù\n1. Configure your main GitHub Page and its repo\n\nThe first step is to set up the main GitHub Page site and the repository that will host it. This is very simple, as you will only need to follow [these steps](https://pages.github.com/).\nAfter you have created the *organization/username*github.io, it is time to configure your `Project` repository webpage using MkDocs!\n\n2. Start a new project from cookiecutter or use one from the previous exercise.\n\nIf you use a `Project` repo from the first exercise, go to the next paragraph. Using cookiecutter, create a new data analysis project. Remember to fill up your metadata and description files! After you have created the folder, it would be best to initialize a Git repo following the instructions from the [previous section](#creating-a-git-repo-online-and-copying-your-project-folder).\n\nNext, link your data of interest (or create a small fake dataset) and make an example of data analysis notebook/report (this could be just a scatter plot of a random matrix of values). Depending on your setup, you might be using Jupyter Notebooks or Rmarkdowns. The extensions that we have installed using `pip` allows you to directly add a Jupyter Notebook file to the `mkdocs.yml` navigation section. On the other hand, if you are using Rmarkdown, you will have to knit your document into either an html page or a github document.\n\nFor the purposes of this exercise, we have already included a basic `index.md` markdown file that can serve as the intro page of your repo, and a `jupyter_example.ipynb` with some code in it. You are welcome to modify them further to test them out!\n\n3. Use MkDocs to create your webpage\n\nWhen you are happy with your files and are ready too publish them, make sure to add, commit and push the changes to the remote. Then, build up your webpage using MkDocs and the [`mkdocs gh-deploy`](https://www.mkdocs.org/user-guide/deploying-your-docs/) command from the same directory where the `mkdocs.yml` file is. For example, if your `mkdocs.yml` for your `Project` folder is in `/Users/JARH/Projects/project1_JARH_20231010/mkdocs.yml`, do `cd /Users/JARH/Projects/project1_JARH_20231010/` and then `mkdocs gh-deploy`.\n\nFinally, we only need to set up the GitHub `Project` repo settings.\n\n4. Publishing your GitHub Page\n\nGo to your GitHub repo settings and configure the Page section. Since you are using the `mkdocs gh-deploy` command to publish your site in the `gh-pages` branch (as explained the the mkdocs documentation), we need to change where GitHub is fetching the website from:\n\n![GitHub Pages setup](./images/git_pages.png)\n\n- Branch should be `gh-pages`\n- Folder should be `root`\n\nAfter a couple of minutes, your webpage should be ready!",
    "crumbs": [
      "Course material",
      "Data analysis version control"
    ]
  },
  {
    "objectID": "develop/09_version_control.html#wrap-up",
    "href": "develop/09_version_control.html#wrap-up",
    "title": "Data analysis version control",
    "section": "",
    "text": "In this lesson we have learned about version control and how we can use Git and GitHub to create data analyses repositories from your Project folders. We have also seen how one can create a GitHub organization and use GitHub Pages to display your data analyses scripts and notebooks for the general public! In the next lesson we will learn about where to archive the raw NGS data (and its metadata), as well as archiving your GitHub repositories.",
    "crumbs": [
      "Course material",
      "Data analysis version control"
    ]
  },
  {
    "objectID": "develop/10_repos.html",
    "href": "develop/10_repos.html",
    "title": "Repositories for NGS",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n1. Learn what are repositories and which ones are relevant for NGS data 2. Learn where to archive your GitHub data analysis repositories\n\n\nIn this lesson, we‚Äôre going to explore repositories like Zenodo, Gene Expression Omnibus, and Annotare. While platforms like GitHub are great for version control and collaborative coding, these repositories serve a different purpose. They‚Äôre designed specifically for archiving and sharing scientific data, ensuring it‚Äôs preserved for the long term and accessible to the global research community. You could think of them as secure digital libraries for your valuable NGS data.\n\n\nSpecialized repositories/archives are dedicated digital platforms designed for the secure storage, curation, and dissemination of scientific data. These repositories hold great importance in the research community as they serve as reliable archives for preserving valuable datasets. Their standardized formats and robust curation processes ensure the long-term accessibility and citability of research findings. Researchers worldwide rely on these repositories to share, discover, and validate scientific information, thereby fostering transparency, collaboration, and the advancement of knowledge across various domains of study.\n\n\n\nArchiving your data in these repositories offers a multitude of benefits. Firstly, it ensures the enduring accessibility and preservation of your research, safeguarding it for future generations of scientists. Additionally, repositories typically provide a unique Identifier (often a DOI) for your dataset, granting it a citable status in the academic world. This not only enhances the visibility and impact of your work but also facilitates proper attribution.\nMoreover, these repositories often encourage or require comprehensive metadata to accompany your data. This rich contextual information includes details about the methodology, experimental setup, and any other pertinent information. Such metadata greatly enhances the discoverability and interpretability of your dataset, enabling fellow researchers to effectively use and build upon your work. In essence, archiving your data in these repositories not only fulfills scholarly obligations but also amplifies the reach and influence of your research in the scientific community.\nIn addition, depositing data in these archives is often a mandatory requirement set by scientific journals and funding agencies. This reflects the growing recognition of the critical role these repositories play in ensuring transparency, reproducibility, and the integrity of research outcomes. By adhering to these guidelines, researchers contribute to the broader scientific community and increase the quality of their research.\n\n\n\nWe could divide archives in two mayor categories, general or domain-specific. General repositories like Zenodo cater to a wide range of disciplines, providing a versatile platform for data archiving. On the other hand, domain-specific repositories are tailored to particular scientific fields, like GEO and Annotare in the field of NGS, offering specialized curation and context-specific features. Utilizing domain-specific repositories often provides researchers with a more targeted audience, deeper domain expertise, and enhanced visibility within their specific research community. This focused approach ensures that data is curated and contextualized in a way that aligns closely with the standards and expectations of that particular field, maximizing its impact and utility.\n\n\n\n\nThe Gene Expression Omnibus, commonly known as GEO, is a specialized repository curated by the National Center for Biotechnology Information (NCBI). It is dedicated to archiving and sharing high-throughput functional genomic data sets, primarily focused on gene expression data.\nResearchers can easily deposit and access a wide range of genomic data, including microarray and high-throughput sequencing studies. GEO provides a structured platform for researchers to share their findings with the scientific community, enhancing data transparency and reproducibility.\nGEO assigns unique accession numbers to each dataset, ensuring traceability and enabling proper citation in research publications. Its domain-specific focus on functional genomics makes it an invaluable resource for researchers in genetics, genomics, and related fields, allowing for the comprehensive exploration of gene expression patterns across various biological conditions and experimental designs.\n\n\n\nAnnotare is a specialized repository hosted by the European Bioinformatics Institute (EBI) that is tailored for the submission and storage of functional genomics experiments, particularly those involving high-throughput sequencing data. Unlike general repositories, Annotare provides a domain-specific platform optimized for researchers in the field of functional genomics.\nResearchers can upload their experimental data along with comprehensive metadata, ensuring that the context and details of the experiment are preserved. Annotare supports a wide range of genomic data types, making it a versatile platform for archiving diverse functional genomics studies.\nBy focusing specifically on functional genomics, Annotare offers researchers a curated environment that aligns closely with the standards and practices of this specialized field. This ensures that data is stored and curated in a manner that is most useful for researchers in the genomics community. The platform‚Äôs specialization enhances data discoverability, promotes collaboration, and facilitates deeper insights into the functional aspects of the genome.\n\n\n\nGEO and Annotare are excellent repository choices to deposit your NGS data. Both Annotare and GEO adhere to established community standards for data submission and sharing in the field of functional genomics:\n\nMinimum Information About a Microarray Experiment (MIAME): This is a set of guidelines established to ensure the comprehensive and standardized reporting of microarray experiments. Both Annotare and GEO require compliance with MIAME standards for microarray data submissions.\nMinimum Information about a high-throughput SeQuencing Experiment (MIxS): MIxS is a set of standards developed by the Genomic Standards Consortium to ensure consistent reporting of metadata for high-throughput sequencing experiments. Annotare and GEO require adherence to MIxS standards for sequencing data submissions.\nSequence Read Archive (SRA) Submission Guidelines: Both Annotare and GEO follow the submission guidelines set forth by the Sequence Read Archive, which include requirements for data formatting, metadata inclusion, and quality control.\nCommunity-Specific Standards: In addition to the above, Annotare and GEO may also adhere to community-specific standards and guidelines established by the functional genomics research community. These standards are designed to ensure that submitted data meets the specific requirements and expectations of the field.\n\nBy adhering to these standards, Annotare and GEO ensure that the data submitted to their repositories is of high quality, well-documented, and compliant with community best practices. This facilitates data discovery, reproducibility, and interoperability within the scientific community.\nThese repositories will only accept NGS data and information related to the creation of the data. This includes the raw FASTQ files, sample metadata, including protocols and descriptions of how the samples and data where processed, as well as final pre-processing results such as read count matrices or genomic position files (like BED). If you adhere to the Assay folder creation guideline of lesson 6, you will have a very easy time filling up the required documentation and information needed to submit the data in your Assay folder to one of these repositories.\nNonetheless, the repositories will not accept other data created by your down-stream analyses, neither the code used for data analyses! This means anything that you have done in your Project folder. However, your Project folder is already version controlled by GitHub (see previous lesson), so there is no need to worry. We will see in the section below how to archive your `Project`` folder as well using a general repository like Zenodo.\n\n\n\n\nZenodo[https://zenodo.org/] is an open-access digital repository designed to facilitate the archiving of scientific research outputs. It operates under the umbrella of the European Organization for Nuclear Research (CERN) and is supported by the European Commission. Zenodo accommodates a broad spectrum of research outputs, including datasets, papers, software, and multimedia files. This versatility makes it an invaluable resource for researchers across a wide array of domains, promoting transparency, collaboration, and the advancement of knowledge on a global scale.\nOperating on a user-friendly web platform, Zenodo allows researchers to easily upload, share, and preserve their research data and related materials. Upon deposit, each item is assigned a unique Digital Object Identifier (DOI), granting it a citable status and ensuring its long-term accessibility. Additionally, Zenodo provides robust metadata capabilities, enabling researchers to enrich their submissions with detailed contextual information. In addition, it allows you to link you GitHub account, providing a streamlined way to archive a specific release of your GitHub repository directly into Zenodo. This integration simplifies the process of preserving a snapshot of your project‚Äôs progress for long-term accessibility and citation.\n\n\nOnce your accounts are linked, creating a Zenodo archive is as simple as tagging a release in your GitHub repository. Zenodo will automatically detect the release and generate a corresponding archive. This archive is assigned a unique Digital Object Identifier (DOI), making it a citable reference for your work. So, before submitting your work in a journal, make sure to link your data analysis repository to Zenodo, get a DOI and cite it in your manuscript!\nBy leveraging this integration, you ensure that significant milestones in your project are preserved in a reliable and accessible manner. This not only facilitates proper attribution but also contributes to the broader scientific community‚Äôs ability to reproduce and build upon your research.\n!!! question ‚ÄúExercise 6: Archive a Project GitHub repo in Zenodo‚Äù\n1. In order to archive your GitHub repos in Zenodo, you will first need to [link your Zenodo and GitHub accounts](https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content).\n2. Once your accounts are linked, go to your Zenodo GitHub account settings and turn on the GitHub repository you want to archive.\n![zenodo_github_link](./images/zenodo_github.png)\n3. Creating a Zenodo archive is now as simple as [making a release](https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository) in your GitHub repository. Remember to make a proper tag! **NOTE: If you make a release before enabling the GitHub repository in Zenodo, it will not appear in Zenodo!**\n![github_release](./images/github_release.png)\n4. Zenodo will automatically detect the release and it should appear in your Zenodo upload page.\n![zenodo_archives](./images/zenodo_archives.png)\n5. This archive is assigned a unique Digital Object Identifier (DOI), making it a citable reference for your work.\n![zenodo_example](./images/zenodo_example.png)\n\nBefore submitting your work in a journal, make sure to link your data analysis repository to [Zenodo](https://zenodo.org/), get a DOI and cite it in your manuscript!\n\n\n\n\n\nIn this final lesson we have learned how to wrap up a project/manuscript experiment by submitting your data to a domain-specific repository, while archiving your data analysis GitHub repositories in Zenodo. By following the simple lessons shown in this workshop, you will dramatically improve the FAIRability of your data, as well as organizing and structuring it in a way that will be much more useful in the future. This advantages do not serve yourself only, but your teammates, group leader and the general scientific population!\nWe hope that you found this workshop useful. If you would like to leave us some comments or suggestions, feel free to answer this form!\nFEEDBACK FORM",
    "crumbs": [
      "Course material",
      "Repositories for NGS"
    ]
  },
  {
    "objectID": "develop/10_repos.html#what-is-a-repositoryarchive",
    "href": "develop/10_repos.html#what-is-a-repositoryarchive",
    "title": "Repositories for NGS",
    "section": "",
    "text": "Specialized repositories/archives are dedicated digital platforms designed for the secure storage, curation, and dissemination of scientific data. These repositories hold great importance in the research community as they serve as reliable archives for preserving valuable datasets. Their standardized formats and robust curation processes ensure the long-term accessibility and citability of research findings. Researchers worldwide rely on these repositories to share, discover, and validate scientific information, thereby fostering transparency, collaboration, and the advancement of knowledge across various domains of study.",
    "crumbs": [
      "Course material",
      "Repositories for NGS"
    ]
  },
  {
    "objectID": "develop/10_repos.html#why-are-they-important",
    "href": "develop/10_repos.html#why-are-they-important",
    "title": "Repositories for NGS",
    "section": "",
    "text": "Archiving your data in these repositories offers a multitude of benefits. Firstly, it ensures the enduring accessibility and preservation of your research, safeguarding it for future generations of scientists. Additionally, repositories typically provide a unique Identifier (often a DOI) for your dataset, granting it a citable status in the academic world. This not only enhances the visibility and impact of your work but also facilitates proper attribution.\nMoreover, these repositories often encourage or require comprehensive metadata to accompany your data. This rich contextual information includes details about the methodology, experimental setup, and any other pertinent information. Such metadata greatly enhances the discoverability and interpretability of your dataset, enabling fellow researchers to effectively use and build upon your work. In essence, archiving your data in these repositories not only fulfills scholarly obligations but also amplifies the reach and influence of your research in the scientific community.\nIn addition, depositing data in these archives is often a mandatory requirement set by scientific journals and funding agencies. This reflects the growing recognition of the critical role these repositories play in ensuring transparency, reproducibility, and the integrity of research outcomes. By adhering to these guidelines, researchers contribute to the broader scientific community and increase the quality of their research.",
    "crumbs": [
      "Course material",
      "Repositories for NGS"
    ]
  },
  {
    "objectID": "develop/10_repos.html#types-of-repositories",
    "href": "develop/10_repos.html#types-of-repositories",
    "title": "Repositories for NGS",
    "section": "",
    "text": "We could divide archives in two mayor categories, general or domain-specific. General repositories like Zenodo cater to a wide range of disciplines, providing a versatile platform for data archiving. On the other hand, domain-specific repositories are tailored to particular scientific fields, like GEO and Annotare in the field of NGS, offering specialized curation and context-specific features. Utilizing domain-specific repositories often provides researchers with a more targeted audience, deeper domain expertise, and enhanced visibility within their specific research community. This focused approach ensures that data is curated and contextualized in a way that aligns closely with the standards and expectations of that particular field, maximizing its impact and utility.\n\n\n\n\nThe Gene Expression Omnibus, commonly known as GEO, is a specialized repository curated by the National Center for Biotechnology Information (NCBI). It is dedicated to archiving and sharing high-throughput functional genomic data sets, primarily focused on gene expression data.\nResearchers can easily deposit and access a wide range of genomic data, including microarray and high-throughput sequencing studies. GEO provides a structured platform for researchers to share their findings with the scientific community, enhancing data transparency and reproducibility.\nGEO assigns unique accession numbers to each dataset, ensuring traceability and enabling proper citation in research publications. Its domain-specific focus on functional genomics makes it an invaluable resource for researchers in genetics, genomics, and related fields, allowing for the comprehensive exploration of gene expression patterns across various biological conditions and experimental designs.\n\n\n\nAnnotare is a specialized repository hosted by the European Bioinformatics Institute (EBI) that is tailored for the submission and storage of functional genomics experiments, particularly those involving high-throughput sequencing data. Unlike general repositories, Annotare provides a domain-specific platform optimized for researchers in the field of functional genomics.\nResearchers can upload their experimental data along with comprehensive metadata, ensuring that the context and details of the experiment are preserved. Annotare supports a wide range of genomic data types, making it a versatile platform for archiving diverse functional genomics studies.\nBy focusing specifically on functional genomics, Annotare offers researchers a curated environment that aligns closely with the standards and practices of this specialized field. This ensures that data is stored and curated in a manner that is most useful for researchers in the genomics community. The platform‚Äôs specialization enhances data discoverability, promotes collaboration, and facilitates deeper insights into the functional aspects of the genome.\n\n\n\nGEO and Annotare are excellent repository choices to deposit your NGS data. Both Annotare and GEO adhere to established community standards for data submission and sharing in the field of functional genomics:\n\nMinimum Information About a Microarray Experiment (MIAME): This is a set of guidelines established to ensure the comprehensive and standardized reporting of microarray experiments. Both Annotare and GEO require compliance with MIAME standards for microarray data submissions.\nMinimum Information about a high-throughput SeQuencing Experiment (MIxS): MIxS is a set of standards developed by the Genomic Standards Consortium to ensure consistent reporting of metadata for high-throughput sequencing experiments. Annotare and GEO require adherence to MIxS standards for sequencing data submissions.\nSequence Read Archive (SRA) Submission Guidelines: Both Annotare and GEO follow the submission guidelines set forth by the Sequence Read Archive, which include requirements for data formatting, metadata inclusion, and quality control.\nCommunity-Specific Standards: In addition to the above, Annotare and GEO may also adhere to community-specific standards and guidelines established by the functional genomics research community. These standards are designed to ensure that submitted data meets the specific requirements and expectations of the field.\n\nBy adhering to these standards, Annotare and GEO ensure that the data submitted to their repositories is of high quality, well-documented, and compliant with community best practices. This facilitates data discovery, reproducibility, and interoperability within the scientific community.\nThese repositories will only accept NGS data and information related to the creation of the data. This includes the raw FASTQ files, sample metadata, including protocols and descriptions of how the samples and data where processed, as well as final pre-processing results such as read count matrices or genomic position files (like BED). If you adhere to the Assay folder creation guideline of lesson 6, you will have a very easy time filling up the required documentation and information needed to submit the data in your Assay folder to one of these repositories.\nNonetheless, the repositories will not accept other data created by your down-stream analyses, neither the code used for data analyses! This means anything that you have done in your Project folder. However, your Project folder is already version controlled by GitHub (see previous lesson), so there is no need to worry. We will see in the section below how to archive your `Project`` folder as well using a general repository like Zenodo.\n\n\n\n\nZenodo[https://zenodo.org/] is an open-access digital repository designed to facilitate the archiving of scientific research outputs. It operates under the umbrella of the European Organization for Nuclear Research (CERN) and is supported by the European Commission. Zenodo accommodates a broad spectrum of research outputs, including datasets, papers, software, and multimedia files. This versatility makes it an invaluable resource for researchers across a wide array of domains, promoting transparency, collaboration, and the advancement of knowledge on a global scale.\nOperating on a user-friendly web platform, Zenodo allows researchers to easily upload, share, and preserve their research data and related materials. Upon deposit, each item is assigned a unique Digital Object Identifier (DOI), granting it a citable status and ensuring its long-term accessibility. Additionally, Zenodo provides robust metadata capabilities, enabling researchers to enrich their submissions with detailed contextual information. In addition, it allows you to link you GitHub account, providing a streamlined way to archive a specific release of your GitHub repository directly into Zenodo. This integration simplifies the process of preserving a snapshot of your project‚Äôs progress for long-term accessibility and citation.\n\n\nOnce your accounts are linked, creating a Zenodo archive is as simple as tagging a release in your GitHub repository. Zenodo will automatically detect the release and generate a corresponding archive. This archive is assigned a unique Digital Object Identifier (DOI), making it a citable reference for your work. So, before submitting your work in a journal, make sure to link your data analysis repository to Zenodo, get a DOI and cite it in your manuscript!\nBy leveraging this integration, you ensure that significant milestones in your project are preserved in a reliable and accessible manner. This not only facilitates proper attribution but also contributes to the broader scientific community‚Äôs ability to reproduce and build upon your research.\n!!! question ‚ÄúExercise 6: Archive a Project GitHub repo in Zenodo‚Äù\n1. In order to archive your GitHub repos in Zenodo, you will first need to [link your Zenodo and GitHub accounts](https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content).\n2. Once your accounts are linked, go to your Zenodo GitHub account settings and turn on the GitHub repository you want to archive.\n![zenodo_github_link](./images/zenodo_github.png)\n3. Creating a Zenodo archive is now as simple as [making a release](https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository) in your GitHub repository. Remember to make a proper tag! **NOTE: If you make a release before enabling the GitHub repository in Zenodo, it will not appear in Zenodo!**\n![github_release](./images/github_release.png)\n4. Zenodo will automatically detect the release and it should appear in your Zenodo upload page.\n![zenodo_archives](./images/zenodo_archives.png)\n5. This archive is assigned a unique Digital Object Identifier (DOI), making it a citable reference for your work.\n![zenodo_example](./images/zenodo_example.png)\n\nBefore submitting your work in a journal, make sure to link your data analysis repository to [Zenodo](https://zenodo.org/), get a DOI and cite it in your manuscript!",
    "crumbs": [
      "Course material",
      "Repositories for NGS"
    ]
  },
  {
    "objectID": "develop/10_repos.html#wrap-up",
    "href": "develop/10_repos.html#wrap-up",
    "title": "Repositories for NGS",
    "section": "",
    "text": "In this final lesson we have learned how to wrap up a project/manuscript experiment by submitting your data to a domain-specific repository, while archiving your data analysis GitHub repositories in Zenodo. By following the simple lessons shown in this workshop, you will dramatically improve the FAIRability of your data, as well as organizing and structuring it in a way that will be much more useful in the future. This advantages do not serve yourself only, but your teammates, group leader and the general scientific population!\nWe hope that you found this workshop useful. If you would like to leave us some comments or suggestions, feel free to answer this form!\nFEEDBACK FORM",
    "crumbs": [
      "Course material",
      "Repositories for NGS"
    ]
  },
  {
    "objectID": "develop/04_OS_FAIR.html",
    "href": "develop/04_OS_FAIR.html",
    "title": "Open Science and FAIR principles",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n1. Learn about Open Science 2. Learn about FAIR principles 3. Learn about how they can be applied to NGS data\n\n\nOpen Science and FAIR principles have emerged as powerful frameworks that promote transparency, accessibility, and reusability in scientific research. As the landscape of scientific knowledge sharing evolves, these principles stand at the front of a global movement to accelerate scientific discovery and innovation. Open Science advocates for the unrestricted access to research outputs, data, and methodologies, fostering collaboration and opening scientific knowledge. On the other hand, the FAIR principles emphasize the importance of making data Findable, Accessible, Interoperable, and Reusable, laying the foundation for a data-driven approach that transcends disciplinary boundaries and supports long-term data preservation. Together, Open Science and FAIR principles offer a vision for a more inclusive and impactful scientific ecosystem that advances the boundaries of knowledge for the benefit of society as a whole. Funding agencies and governments worldwide are increasingly recognizing the value and potential of Open Science and FAIR principles in advancing research and driving societal impact. As a result, they have been actively promoting and requiring the adoption of these principles in academia through various mechanisms and policies. In this lesson we will learn what is Open Science and the FAIR principles, and how they can be applied to NGS data.\n\n\nFunding agencies and regulatory agencies, such as the EU, have been actively promoting Open Science through various policies, recognizing its potential to advance research, innovation, and societal well-being. The reasons for this push are plentiful. Open Science allows research outputs to be accessible to a wider audience, accelerating the impact of research and fostering collaboration among researchers and institutions, as well as encourages transparency and reproducibility, fostering responsible research conduct and mitigating issues like scientific fraud and data manipulation. The reasons are also economical: by promoting Open Access and data sharing, the EU seeks to maximize the return on its research investments by ensuring that knowledge and data are used and built upon by the broader scientific community. This also provides a foundation for innovation and entrepreneurship, enabling businesses and startups to access and leverage research findings for creating new products and services, and allows researchers to integrate and analyze large datasets from different sources, leading to new insights and discoveries.\n\n\n\nOpen Science, Australian Citizen Science Association.\n\n\nMore and more agencies are promoting Open Science initiatives, supporting research that adheres to OS principles. Here are some more examples:\n\nNational Institutes of Health (NIH): In the United States, NIH encourages Open Science practices, such as data sharing, through various policies and initiatives. The NIH Data Sharing Policy requires researchers to share their data with the scientific community.\nWellcome Trust: The Wellcome Trust, a global charitable foundation, has a strong commitment to Open Science. It mandates that research outputs funded by Wellcome must be made openly available through Open Access platforms.\nEuropean Molecular Biology Organization (EMBO): EMBO supports Open Access to research outputs and provides guidelines for best practices in data sharing.\nBill & Melinda Gates Foundation: The foundation advocates for Open Access and data sharing to maximize the impact of its global health and development research.\nEuropean Research Council (ERC): The ERC promotes Open Access to research outputs, requiring grantees to make their publications openly accessible. It also encourages data sharing and adheres to the FAIR principles to ensure the findability, accessibility, interoperability, and reusability of research data.\n\n\n\n\n\n\n\nBenefits of Open Science\n\n\n\nWhile it may seem that this push for Open Science is being enforced by funding agencies to get more bang for their buck, adhering to Open Science practices can offer numerous benefits to you, as a researcher. Some of the key advantages include: - Increased Visibility and Impact: When you share your research openly, more people can access and read your papers and data, increasing the visibility and impact of your work. - Enhanced Collaboration: Open Science encourages collaboration with other researchers, leading to new ideas and projects that can be more impactful. - Build Trust in Your Findings: By sharing your data and methods openly, other researchers can verify and validate your findings, increasing the credibility of your research. - Research Moves Faster: Open Science lets researchers build upon each other‚Äôs work, accelerating the pace of discovery and progress in science. - Inspire New Discoveries: Your shared data can inspire others to explore new research questions and find insights you might not have considered. - Attract Funding Opportunities: More and more funding agencies are supporting Open Science, and following open practices can help you qualify for more funding opportunities. - Transparency and Accountability: Open Science practices promote transparency, accountability, and responsible conduct in research, reducing the risk of scientific misconduct. - Preserve Your Data: By archiving your data in repositories, you ensure that it is preserved and available for future generations of scientists.\n\n\n\n\n\nThe FAIR principles are a set of guiding principles designed to enhance the management, sharing, and usability of research data, and could be considered as complementary to Open Science. FAIR stands for Findable, Accessible, Interoperable, and Reusable, and these principles aim to make research data more valuable, impactful, and sustainable. By adhering to the FAIR principles, researchers and institutions can maximize the value and impact of their research data. FAIR data not only benefits individual researchers but also contributes to the broader scientific community by fostering collaboration, data-driven discoveries, and the advancement of knowledge across disciplines. Furthermore, FAIR data supports long-term data preservation, making it valuable for future research and ensuring the sustainability of scientific progress.\nTake note that adhering to the FAIR principles is not a black and white matter. There are different measure of FAIR compliance and some key points are more complicated to accomplish than others. This is specially true when we talk about using metadata standards and controlled vocabularies!\nLet‚Äôs break down each FAIR principle:\n\n\n\n\n\nFAIR findable\n\n\nResearch data should be easy to find and identify. To achieve this, data should be assigned persistent and unique identifiers (e.g., DOIs) and described using standard metadata. Providing clear and comprehensive metadata helps researchers and machines discover relevant data through various search engines and data repositories.\n\n\n\n\n\n\nKey components\n\n\n\n- **Persistent Identifiers**: Research data should be assigned unique and persistent identifiers, such as Digital Object Identifiers (DOIs) or Uniform Resource Identifiers (URIs). These identifiers ensure that data can be located and referenced over time, even if it is moved or its location changes.\n- **Rich Metadata**: Data should be accompanied by comprehensive and standardized metadata that describes the content, context, and characteristics of the data. This metadata includes information about the data's origin, format, version, licensing, and the conditions for reuse.\n- **Data Repository or Catalog**: Data should be deposited in trusted repositories or data catalogs that follow FAIR principles, making it easier to discover and access data from a centralized and reliable source.\n\n\n\n\n\n\n\n\nFAIR accessible\n\n\nData should be openly accessible to anyone who needs it. This means that there should be minimal restrictions on accessing and downloading the data. Open access to data encourages collaboration, enables verification of research findings, and ensures transparency in research processes.\n\n\n\n\n\n\nKey components\n\n\n\n- **Open Access**: Data should be openly accessible to anyone without restrictions or unnecessary barriers. Researchers should choose appropriate licenses or waivers that allow the broadest possible reuse of the data.\n- **Authentication and Authorization**: If access restrictions are necessary (e.g., for sensitive or confidential data), proper authentication and authorization mechanisms should be in place to grant access only to authorized individuals or groups.\n- **Metadata**: Always deposit the metadata for your data. Even if you cannot deposit your data due to restrictions, it is a good idea to deposit the metadata of your data so that at least the record for that data exists, and what kind of data/information contains.\n\n\n\n\n\n\n\n\nImportant note\n\n\n\nAccessible does not equal to free, unrestricted access! There are cases when your data might be protected by GDPR regulations due to its sensitivity! If this is the case, you should, of course not make your data freely available. As said before, it is a great idea to at least deposit and make available the metadata for your data. \n\n*\"As open as possible, as closed as necessary\"* should be your motto.\n\nFor example, imagine that you obtain your sensitive data from a national authority regarding personal information. Most likely, you will not be able to access **all** the data, but you will be interested in a sample of the population that fits your purposes (e.g., people between 20-30 years old, smokers). You cannot publish or deposit your data, but you can publish what was the query you used to obtain this data, and its source!\n\n\n\n\n\n\n\n\nFAIR interoperable\n\n\nData should be structured and formatted in a way that allows it to be used and combined with other data seamlessly. Following standard data formats and adopting widely used vocabularies and ontologies promotes interoperability, enabling integration and comparison of data from different sources.\n\n\n\n\n\n\nKey components\n\n\n\n- **Standard Data Formats**: Data should be stored in standard and widely-used data formats, facilitating data exchange and interoperability with various software tools and platforms.\n- **Vocabularies and Ontologies**: Researchers should use community-accepted vocabularies and ontologies to provide common definitions and concepts, ensuring data can be understood and combined with other datasets more effectively.\n- **Linked Data**: By linking data with other related data and resources, researchers can enrich the context of their datasets, enabling better integration and discovery of interconnected information.\n\n\n\n\n\n\n\n\nFAIR reusable\n\n\nData should be well-documented and prepared for reuse in different contexts. This involves providing detailed information on data collection, processing, and methodology, allowing other researchers to understand and replicate the study. Additionally, licensing and ethical considerations should be clearly stated to enable legal and ethical reuse of the data.\n\n\n\n\n\n\nKey components\n\n\n\n- **Documentation and Provenance**: Data should be accompanied by comprehensive documentation that explains how the data was collected, processed, and analyzed. Provenance information ensures that data users can understand the data's origin and processing history.\n- **Ethical and Legal Considerations**: Researchers should provide clear information about ethical considerations related to data collection and use. Additionally, data should adhere to legal and regulatory requirements, ensuring its responsible and ethical reuse.\n- **Data Licensing**: Researchers should clearly state the licensing terms for data reuse, specifying how others can use, modify, and redistribute the data while respecting intellectual property rights and legal constraints.\n\n\n\n\n\n\nIn this section we will see how we can apply Open Science and FAIR principles to your NGS data. Note that not all data needs to be archived and deposited. NGS data processing generates vast amounts of data that might not need to be share publicly, as long as you describe how you produced the data. For example, in an usual bulk RNAseq experiment, FASTQ reads are cleaned and subsequently aligned to a reference genome, creating a subset of cleaned FASTQ files and BAM files. After transcript/gene quantification, you can obtain a final count matrix that can be used for data analysis, such as Differential Expression Analysis. If you provide enough documentation on how these files were processed (which softwares, which versions and which options), you won‚Äôt need to deposit neither the cleaned nor aligned reads, only the original FASTQ files and the final result of your preprocessing. This will save quite the computational resources and metadata needed to preserve the intermediary data. Providing documentation on how the data was generated is much simpler if you are using community curated pipelines such as the ones created by the nf-core community.\n\n\n\n\n\n\nYou should share at least this data\n\n\n\n- Raw NGS files in fastq format.\n- Final preprocessing results, such as count matrices for RNAseq, peak calling results for ChIPseq/ATACseq, VCF files for variant calling, etc.\n- Metadata about the raw files and final results.\n\nWe will see what metadata you should submit below.\n\n\n\n\nUnless your data is of sensitive nature (human individual samples, patient data, or anything protected), you should always deposit your data with as little restrictions as possible. This includes publishing your manuscript in Open Access journals as well! For your generated NGS data, our suggestion is that you use a license such as Creative Commons licence like CC-BY license, which only requires users to attribute the source of the data, but this also depends on the repository that you use for your data. We will see more about it in the lesson 10\n\n\n\nCC licenses\n\n\n\n\n\nNext we will see how we can apply each of the FAIR principles to your NGS data.\n\n\nTo make your NGS data easy to find, you should deposit it in a domain specific repository, such as the Gene Expression Omnibus or Annotare. We will see more about them in lesson 10. Both of these repositories will help you give your data a unique identifier, and provide information (metadata) on how the data was generated. The metadata you include in your submission should contain, at least, the minimum necessary information to understand what kind of data it is submitted, and how it was generated. This includes:\n\nSample metadata in tabular format, containing information about the samples used in the experiment as well as variables of interest for the analysis.\nExperiment metadata, including data provenance, that is, how the samples were obtain, from which organism, following what protocols, kits, sequencing libraries, sequencing method and data preprocessing workflows/pipelines. This is usually submitted as part of a submission form and it depends on the repository.\nKeywords, such as type of NGS data, conditions or diseases studied in the experiment, organisms used, genes studied, etc.\n\n\n\n\nBoth GEO and Annotare repositories promote the use of unrestricted access to the data. In the case of Annotare, deposited data is under CC0 license, while GEO states deposited data is public domain. Depositing your data will require you to have an account but it does not require authentication from the user to access and download the data.\n\n\n\nAnnotare main page\n\n\n\n\n\n\n\n\nOn sensitive data\n\n\n\nIf you would like to deposit sensitive data that needs controlled access, it is possible to do so through the [European Genome-phenome Archive (EGA)](https://ega-archive.org/submission/).\n\n\nIn addition, if you have deposited your data with rich metadata, as explained in the previous step, it will be easier for users to query your data by date, author, organism, type of NGS data, etc etc.\n\n\n\nBy using standard bioinformatics formats, such as fastq files for raw NGS data, count matrices in tabular format, BED files for peak calling results, etc., you are already complying to this section. In addition, GEO and Annotare repositories are complaint to NGS data standards, such as MIAME/MINSEQE/MINSCE guidelines.\n\n\n\nMINSEQ\n\n\nNonetheless, this is the easy part! Adhering to controlled vocabularies seems to be the most difficult part of the FAIR principles. Here are some cases:\n\nUsing organism names instead of their taxonomy. For example: mouse instead of Mus musculus, or human, instead of Homo sapiens. Even better, we should use a taxonomy ID, such as the NCBI taxonomy ID for human NCBITaxon_9606, which will unequivocally refer to human.\nUsing gene names or symbols instead of gene IDs. For example: the gene POU5F1 has many synonyms, like OCT4, OCT, OTF4. In order to be explicit, it is better to reference the gene ID, like an ENSEMBL gene ID ENSG00000204531.\nUsing disease names instead of disease IDs. Again, this will reference specifically the disease you mention.\n\nThere are many more stances where you can use controlled vocabularies for other variables of interest, like cell type, tissue, cell cycle, etc. We will see in the metadata lesson where you can find controlled vocabularies for different variables of interest in NGS data.\n\n\n\nIn order for your NGS data to be reusable, you will have to provide a thorough documentation on how it was generated, as well as the terms (that is a license) on how the data can be used/retrieved. We have talked already about collecting metadata on how the samples were generated (laboratory protocols, sequencing library, kits, technology, etc) and processed (workflows or pipelines along with the software used, which versions adn options). We also talked on what type of standard file formats you should use, such as fastq files for raw data and tabular formats for sample metadata. Finally, we have discussed in the Open Science section that you should try to license your data as freely as possible, like a CC0 license or CC-BY license. If your data is of sensitive nature and has restricted access or conditions, you should instead provide information on how other people can access the data, as well as any agreements or ethical approvals necessary for its reuse.\n\n\n\ncc-by license\n\n\n\n\n\n\n\nIn this lesson we have learned about Open Science and the FAIR principles and how to apply them in NGS data. In the next lessons, we will see how to specifically apply this knowledge, from organising your data, what metadata to collect and how to share it.",
    "crumbs": [
      "Course material",
      "Open Science and FAIR principles"
    ]
  },
  {
    "objectID": "develop/04_OS_FAIR.html#open-science",
    "href": "develop/04_OS_FAIR.html#open-science",
    "title": "Open Science and FAIR principles",
    "section": "",
    "text": "Funding agencies and regulatory agencies, such as the EU, have been actively promoting Open Science through various policies, recognizing its potential to advance research, innovation, and societal well-being. The reasons for this push are plentiful. Open Science allows research outputs to be accessible to a wider audience, accelerating the impact of research and fostering collaboration among researchers and institutions, as well as encourages transparency and reproducibility, fostering responsible research conduct and mitigating issues like scientific fraud and data manipulation. The reasons are also economical: by promoting Open Access and data sharing, the EU seeks to maximize the return on its research investments by ensuring that knowledge and data are used and built upon by the broader scientific community. This also provides a foundation for innovation and entrepreneurship, enabling businesses and startups to access and leverage research findings for creating new products and services, and allows researchers to integrate and analyze large datasets from different sources, leading to new insights and discoveries.\n\n\n\nOpen Science, Australian Citizen Science Association.\n\n\nMore and more agencies are promoting Open Science initiatives, supporting research that adheres to OS principles. Here are some more examples:\n\nNational Institutes of Health (NIH): In the United States, NIH encourages Open Science practices, such as data sharing, through various policies and initiatives. The NIH Data Sharing Policy requires researchers to share their data with the scientific community.\nWellcome Trust: The Wellcome Trust, a global charitable foundation, has a strong commitment to Open Science. It mandates that research outputs funded by Wellcome must be made openly available through Open Access platforms.\nEuropean Molecular Biology Organization (EMBO): EMBO supports Open Access to research outputs and provides guidelines for best practices in data sharing.\nBill & Melinda Gates Foundation: The foundation advocates for Open Access and data sharing to maximize the impact of its global health and development research.\nEuropean Research Council (ERC): The ERC promotes Open Access to research outputs, requiring grantees to make their publications openly accessible. It also encourages data sharing and adheres to the FAIR principles to ensure the findability, accessibility, interoperability, and reusability of research data.\n\n\n\n\n\n\n\nBenefits of Open Science\n\n\n\nWhile it may seem that this push for Open Science is being enforced by funding agencies to get more bang for their buck, adhering to Open Science practices can offer numerous benefits to you, as a researcher. Some of the key advantages include: - Increased Visibility and Impact: When you share your research openly, more people can access and read your papers and data, increasing the visibility and impact of your work. - Enhanced Collaboration: Open Science encourages collaboration with other researchers, leading to new ideas and projects that can be more impactful. - Build Trust in Your Findings: By sharing your data and methods openly, other researchers can verify and validate your findings, increasing the credibility of your research. - Research Moves Faster: Open Science lets researchers build upon each other‚Äôs work, accelerating the pace of discovery and progress in science. - Inspire New Discoveries: Your shared data can inspire others to explore new research questions and find insights you might not have considered. - Attract Funding Opportunities: More and more funding agencies are supporting Open Science, and following open practices can help you qualify for more funding opportunities. - Transparency and Accountability: Open Science practices promote transparency, accountability, and responsible conduct in research, reducing the risk of scientific misconduct. - Preserve Your Data: By archiving your data in repositories, you ensure that it is preserved and available for future generations of scientists.",
    "crumbs": [
      "Course material",
      "Open Science and FAIR principles"
    ]
  },
  {
    "objectID": "develop/04_OS_FAIR.html#fair-principles",
    "href": "develop/04_OS_FAIR.html#fair-principles",
    "title": "Open Science and FAIR principles",
    "section": "",
    "text": "The FAIR principles are a set of guiding principles designed to enhance the management, sharing, and usability of research data, and could be considered as complementary to Open Science. FAIR stands for Findable, Accessible, Interoperable, and Reusable, and these principles aim to make research data more valuable, impactful, and sustainable. By adhering to the FAIR principles, researchers and institutions can maximize the value and impact of their research data. FAIR data not only benefits individual researchers but also contributes to the broader scientific community by fostering collaboration, data-driven discoveries, and the advancement of knowledge across disciplines. Furthermore, FAIR data supports long-term data preservation, making it valuable for future research and ensuring the sustainability of scientific progress.\nTake note that adhering to the FAIR principles is not a black and white matter. There are different measure of FAIR compliance and some key points are more complicated to accomplish than others. This is specially true when we talk about using metadata standards and controlled vocabularies!\nLet‚Äôs break down each FAIR principle:\n\n\n\n\n\nFAIR findable\n\n\nResearch data should be easy to find and identify. To achieve this, data should be assigned persistent and unique identifiers (e.g., DOIs) and described using standard metadata. Providing clear and comprehensive metadata helps researchers and machines discover relevant data through various search engines and data repositories.\n\n\n\n\n\n\nKey components\n\n\n\n- **Persistent Identifiers**: Research data should be assigned unique and persistent identifiers, such as Digital Object Identifiers (DOIs) or Uniform Resource Identifiers (URIs). These identifiers ensure that data can be located and referenced over time, even if it is moved or its location changes.\n- **Rich Metadata**: Data should be accompanied by comprehensive and standardized metadata that describes the content, context, and characteristics of the data. This metadata includes information about the data's origin, format, version, licensing, and the conditions for reuse.\n- **Data Repository or Catalog**: Data should be deposited in trusted repositories or data catalogs that follow FAIR principles, making it easier to discover and access data from a centralized and reliable source.\n\n\n\n\n\n\n\n\nFAIR accessible\n\n\nData should be openly accessible to anyone who needs it. This means that there should be minimal restrictions on accessing and downloading the data. Open access to data encourages collaboration, enables verification of research findings, and ensures transparency in research processes.\n\n\n\n\n\n\nKey components\n\n\n\n- **Open Access**: Data should be openly accessible to anyone without restrictions or unnecessary barriers. Researchers should choose appropriate licenses or waivers that allow the broadest possible reuse of the data.\n- **Authentication and Authorization**: If access restrictions are necessary (e.g., for sensitive or confidential data), proper authentication and authorization mechanisms should be in place to grant access only to authorized individuals or groups.\n- **Metadata**: Always deposit the metadata for your data. Even if you cannot deposit your data due to restrictions, it is a good idea to deposit the metadata of your data so that at least the record for that data exists, and what kind of data/information contains.\n\n\n\n\n\n\n\n\nImportant note\n\n\n\nAccessible does not equal to free, unrestricted access! There are cases when your data might be protected by GDPR regulations due to its sensitivity! If this is the case, you should, of course not make your data freely available. As said before, it is a great idea to at least deposit and make available the metadata for your data. \n\n*\"As open as possible, as closed as necessary\"* should be your motto.\n\nFor example, imagine that you obtain your sensitive data from a national authority regarding personal information. Most likely, you will not be able to access **all** the data, but you will be interested in a sample of the population that fits your purposes (e.g., people between 20-30 years old, smokers). You cannot publish or deposit your data, but you can publish what was the query you used to obtain this data, and its source!\n\n\n\n\n\n\n\n\nFAIR interoperable\n\n\nData should be structured and formatted in a way that allows it to be used and combined with other data seamlessly. Following standard data formats and adopting widely used vocabularies and ontologies promotes interoperability, enabling integration and comparison of data from different sources.\n\n\n\n\n\n\nKey components\n\n\n\n- **Standard Data Formats**: Data should be stored in standard and widely-used data formats, facilitating data exchange and interoperability with various software tools and platforms.\n- **Vocabularies and Ontologies**: Researchers should use community-accepted vocabularies and ontologies to provide common definitions and concepts, ensuring data can be understood and combined with other datasets more effectively.\n- **Linked Data**: By linking data with other related data and resources, researchers can enrich the context of their datasets, enabling better integration and discovery of interconnected information.\n\n\n\n\n\n\n\n\nFAIR reusable\n\n\nData should be well-documented and prepared for reuse in different contexts. This involves providing detailed information on data collection, processing, and methodology, allowing other researchers to understand and replicate the study. Additionally, licensing and ethical considerations should be clearly stated to enable legal and ethical reuse of the data.\n\n\n\n\n\n\nKey components\n\n\n\n- **Documentation and Provenance**: Data should be accompanied by comprehensive documentation that explains how the data was collected, processed, and analyzed. Provenance information ensures that data users can understand the data's origin and processing history.\n- **Ethical and Legal Considerations**: Researchers should provide clear information about ethical considerations related to data collection and use. Additionally, data should adhere to legal and regulatory requirements, ensuring its responsible and ethical reuse.\n- **Data Licensing**: Researchers should clearly state the licensing terms for data reuse, specifying how others can use, modify, and redistribute the data while respecting intellectual property rights and legal constraints.",
    "crumbs": [
      "Course material",
      "Open Science and FAIR principles"
    ]
  },
  {
    "objectID": "develop/04_OS_FAIR.html#open-science-and-fair-principles-applied-to-ngs-data",
    "href": "develop/04_OS_FAIR.html#open-science-and-fair-principles-applied-to-ngs-data",
    "title": "Open Science and FAIR principles",
    "section": "",
    "text": "In this section we will see how we can apply Open Science and FAIR principles to your NGS data. Note that not all data needs to be archived and deposited. NGS data processing generates vast amounts of data that might not need to be share publicly, as long as you describe how you produced the data. For example, in an usual bulk RNAseq experiment, FASTQ reads are cleaned and subsequently aligned to a reference genome, creating a subset of cleaned FASTQ files and BAM files. After transcript/gene quantification, you can obtain a final count matrix that can be used for data analysis, such as Differential Expression Analysis. If you provide enough documentation on how these files were processed (which softwares, which versions and which options), you won‚Äôt need to deposit neither the cleaned nor aligned reads, only the original FASTQ files and the final result of your preprocessing. This will save quite the computational resources and metadata needed to preserve the intermediary data. Providing documentation on how the data was generated is much simpler if you are using community curated pipelines such as the ones created by the nf-core community.\n\n\n\n\n\n\nYou should share at least this data\n\n\n\n- Raw NGS files in fastq format.\n- Final preprocessing results, such as count matrices for RNAseq, peak calling results for ChIPseq/ATACseq, VCF files for variant calling, etc.\n- Metadata about the raw files and final results.\n\nWe will see what metadata you should submit below.\n\n\n\n\nUnless your data is of sensitive nature (human individual samples, patient data, or anything protected), you should always deposit your data with as little restrictions as possible. This includes publishing your manuscript in Open Access journals as well! For your generated NGS data, our suggestion is that you use a license such as Creative Commons licence like CC-BY license, which only requires users to attribute the source of the data, but this also depends on the repository that you use for your data. We will see more about it in the lesson 10\n\n\n\nCC licenses\n\n\n\n\n\nNext we will see how we can apply each of the FAIR principles to your NGS data.\n\n\nTo make your NGS data easy to find, you should deposit it in a domain specific repository, such as the Gene Expression Omnibus or Annotare. We will see more about them in lesson 10. Both of these repositories will help you give your data a unique identifier, and provide information (metadata) on how the data was generated. The metadata you include in your submission should contain, at least, the minimum necessary information to understand what kind of data it is submitted, and how it was generated. This includes:\n\nSample metadata in tabular format, containing information about the samples used in the experiment as well as variables of interest for the analysis.\nExperiment metadata, including data provenance, that is, how the samples were obtain, from which organism, following what protocols, kits, sequencing libraries, sequencing method and data preprocessing workflows/pipelines. This is usually submitted as part of a submission form and it depends on the repository.\nKeywords, such as type of NGS data, conditions or diseases studied in the experiment, organisms used, genes studied, etc.\n\n\n\n\nBoth GEO and Annotare repositories promote the use of unrestricted access to the data. In the case of Annotare, deposited data is under CC0 license, while GEO states deposited data is public domain. Depositing your data will require you to have an account but it does not require authentication from the user to access and download the data.\n\n\n\nAnnotare main page\n\n\n\n\n\n\n\n\nOn sensitive data\n\n\n\nIf you would like to deposit sensitive data that needs controlled access, it is possible to do so through the [European Genome-phenome Archive (EGA)](https://ega-archive.org/submission/).\n\n\nIn addition, if you have deposited your data with rich metadata, as explained in the previous step, it will be easier for users to query your data by date, author, organism, type of NGS data, etc etc.\n\n\n\nBy using standard bioinformatics formats, such as fastq files for raw NGS data, count matrices in tabular format, BED files for peak calling results, etc., you are already complying to this section. In addition, GEO and Annotare repositories are complaint to NGS data standards, such as MIAME/MINSEQE/MINSCE guidelines.\n\n\n\nMINSEQ\n\n\nNonetheless, this is the easy part! Adhering to controlled vocabularies seems to be the most difficult part of the FAIR principles. Here are some cases:\n\nUsing organism names instead of their taxonomy. For example: mouse instead of Mus musculus, or human, instead of Homo sapiens. Even better, we should use a taxonomy ID, such as the NCBI taxonomy ID for human NCBITaxon_9606, which will unequivocally refer to human.\nUsing gene names or symbols instead of gene IDs. For example: the gene POU5F1 has many synonyms, like OCT4, OCT, OTF4. In order to be explicit, it is better to reference the gene ID, like an ENSEMBL gene ID ENSG00000204531.\nUsing disease names instead of disease IDs. Again, this will reference specifically the disease you mention.\n\nThere are many more stances where you can use controlled vocabularies for other variables of interest, like cell type, tissue, cell cycle, etc. We will see in the metadata lesson where you can find controlled vocabularies for different variables of interest in NGS data.\n\n\n\nIn order for your NGS data to be reusable, you will have to provide a thorough documentation on how it was generated, as well as the terms (that is a license) on how the data can be used/retrieved. We have talked already about collecting metadata on how the samples were generated (laboratory protocols, sequencing library, kits, technology, etc) and processed (workflows or pipelines along with the software used, which versions adn options). We also talked on what type of standard file formats you should use, such as fastq files for raw data and tabular formats for sample metadata. Finally, we have discussed in the Open Science section that you should try to license your data as freely as possible, like a CC0 license or CC-BY license. If your data is of sensitive nature and has restricted access or conditions, you should instead provide information on how other people can access the data, as well as any agreements or ethical approvals necessary for its reuse.\n\n\n\ncc-by license",
    "crumbs": [
      "Course material",
      "Open Science and FAIR principles"
    ]
  },
  {
    "objectID": "develop/04_OS_FAIR.html#wrap-up",
    "href": "develop/04_OS_FAIR.html#wrap-up",
    "title": "Open Science and FAIR principles",
    "section": "",
    "text": "In this lesson we have learned about Open Science and the FAIR principles and how to apply them in NGS data. In the next lessons, we will see how to specifically apply this knowledge, from organising your data, what metadata to collect and how to share it.",
    "crumbs": [
      "Course material",
      "Open Science and FAIR principles"
    ]
  },
  {
    "objectID": "develop/05_DMP.html",
    "href": "develop/05_DMP.html",
    "title": "Data Management Plans",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n1. Learn what is a DMP 2. Learn about the different DMP templates 3. How to write a DMP focused on NGS data\n\n\nThere are a lot of terms regarding data management and a lot of best practices to collect and implement, but how do we gather all the decisions made and how do we know that we have covered it all, that our data will be well managed throughout its life cycle? The answer is to write a data management plan (DMP).\nA DMP is a document addressing requirements and practices for managing the project‚Äôs data, code and documentation, throughout the data life cycle, i.e from the initial planning until the project ends and beyond. It outlines the data management strategies in a project. Making plans for how you will collect, document, organize, and preserve your data are all part of the data management strategy.\n\n\nThere are several reasons why writing a data management plan is a very good idea:\n\nThink of the DMP as a checklist. Going through the checklist allows you to identify gaps in current data management strategies. Identifying the gaps early on saves a lot of headache and time spent later. Going through the process of planning is more important than the actual plan itself.\nIn a project with several members, it is important to decide on standards that all collaborators should adhere to, e.g.¬†regarding how to organise the data, how to name it, which metadata standards to use, what vocabularies to use, etc.\nWriting a DMP also enables you to estimate costs regarding data production, storage, data management, etc.\nIt is also a good way to clarify responsibilities regarding the data and the data management, e.g.¬†who is responsible for the execution of the DMP.\nBy planning how the data will be managed, there‚Äôs greater chance that the research data will be well-managed (no guarantee, since you still need to have good strategies and actually implement them for this to happen). Of course there are many benefits with well-managed data but the main ones are:\n\nreproducibility, so that the results can be verified\nreusability, so that this data can be used for answering other scientific questions, thus reducing redundancy\nA DMP is the first step towards being FAIR in your project.\n\n\nIf the reasons above don‚Äôt persuade you, the last argument is that it is becoming more and more a requirement by funders and other stakeholders:\n\nFor transparency and openness: publicly funded research data must be discoverable, accessible, and reusable to the public\nReturn on investment: well planned data maximizes the research potential of the data and provides greater returns on public investments and research.\n\n\n\n\nA DMP is a living document, the initial version is written the same time as a new project idea is emerging, before e.g.¬†applying for funds, and then successively updated as the project continues and new decisions are made. Ideally it should be updated continuously, but there are three major time points:\n\nProject planning: The DMP should outline the strategies for data management in sufficient detail to be able to estimate the resources needed to implement the DMP, so that this can be included in the proposal for funding (e.g.¬†data production, data analysis, storage during and after project, costs related to publishing of data).\nProject start: The DMP is completed with more details e.g.¬†about documentation, data quality measures, file and folder strategies, etc.\nProject end: The DMP is updated a final time with e.g.¬†links to published data and details about archiving (what data and where), so that this document enables future re-use of the project (by yourself or others).\n\n\n\n\n\nDescription of data\n\nWhat types of data will be created and/or collected, in terms of data format and amount/volume of data?\n\nDocumentation\n\nHow will the material be documented and described, with associated metadata relating to structure, standards and format for descriptions of the content, collection method, etc.?\n\nStorage and backup\n\nHow is data security, storage and backup of data and metadata safeguarded during the research process?\n\nLegal and ethical aspects\n\nHow is data handling according to legal requirements safeguarded, e.g.¬†in terms of handling of personal data, confidentiality and intellectual property rights?\n\nAccessibility and long-term storage\n\nHow, when and where will research data or information about data (i.e.¬†metadata) be made accessible? E.g. via deposition to international public repositories.\nIn what way is long-term storage safeguarded, and by whom?\n\nResponsibility and resources\n\nWho are the responsible persons for data management?\nWhat resources (costs, labour input or other) will be required for data management?\n\n\n\n\n\nDifferent agencies and funders may have different DMP templates that are mandatory for researchers to comply with data management requirements. Here you can find some examples.\n\n\nThe University of Copenhagen has developed a DMP template in alignment with the UCPH Policy on Research Data Manage‚Äãment. This is the recommended template for all researchers, guests and students involved in research activities at UCPH, unless other specific DMP requirements from a funding agency or a partner institution apply.\nThe UCPH DMP template is available within the DMPonline tool (see information below) or can be downloaded as a Word document.\nGuidance for all questions in this template can be found here.\n\n\n\nAll projects funded in the Horizon Europe framework programme must prepare and submit a data management plan as deliverable according to the details described in the respective grant agreement.\n\n\n\n\nStandard DMP templates can typically be found at funder agencies, e.g.¬†Swedish Research Council and Science Europe, and it is of course possible to write in your favorite text editor. In addition, there are several tools that can assist you in writing a DMP as well as guidance.\n!!! tip ‚ÄúTools to write your DMP‚Äù\n- **DMPonline**: DMPOnline is an online tool for writing, sharing and reviewing data management plans. A Danish installation is available under [dmponline.deic.dk](https://dmponline.deic.dk/). It is provided by the [Danish e-Infrastructure Cooperation](https://deic.dk/) (DeiC) and jointly administered by the Royal Danish Library and DTU Library. DMPonline contains a number of different DMP templates, guidance texts and examples from Danish research institutions and relevant funders.  \n    - The tool most universities have chosen to offer (check with your institute)\n    - Good guidance but typically generic and not Life Science specific\n    - Most often free text answers\n    - Contains guidance for several DMP templates\n- [Data Stewardship wizard](http://dsw.scilifelab.se/). DSW is a interactive tool to create data management plans based on questionnaires. \n    - Provided by [SciLifeLab](https://www.scilifelab.se)\n    - Gives Life Science specific guidance\n    - Less free text answers, instead many questions with answer options\n\n\n\nWe are have written a DMP template that it is prefilled with repetitive information using DMPonline and the Horizon Europe guidelines. This template contains all the necessary information regarding common practices that we will use, the repositories we use for NGS, etc. The template is part of the project folder template, under documents. You can check the file here.\nThe Horizon Europe template is mostly focused on digital data and so, it is maybe not the best option if you are also interested in recording in your DMP physical data, such as samples, reagents, media, cultures, model organisms, etc.\n!!! question ‚ÄúExercise: write a draft for a DMP‚Äù\nYou can write a DMP draft from scratch, or you could use the template mentioned above and modify it to your own needs!\n\n\n\nIn this lesson we have learned about DMPs and tools that could help you write one, as well as shown you a possible template for your own projects! DMPs are a great tool to help you think about your project, the data you will reuse/generate, and how to preserve it properly for future use, either by you, collaborators or the broad scientific community. In the next lesson we will explore how you could organize your NGS data using a simple but clear system, and provide you some tools that will make the job easier.\n\nSome parts of this lesson are taken from the NBISweden workshop on RDM practices."
  },
  {
    "objectID": "develop/05_DMP.html#why-are-dmps-important",
    "href": "develop/05_DMP.html#why-are-dmps-important",
    "title": "Data Management Plans",
    "section": "",
    "text": "There are several reasons why writing a data management plan is a very good idea:\n\nThink of the DMP as a checklist. Going through the checklist allows you to identify gaps in current data management strategies. Identifying the gaps early on saves a lot of headache and time spent later. Going through the process of planning is more important than the actual plan itself.\nIn a project with several members, it is important to decide on standards that all collaborators should adhere to, e.g.¬†regarding how to organise the data, how to name it, which metadata standards to use, what vocabularies to use, etc.\nWriting a DMP also enables you to estimate costs regarding data production, storage, data management, etc.\nIt is also a good way to clarify responsibilities regarding the data and the data management, e.g.¬†who is responsible for the execution of the DMP.\nBy planning how the data will be managed, there‚Äôs greater chance that the research data will be well-managed (no guarantee, since you still need to have good strategies and actually implement them for this to happen). Of course there are many benefits with well-managed data but the main ones are:\n\nreproducibility, so that the results can be verified\nreusability, so that this data can be used for answering other scientific questions, thus reducing redundancy\nA DMP is the first step towards being FAIR in your project.\n\n\nIf the reasons above don‚Äôt persuade you, the last argument is that it is becoming more and more a requirement by funders and other stakeholders:\n\nFor transparency and openness: publicly funded research data must be discoverable, accessible, and reusable to the public\nReturn on investment: well planned data maximizes the research potential of the data and provides greater returns on public investments and research."
  },
  {
    "objectID": "develop/05_DMP.html#when-to-write-a-dmp",
    "href": "develop/05_DMP.html#when-to-write-a-dmp",
    "title": "Data Management Plans",
    "section": "",
    "text": "A DMP is a living document, the initial version is written the same time as a new project idea is emerging, before e.g.¬†applying for funds, and then successively updated as the project continues and new decisions are made. Ideally it should be updated continuously, but there are three major time points:\n\nProject planning: The DMP should outline the strategies for data management in sufficient detail to be able to estimate the resources needed to implement the DMP, so that this can be included in the proposal for funding (e.g.¬†data production, data analysis, storage during and after project, costs related to publishing of data).\nProject start: The DMP is completed with more details e.g.¬†about documentation, data quality measures, file and folder strategies, etc.\nProject end: The DMP is updated a final time with e.g.¬†links to published data and details about archiving (what data and where), so that this document enables future re-use of the project (by yourself or others)."
  },
  {
    "objectID": "develop/05_DMP.html#the-main-parts-of-a-dmp",
    "href": "develop/05_DMP.html#the-main-parts-of-a-dmp",
    "title": "Data Management Plans",
    "section": "",
    "text": "Description of data\n\nWhat types of data will be created and/or collected, in terms of data format and amount/volume of data?\n\nDocumentation\n\nHow will the material be documented and described, with associated metadata relating to structure, standards and format for descriptions of the content, collection method, etc.?\n\nStorage and backup\n\nHow is data security, storage and backup of data and metadata safeguarded during the research process?\n\nLegal and ethical aspects\n\nHow is data handling according to legal requirements safeguarded, e.g.¬†in terms of handling of personal data, confidentiality and intellectual property rights?\n\nAccessibility and long-term storage\n\nHow, when and where will research data or information about data (i.e.¬†metadata) be made accessible? E.g. via deposition to international public repositories.\nIn what way is long-term storage safeguarded, and by whom?\n\nResponsibility and resources\n\nWho are the responsible persons for data management?\nWhat resources (costs, labour input or other) will be required for data management?"
  },
  {
    "objectID": "develop/05_DMP.html#dmp-templates",
    "href": "develop/05_DMP.html#dmp-templates",
    "title": "Data Management Plans",
    "section": "",
    "text": "Different agencies and funders may have different DMP templates that are mandatory for researchers to comply with data management requirements. Here you can find some examples.\n\n\nThe University of Copenhagen has developed a DMP template in alignment with the UCPH Policy on Research Data Manage‚Äãment. This is the recommended template for all researchers, guests and students involved in research activities at UCPH, unless other specific DMP requirements from a funding agency or a partner institution apply.\nThe UCPH DMP template is available within the DMPonline tool (see information below) or can be downloaded as a Word document.\nGuidance for all questions in this template can be found here.\n\n\n\nAll projects funded in the Horizon Europe framework programme must prepare and submit a data management plan as deliverable according to the details described in the respective grant agreement."
  },
  {
    "objectID": "develop/05_DMP.html#writing-a-dmp",
    "href": "develop/05_DMP.html#writing-a-dmp",
    "title": "Data Management Plans",
    "section": "",
    "text": "Standard DMP templates can typically be found at funder agencies, e.g.¬†Swedish Research Council and Science Europe, and it is of course possible to write in your favorite text editor. In addition, there are several tools that can assist you in writing a DMP as well as guidance.\n!!! tip ‚ÄúTools to write your DMP‚Äù\n- **DMPonline**: DMPOnline is an online tool for writing, sharing and reviewing data management plans. A Danish installation is available under [dmponline.deic.dk](https://dmponline.deic.dk/). It is provided by the [Danish e-Infrastructure Cooperation](https://deic.dk/) (DeiC) and jointly administered by the Royal Danish Library and DTU Library. DMPonline contains a number of different DMP templates, guidance texts and examples from Danish research institutions and relevant funders.  \n    - The tool most universities have chosen to offer (check with your institute)\n    - Good guidance but typically generic and not Life Science specific\n    - Most often free text answers\n    - Contains guidance for several DMP templates\n- [Data Stewardship wizard](http://dsw.scilifelab.se/). DSW is a interactive tool to create data management plans based on questionnaires. \n    - Provided by [SciLifeLab](https://www.scilifelab.se)\n    - Gives Life Science specific guidance\n    - Less free text answers, instead many questions with answer options"
  },
  {
    "objectID": "develop/05_DMP.html#example-of-a-dmp-for-ngs-data",
    "href": "develop/05_DMP.html#example-of-a-dmp-for-ngs-data",
    "title": "Data Management Plans",
    "section": "",
    "text": "We are have written a DMP template that it is prefilled with repetitive information using DMPonline and the Horizon Europe guidelines. This template contains all the necessary information regarding common practices that we will use, the repositories we use for NGS, etc. The template is part of the project folder template, under documents. You can check the file here.\nThe Horizon Europe template is mostly focused on digital data and so, it is maybe not the best option if you are also interested in recording in your DMP physical data, such as samples, reagents, media, cultures, model organisms, etc.\n!!! question ‚ÄúExercise: write a draft for a DMP‚Äù\nYou can write a DMP draft from scratch, or you could use the template mentioned above and modify it to your own needs!"
  },
  {
    "objectID": "develop/05_DMP.html#wrap-up",
    "href": "develop/05_DMP.html#wrap-up",
    "title": "Data Management Plans",
    "section": "",
    "text": "In this lesson we have learned about DMPs and tools that could help you write one, as well as shown you a possible template for your own projects! DMPs are a great tool to help you think about your project, the data you will reuse/generate, and how to preserve it properly for future use, either by you, collaborators or the broad scientific community. In the next lesson we will explore how you could organize your NGS data using a simple but clear system, and provide you some tools that will make the job easier.\n\nSome parts of this lesson are taken from the NBISweden workshop on RDM practices."
  },
  {
    "objectID": "cards/JARomero.html",
    "href": "cards/JARomero.html",
    "title": "Jos√© Alejandro Romero Herrera",
    "section": "",
    "text": "Alex is a former Sandbox data scientist at the University of Copenhagen. He is currently working at Lundbeck as a principal bioinformatician."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to RDM for biodata",
    "section": "",
    "text": "Welcome to RDM for biodata\n\n\n\n\n\n\n\nWe offer workshops on practical RDM for biodata. Keep an eye on the upcoming events on the Sandbox website.\n\n\n\n\nThe course ‚ÄúResearch Data Management (RDM) for biological data‚Äù is designed to provide participants with foundational knowledge and practical skills in handling the extensive data generated by modern studies. It emphasizes the importance of Open Science and FAIR principles in managing data effectively. This course covers essential principles and best practices guidelines in data organization, metadata annotation, version control, and data preservation. These principles are explored from a computational perspective, ensuring participants gain hands-on experience in applying them to real-world scenarios in their research labs, hence, helping them in their daily data analysis work. Additionally, the course delves into FAIR principles and Open Science, promoting collaboration and reproducibility in research endeavors. By the course‚Äôs conclusion, attendees will possess essential tools and techniques to address the data challenges prevalent in today‚Äôs research landscape, with a focus on fields related to omics, health and bioinformatics.\n\n\n\n\n\n\nCourse Overview\n\n\n\n\nüìñ Syllabus:\n\n\nData Lifecycle Management\nData Management Plans (DMPs)\nData Organization and storage\nDocumentation standards for biodata\nVersion Control and Collaboration\nProcessing and analyzing biodata\nStoring and sharing biodata\n\n\n‚è∞ Total Time Estimation: X hours\n\nüìÅ Supporting Materials:\n\nüë®‚Äçüíª Target Audience: Ph.D., MSc, anyone interested in RDM for NGS data or other related fields within bioinformatics.\nüë©‚Äçüéì Level: Beginner.\nüîí License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\nüí∞ Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).\n\n\n\n\nThis course offers participants with an in-depth introduction to effectively managing the vast amounts of data generated in modern studies. Throughout the program, emphasis is placed on practical understanding of RDM principles and the importance of efficient handling of large datasets. In this context, participants will learn the necessity of adopting Open Science and FAIR principles for enhancing data accessibility and reusability Special attention is given to the development of Data Management Plans (DMPs) with examples tailored to omics data, ensuring compliance with institutional and funding agency requirements while maintaining data integrity.\nDespite DMPs being essential, they are often too general and lack specific guidelines for practical implementation. That is why we have designed this course to cover practical aspects in detail. Participants will acquire practical skills for organizing data, including the creation of folder and file structures, and the implementation of metadata to facilitate data discoverability and interpretation. Attendees will also gain insights into the establishment of simple databases and the use of version control systems to track changes in data analysis, thereby promoting collaboration and reproducibility. The course concludes with a focus on archiving and data repositories, enabling participants to learn strategies for preserving and sharing data for long-term scientific usage. By the end of the course, attendees will be equipped with essential tools and techniques to effectively navigate the challenges prevalent in today‚Äôs research landscape. This will not only foster successful data management practices but also enhance collaboration within the scientific community.\n\n\n\n\n\n\nCourse Requirements\n\n\n\n\nBasic understanding Next Generation Sequencing data and formats.\nCommand Line experience\nBasic programming experience\nQuarto or Mkdocs tools\n\n\n\n\n\n\n\n\n\nCourse Goals\n\n\n\nBy the end of this workshop, you should be able to apply the following concepts in the context of Next Generation Sequencing data:\n\nUnderstand the Importance of Research Data Management (RDM)\nFamiliarize Yourself with FAIR and Open Science Principles\nDraft a Data Management Plan for your own Data\nEstablish File and Folder Naming Conventions\nEnhance Data with Descriptive Metadata\nImplement Version Control for Data Analysis\nSelect an Appropriate Repository for Data Archiving\nMake your data analysis and workflows reproducible and FAIR\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is a computational workshop that focuses primarily on the digital aspect of our data. While wet lab Research Data Management (RDM) involving protocols, instruments, reagents, ELM or LIMS systems is integral to the entire RDM process, it won‚Äôt be covered in this course.\nAs part of effective data management, it‚Äôs crucial to prioritize strategies that ensure security and privacy. While these aspects are important, please note that they won‚Äôt be covered in our course. However, we highly recommend enrolling in the GDPR course offered by Center for Health Data Science, specially if you‚Äôre working with sensitive data. This course specifically focuses on GDPR compliance and will provide you with valuable insights and skills in managing data privacy and security.\n\n\n\nDanish institutional RDM links\n\nUniversity of Copenhagen\nUniversity Library of Southern Denmark\nTechnical University of Denmark\nAalborg University\nAarhus University\n\n\n\nAcknowledgements\n\nRDMkit, ELIXIR (2021) Research Data Management Kit. A deliverable from the EU-funded ELIXIR-CONVERGE project (grant agreement 871075).\nUniversity of Copenhagen Research Data Management Team.\nMartin Proks and Sarah Lundregan, Brickman Lab, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nRichard Dennis, Data Steward, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nNBISweden.\n\n\n\n\n\n\nCopyrightCC-BY-SA 4.0 license"
  },
  {
    "objectID": "develop/06_file_structure.html",
    "href": "develop/06_file_structure.html",
    "title": "Best Practices for Data Storage",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n1. Learn how to organize your NGS data.\n2. Separate your data into projects and assays.\n3. Manage genomic resources.\n4. Learn general rules for naming files and folders.\n5. Create your own rules for naming specific results and figures."
  },
  {
    "objectID": "develop/06_file_structure.html#folder-organization",
    "href": "develop/06_file_structure.html#folder-organization",
    "title": "Best Practices for Data Storage",
    "section": "Folder organization",
    "text": "Folder organization\nYou will probably want to divide your NGS data into three different types of folders:\n\nAssay folders: This folder contains the raw and processed NGS datasets, as well as the pipeline/workflow used to generate the processed data, provenance of the raw data and quality control reports of the data. This data should be locked and read-only to prevent unwanted modifications.\nProject folders: This folder contains all the necessary files for a specific research project. A project may use several assays or results from other projects. The assay data should not be copied or duplicated, but linked from the original source.\nGenomic resources folders: This folder contains common genomic resources, such as genome references (fasta files) and annotations (gtf files) for different species, as well as indexes for different alignment algorithms. This data should also be locked and read-only to prevent unwanted modifications.\n\nThis will help you to keep your data tidied up, specially if you are working on a big lab where assays may be used for different purposes and different people!"
  },
  {
    "objectID": "develop/06_file_structure.html#assay-folder",
    "href": "develop/06_file_structure.html#assay-folder",
    "title": "Best Practices for Data Storage",
    "section": "Assay folder",
    "text": "Assay folder\nFor each NGS experiment there should be an Assay folder that will contain all experimental datasets, that is, an Assay (raw files and pipeline processed files). Raw files should not be modified at all, but you should probably lock modifications to the final results once you are done with preprocessing the data. This will help you prevent unwanted modifications to the data. Each Assay subfolder should be named in a way that it is unique, easily readable, distinguishable and understood at a glance. For example, you could name an NGS assay using an acronym for the type of NGS assay (RNAseq, ChIPseq, ATACseq), a keyword that represents a unique descriptive element of that assay, and the date. Like this:\n&lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD\nFor example CHIP_Oct4_20230101 is a ChIPseq assay made on 1st January 2023 with the keyword Oct4, so it is easily identifiable by eye.\n\nAssay ID code names\nBelow is a example list of different Assay-ID code names. You are welcome to use it and expand it as you wish!\n\nCHIP: ChIP-seq\nRNA: RNA-seq\nATAC: ATAC-seq\nSCR: scRNA-seq\nPROT: Mass Spectrometry Assay\nCAT: Cut&Tag\nCAR: Cut&Run\nRIME: Rapid Immunoprecipitation Mass spectrometry of Endogenous proteins\n‚Ä¶\n\n\n\nAssay folder structure\nNext, let‚Äôs take a look at a possible folder structure and what kind of files you can find there.\nCHIP_Oct4_20230101/\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ metadata.yml\n‚îú‚îÄ‚îÄ pipeline.md\n‚îú‚îÄ‚îÄ processed\n‚îî‚îÄ‚îÄ raw\n   ‚îú‚îÄ‚îÄ .fastq.gz\n   ‚îî‚îÄ‚îÄ samplesheet.csv\n\nREADME.md: Long description of the assay in markdown format. It should contain provenance of the raw NGS data (samples, laboratory protocols used, aim of the assay, etc)\nmetadata.yml: metadata file for the assay describing different keys and important information regarding that assay (see this lesson).\npipeline.md: description of the pipeline used to process raw data, as well as the commands used to run the pipeline.\nprocessed: folder with results of the preprocessing pipeline. Contents depend on the pipeline used.\nraw: folder with the raw data.\n\n.fastq.gz:In the case of NGS assays, there should be fastq files.\nsamplesheet.csv: file that contains metadata information for the samples. This file is used to run the nf-core pipelines. You can also add extra columns with info regarding the experimental variables and batches so it can be used for downstream analysis as well.\n\n\nThis folder structure is simple and straight forward, and files will be exactly where you expect them to be! By including a description and metadata file, you will always be able to understand what this experiment was and how it was generated. This information will be specially useful once you need to submit your data for archiving or write a manuscript. The metadata file will be really handy if you want to collect information on all your NGS datasets and create a database (see this lesson)!\nNote that the processed folder is not expanded upon. This folder will be very dependant on the workflows/pipelines that you use. We strongly recommend that you use reproducible and community curated pipelines, such as the ones developed by the nf-core community, which have a through documentation on the results they produce. For example, imagine you have run an RNAseq experiment and processed the data using the nf-core:rnaseq pipeline with the pseudoalignment and quantification option:\nprocessed/\n‚îú‚îÄ‚îÄ fastqc/\n‚îú‚îÄ‚îÄ multiqc/\n‚îú‚îÄ‚îÄ pipeline_info/\n‚îú‚îÄ‚îÄ salmon/\n‚îî‚îÄ‚îÄ trimgalore/\n\nfastqc: Quality Control results of the raw fastq files.\nmultiqc: Full compilation of the Quality Control checks for all your samples for the entire pipeline.\npipeline_info: Information and logs about the pipeline used for each of the steps of the workflow.\nsalmon: Pseudoalignment and quantification results of the salmon algorithm.\ntrimgalore: Cleaned fastq files and Quality Control results of the cleaned files.\n\nBy using a standardized pipeline, you should be able to always find the preprocessing results where you expect them to be! No need to come up with your own file organization (and document it) or worry that your colleagues (or your future self) won‚Äôt be able to find the data."
  },
  {
    "objectID": "develop/06_file_structure.html#project-folder",
    "href": "develop/06_file_structure.html#project-folder",
    "title": "Best Practices for Data Storage",
    "section": "Project folder",
    "text": "Project folder\nOn the other hand, we have the other type of folder called Projects. In this folder you will save a subfolder for each project that you (or your lab) works on. Each Project subfolder will contain project information and all the data analysis notebooks and scripts used in that project.\nProjects and Assays are separated from each other because a project may use one or more assays to answer a scientific question, and assays may be reused several times in different projects. This could be, for example, all the data analysis related to a publication (a RNAseq and a ChIPseq experiment), or a comparison between a previous ATACseq experiment (which was used for a older project) with a new laboratory protocol.\nAs like for an Assay folder, the Project folder should be named in a way that it is unique, easily readable, distinguishable and understood at a glance. For example, you could name it after the main author initials, a keyword that represents a unique descriptive element of that assay, and the date:\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\nFor example, JARH_Oct4_20230101, is a project about the gene Oct4 owned by Jose Alejandro Romero Herrera, created on the 1st of January of 2023.\n\nProject folder structure\nNext, let‚Äôs take a look at a possible folder structure and what kind of files you can find there.\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n‚îú‚îÄ‚îÄ data\n‚îÇ  ‚îî‚îÄ‚îÄ &lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD/\n‚îú‚îÄ‚îÄ documents\n‚îÇ  ‚îî‚îÄ‚îÄ Non-sensitive_NGS_research_project_template.docx\n‚îú‚îÄ‚îÄ notebooks\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis.rmd\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ reports\n‚îÇ  ‚îú‚îÄ‚îÄ figures\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis/\n‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ heatmap_sampleCor_20230102.png\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis.html\n‚îú‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ results\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis/\n‚îÇ      ‚îî‚îÄ‚îÄ DEA_treat-control_LFC1_p01.tsv\n‚îú‚îÄ‚îÄ scripts\n‚îî‚îÄ‚îÄ metadata.yml\n\ndata: folder that contains symlinks or shortcuts to where the data is, avoiding copying and modification of original files.\ndocuments: folder containing word documents, slides or pdfs related to the project, such as explanations of the data or project, papers, etc. It also contains your Data Management Plan.\n\nNon-sensitive_NGS_research_project_template.docx. This is a pre-filled Data Management Plan based on the Horizon Europe guidelines.\n\nnotebooks: folder containing Jupyter, R markdown or Quarto notebooks with the actual data analysis.\nREADME.md: detailed description of the project in markdown format.\nreports: notebooks rendered as html/docx/pdf versions, ideal for sharing with colleagues and also as a formal report of the data analysis procedure.\n\nfigures: figures produced upon rendering notebooks. The figures will be saved under a subfolder named after the notebook that created them. This is for provenance purposes so we know which notebook created which figures.\n\nrequirements.txt: file explaining what software and libraries/packages and their versions are necessary to reproduce the code.\nresults: results from the data analysis, such as tables with differentially expressed genes, enrichment results, etc.\nscripts: folder containing helper scripts needed to run data analysis or reproduce the work of the folder\ndescription.yml: short description of the project.\nmetadata.yml: metadata file for the assay describing different keys (see this lesson).\n\n\n\n\n\n\n\nNote on the data folder\n\n\n\nWhat do we mean with shortcuts to the data is? If you have divided your Assays and Projects as suggested, you do not really need to copy or duplicate your data again, simply make a shortcut to the original folder, and you will have access to the data as if it is in the project folder! You can use this command in Linux or MacOS to create a folder softlink:\n\n```{.bash}\nln -s path/to/assays/Assays/&lt;ASSAY_ID&gt; /path/to/projects/Projects/&lt;PROJECT_ID&gt;/data/\n```\n\n\n\n\n\n\n\n\nNote on the notebooks folder\n\n\n\nUsing annotated notebooks is ideal for reproducibility and readability purposes. Notebooks should be labeled numerically in order they were created, or the order of the data analysis steps, if they are related to each other e.g. `00_preprocessing.rmd` - `01_Differential_Expression_Analysis.rmd` - `02_Functional_Analysis.rmd`\n\n\n\n\n\n\n\n\nNote on the results folder\n\n\n\nResults from your code notebooks should be saved under this folder. Create a subfolder named after the notebook that created them, so you can always identify which notebook created which results!"
  },
  {
    "objectID": "develop/06_file_structure.html#template-engine",
    "href": "develop/06_file_structure.html#template-engine",
    "title": "Best Practices for Data Storage",
    "section": "Template engine",
    "text": "Template engine\nYou may be thinking right now that doing this kind of setup every time you need to create a new folder would be very tedious, and you would be right! Fortunately, it is very easy to create a folder template using cookiecutter. Cookiecutter is a command-line utility that creates projects from cookiecutters (that is, a template), e.g.¬†creating a Python package project from a Python package project template. Here you can find an example of a cookiecutter folder template directed to NGS data, where we have applied the structures explained in the previous sections. You are very welcome to adapt it or modify it to your needs!\nNow, using only cookiecutter might have some issues. If you update the template, it might be really hard to maintain older folders. By using another tool, cruft, when generating assay and project folders, it allows the user to validate and syncronize old templates with the latest version.\n\nQuick tutorial on cookiecutter\nCreating a Cookiecutter template from scratch involves defining a folder structure, creating a cookiecutter.json file, and specifying the placeholders (keywords) that will be replaced during project generation. Let‚Äôs walk through the process step by step:\n\nStep 1: Create a Folder Template\nStart by creating a folder with the structure you want for your template. For example, let‚Äôs create a simple Python project template:\nmy_template/\n|-- {{cookiecutter.project_name}}\n|   |-- main.py\n|-- tests\n|   |-- test_{{cookiecutter.project_name}}.py\n|-- README.md\nIn this example, {cookiecutter.project_name} is a placeholder that will be replaced with the actual project name when the template is used.\n\n\nStep 2: Create cookiecutter.json\nIn the root of your template folder, create a file named cookiecutter.json. This file will define the variables (keywords) that users will be prompted to fill in. For our Python project template, it might look like this:\n{\n  \"project_name\": \"MyProject\",\n  \"author_name\": \"Your Name\",\n  \"description\": \"A short description of your project\"\n}\nThese are the questions users will be asked when generating a project based on your template. The values provided here will be used to replace the corresponding placeholders in the template files.\nIn addition to replacing placeholders in file and directory names, Cookiecutter can also automatically fill in information within the contents of text files. This can be useful for providing default configurations or templates for code files. Let‚Äôs extend our previous example to include a placeholder inside a text file:\nFirst, modify the my_template/main.py file to include a placeholder inside its contents:\n# main.py\n\ndef hello():\n    print(\"Hello, {{cookiecutter.project_name}}!\")\nNow, the {cookiecutter.project_name} placeholder is inside the main.py file. When you run Cookiecutter, it will automatically replace the placeholders not only in file and directory names but also within the contents of text files. After running Cookiecutter, your generated main.py file might look like this:\n# main.py\n\ndef hello():\n    print(\"Hello, MyProject!\")  # Assuming \"MyProject\" was entered as the project_name\n\n\nStep 3: Use Cookiecutter\nNow that your template is set up, you can use Cookiecutter to generate a project based on it. Open a terminal and run:\ncookiecutter path/to/your/template\nCookiecutter will prompt you to fill in the values for project_name, author_name, and description. After you provide these values, Cookiecutter will replace the placeholders in your template files with the entered values.\n\n\nStep 4: Explore the Generated Project\nOnce the generation process is complete, navigate to the directory where Cookiecutter created the new project. You will see a project structure with the placeholders replaced by the values you provided.\n\n\n\n\n\n\nExercise: Create your own template‚Äù\n\n\n\nUsing cookiecutter, create your own templates for your folders. You do not need to copy exactly our suggestions, adjust your template to your own needs! In order to create your cookiecutter template, your will need to install python, cookiecutter, Git and a GitHub account. If you do not have Git and a GitHub account, we suggest you do one as soon as possible. We will take a deeper look at Git and GitHub in the version control lesson.\nWe have prepared already two simple cookiecutter templates in GitHub repositories.\n**Assay**\n\n1. First, fork our `https://github.com/hds-sandbox/assay-template` from the GitHub page into your own account/organization.\n![fork_repo_example](./images/fork_repo.png)\n2. Then, use `git clone &lt;your URL to the template&gt;` to put it in your computer.\n3. Modify the contents of the repository so that it matches the **Assay** example above. You are welcome to do changes as you please!\n4. Modify the `cookiecutter.json` file so that it will include the **Assay** name template\n5. Git add, commit and push your changes\n6. Test your folder by using `cookiecutter &lt;URL to your GitHub repository for \"assay-template&gt;`\n\n**Project**\n\n7. First, fork our `https://github.com/hds-sandbox/project-template` from the GitHub page into your own account/organization.\n![fork_repo_example](./images/fork_repo_project.png)\n8. Then, use `git clone &lt;your URL to the template&gt;` to put it in your computer.\n9.  Modify the contents of the repository so that it matches the **Project** example above. You are welcome to do changes as you please!\n10. Modify the `cookiecutter.json` file so that it will include the **Project** name template\n11. Git add, commit and push your changes\n12. Test your folder by using `cookiecutter &lt;URL to your GitHub repository for \"project-template&gt;`"
  },
  {
    "objectID": "develop/06_file_structure.html#genomic-resources-folder",
    "href": "develop/06_file_structure.html#genomic-resources-folder",
    "title": "Best Practices for Data Storage",
    "section": "Genomic resources folder",
    "text": "Genomic resources folder\nPreprocessing NGS data usually requires different genomic resources in order to align and and annotate fastq files. First and most importantly, you will need reference genomes (human and mouse are the most common ones) in FASTA format. In addition, aligner tools such as STAR, Bowtie, etc., require indexed fasta files to perform alignment of reads. Moreover, GTF or GFF files are necessary in order to quantify reads into genomic regions (such as genes or promoters). Nonetheless, these genomic resources are often updated and are released periodically under different versions from different sources. In order to make your data reproducible, you will need to control and manage these genomic resources and specify their versions and sources. For example, the latest mouse reference genome is GRCm39, but may studies still align their reads to the version GRCm38.\nHow do you keep track of your resources? You could, again, set up a folder structure that hosts reference genomes, annotations and indexes per species and per version, or use a reference genome manager such as refgenie. We will take a look at both options.\n\nRefGenie\nRefgenie manages storage, access, and transfer of reference genome resources. It provides command-line and Python interfaces to download pre-built reference genome ‚Äúassets‚Äù, like indexes used by bioinformatics tools. It can also build assets for custom genome assemblies. Refgenie provides programmatic access to a standard genome folder structure, so software can swap from one genome to another.\nCheck their tutorial if you want to start managing your genomic resources with Refgenie!\n\n\nManual download\nIf you want to keep manual track and manage yourself and not depend on a third party tool, you can always download the genomic resources directly from the source. We will show you how in this section:\n\nCreate a new folder, called genomic_resources.\nInside this folder, separate each species with a subfolder.\nInside the species subfolder, separate it by the resource version\nInside the version subfolder, separate genomic sequences (FASTA) and annotations (GTF/GFF) from different alignment indexes\n\nIn the example below, we are downloading human (version GRCh38) and mouse (version GRCm39) genomic resources from the ensembl FTP server\ngenomic_resources/\n‚îú‚îÄ‚îÄ homo_sapiens/\n‚îÇ  ‚îî‚îÄ‚îÄ GRCh38/\n‚îÇ     ‚îú‚îÄ‚îÄ Homo_sapiens.GRCh38.109.gtf.gz\n‚îÇ     ‚îú‚îÄ‚îÄ Homo_sapiens.GRCh38.109.dna_sm.primary_assembly.fa.gz\n‚îÇ     ‚îî‚îÄ‚îÄ indexes/\n‚îÇ        ‚îú‚îÄ‚îÄ salmon/\n‚îÇ        ‚îî‚îÄ‚îÄ STAR/\n‚îú‚îÄ‚îÄ mus_musculus/\n‚îÇ  ‚îî‚îÄ‚îÄ GRCm39/\n‚îÇ     ‚îú‚îÄ‚îÄ Mus_musculus.GRCm39.109.gtf.gz\n‚îÇ     ‚îú‚îÄ‚îÄ Mus_musculus.GRCm39.109.dna_sm.primary_assembly.fa.gz\n‚îÇ     ‚îî‚îÄ‚îÄ indexes/\n‚îÇ        ‚îú‚îÄ‚îÄ salmon/\n‚îÇ        ‚îî‚îÄ‚îÄ STAR/\n‚îú‚îÄ‚îÄ ref_genomes.sh\n‚îî‚îÄ‚îÄ create_indexes.sh\nAs you can see, we have also keep here a bash script that downloads and sets up the folders. The contents of the bash script should also indicate a date of when was the data downloaded. For example:\n#!/bin/bash\n\n# Download human (GRCh38) and mouse (GRCm39) genome and annotations\n# Release 109\n# 2023-07-10\n\n# Go to home\ncd\n# Create homo_sapiens version folder and go to that folder\nmkdir -p genomic_resources/homo_sapiens/GRCh38\ncd  genomic_resources/homo_sapiens/GRCh38\n\n# Download FASTA file and GTF file from the ensembl release 109\nwget -L ftp://ftp.ensembl.org/pub/release-109/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz\nwget -L ftp://ftp.ensembl.org/pub/release-109/gtf/homo_sapiens/Homo_sapiens.GRCh38.109.gtf.gz\n\n# Go to home\ncd\n# Create mus_musculus version folder and go to that folder\nmkdir -p genomic_resources/mouse/GRCm39\ncd  genomic_resources/mouse/GRCm39\n\n# Download FASTA file and GTF file from the ensembl release 109\nwget -L ftp://ftp.ensembl.org/pub/release-109/fasta/mus_musculus/dna/Mus_musculus.GRCm39.dna_sm.primary_assembly.fa.gz\nwget -L https://ftp.ensembl.org/pub/release-109/gtf/mus_musculus/Mus_musculus.GRCm39.109.gtf.gz\nAfter that is all set up, you can run another bash script that will create a index folder inside each version subfolder and generate indexes for different alignment tools!"
  },
  {
    "objectID": "develop/06_file_structure.html#naming-conventions",
    "href": "develop/06_file_structure.html#naming-conventions",
    "title": "Best Practices for Data Storage",
    "section": "Naming conventions",
    "text": "Naming conventions\nUsing consistent naming conventions is important in scientific research as it helps with the organization and retrieval of data or results. By adopting standardized naming conventions, researchers ensure that files, experiments, or data sets are labeled in a clear, logical manner. This makes it easier to locate and compare similar types of data or results, even when dealing with large datasets or multiple experiments. For instance, in genomics, employing uniform naming conventions for files related to specific experiments or samples allows for swift identification and comparison of relevant data, streamlining the research process and contributing to the reproducibility of findings. This practice promotes efficiency, collaboration, and the integrity of scientific work.\n\nGeneral tips\nBelow you will find a small list of general tips to follow when you name a folder or a file:\n\nUse only alphanumeric characters to write a word: a to z and 0 to 9\nAvoid special characters: ~!@#$%^&*()`‚Äú|\nDate format: use YYYYMMDD format. For example: 20230101.\nAuthors: use initials. For example: JARH\nDon‚Äôt use of spaces! Computers get very confused when you need to point a path to a file and it contains spaces! Instead:\n\nSeparate field sections are separated by underscores _.\nWords in each section are written in camelCase. It would look then like this: field1_word1Word2.txt. For example: heatmap_sampleCor_20230101.png. The first field indicates what this file is, i.e., a heatmap. Second field is what is being plotted, i.e, sample correlations; since the field contains two words, they are written in camelCase. The third field is the date of when the image was created.\n\nUse as short fields as possible. You can try to use understandable abbreviations, like LFC for LogFoldChange, Cor for correlations, Dist for distances, etc.\nAvoid long names as much as you can, be concise!\nAvoid creating many sublevels of folders.\nWrite down your naming convention pattern and document it in the README file\nWhen using a sequential numbering system, use leading zeros to make sure files sort in sequential order. Using 01 instead of just 1 if your sequence only goes up to 99.\nVersions should be used as the last element, and use at least two digits with a leading 0 (e.g.¬†v01, v02)\n\n\n\nSuggestions for NGS data\nMore info on naming conventions for different types of files and analysis is in development.\n\n\nPrint a table\n\n\n\n\n\n\n\n\n\nname\ndescription\nnaming_convention\nfile format\nexample\n\n\n\n\n.fastq\nraw sequencing reads\nnan\nnan\nsampleID_run_read1.fastq\n\n\n.fastqc\nquality control from fastqc\nnan\nnan\nsampleID_run_read1.fastqc\n\n\n.bam\naligned reads\nnan\nnan\nsampleID_run_read1.bam\n\n\nGTF\nsequence annotation\nnan\nnan\none of https://www.gencodegenes.org/\n\n\nGFF\nsequence annotation\nnan\nnan\none of https://www.gencodegenes.org/\n\n\n.bed\ngenome locations\nnan\nnan\nnan\n\n\n.bigwig\ngenome coverage\nnan\nnan\nnan\n\n\n.fasta\nsequence data (nucleotide/aminoacid)\nnan\nnan\none of https://www.gencodegenes.org/\n\n\nMultiqc report\nQC aggregated report\n&lt;assayID\\&gt;_YYYYMMDD.multiqc\nmultiqc\nRNA_20200101.multiqc\n\n\nCount matrix\nfinal count matrix\n&lt;assayID\\&gt;_cm_aligner_YYYYMMDD.tsv\ntsv\nRNA_cm_salmon_20200101.tsv\n\n\nDEA\ndifferential expression analysis results\nDEA_&lt;condition1-condition2\\&gt;_LFC&lt;absolute_threshold\\&gt;_p&lt;pvalue decimals\\&gt;_YYYYMMDD.tsv\ntsv\nDEA_treat-untreat_LFC1_p01_20200101.tsv\n\n\nDBA\ndifferential binding analysis results\nDBA_&lt;condition1-condition2\\&gt;_LFC&lt;absolute_threshold\\&gt;_p&lt;pvalue decimals\\&gt;_YYYYMMDD.tsv\ntsv\nDBA_treat-untreat_LFC1_p01_20200101.tsv\n\n\nMAplot\nMA plot\nMAplot_&lt;condition1-condition2\\&gt;_YYYYMMDD.jpeg\njpeg\nMAplot_treat-untreat_20200101.jpeg\n\n\nHeatmap plot\nHeatmap plot of anything\nheatmap_&lt;type\\&gt;_YYYYMMDD.jpeg\njpeg\nheatmap_sampleCor_20200101.jpeg\n\n\nVolcano plot\nVolcano plot\nvolcano_&lt;condition1-condition2\\&gt;_YYYYMMDD.jpeg\njpeg\nvolcano_treat-untreat_20200101.jpeg\n\n\nVenn diagram\nVenn diagram\nvenn_&lt;type\\&gt;_YYYYMMDD.jpeg\njpeg\nvenn_consensus_20200101.jpeg\n\n\nEnrichment table\nEnrichment results\nnan\ntsv\nnan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚ÄúExercise: Create your own naming conventions‚Äù\n\n\n\nThink about the most common types of files and folders you will be working on, such as visualizations, results tables, processed files, etc. Then come up with a logical and clear way of naming those files using the tips suggested above. Remember to avoid making long and complicated names!"
  },
  {
    "objectID": "develop/06_file_structure.html#wrap-up",
    "href": "develop/06_file_structure.html#wrap-up",
    "title": "Best Practices for Data Storage",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we have learned some practical tips and examples about how to organize your data and bring some order to chaos! We have shown how to use cookiecutter as a template engine that can be used to create new Assay and Project folders without hassle. Hopefully you are now able to create your own templates and reuse them as much as you need. In the next lesson, we will look at what kind of metadata you should record for your Assay and Project folders, from general fields such as author or date, to more NGS specific like organism, cell type or tissue."
  },
  {
    "objectID": "index.html#research-data-management-overview",
    "href": "index.html#research-data-management-overview",
    "title": "Research Data Management for NGS data",
    "section": "",
    "text": "Research Data Management (RDM) for Next Generation Sequencing (NGS) data is a foundational course aimed at providing participants with fundamental knowledge and practical skills in handling the extensive data generated through modern NGS studies in the context of Open Science and FAIR principles. This course covers essential principles of RDM practices, such as data organization, metadata annotation, version control and archiving, enabling researchers to manage NGS data with confidence. Participants will also gain insights into FAIR principles and Open Science, fostering collaboration and reproducibility in NGS research. By the end of the course, attendees will be equipped with essential tools and techniques to navigate the data challenges prevalent in today‚Äôs NGS research landscape.\n\n\n\n\n\n\n\n\nCourse Overview\n\n\n\n\nüìñ Syllabus:\n\nWhat is Research Data Management and why it is important\nWhat is NGS data\nData Life Cycle\nOpen Science and FAIR principles\nData Management plans\nFolder and file structures applied to NGS data\nMetadata applied to NGS data\nCreate a database of your data and projects\nVersion control of your data analysis\nArchiving and repositories\n\n‚è∞ Total Time Estimation: X hours\nüìÅ Supporting Materials:\nüë®‚Äçüíª Target Audience: PhD, MsC, anyone interested in RDM for NGS data.\nüë©‚Äçüéì Level: Beginner.\nüîí License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\nüí∞ Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).\n\n\n\n\n\n\n\n\n\nCourse Requirements\n\n\n\n\nBasic understanding Next Generation Sequencing data and formats.\nCommand Line experience\nBasic programming experience\nQuarto or Mkdocs tools\n\n\n\nThis course provides participants with an overall introduction to effectively manage the vast amounts of data generated in modern NGS studies. Participants will gain a practical understanding of RDM principles and the significance of handling NGS data efficiently. The course covers the unique characteristics of NGS data, its life cycle, and the importance of adopting Open Science and FAIR principles for data accessibility and reusability.\nThroughout the course, participants will learn useful skills for organizing NGS data, including creating folder and file structures and implementing metadata to enhance data discoverability and interpretation. Data management plans (DMPs) tailored to NGS data will be explored, ensuring data integrity and compliance with institutional and funding agency requirements. Attendees will also gain insights into setting up simple databases and using version control systems to track changes in data analysis, promoting collaboration and reproducibility.\nThe course concludes with a focus on archiving and data repositories, enabling participants to preserve and share NGS data for long-term scientific usage. By the end of the course, attendees will be equipped with the necessary tools and techniques to navigate the challenges prevalent in today‚Äôs NGS research landscape, fostering successful data management practices and enhancing collaboration in the scientific community.\n\n\n\n\n\n\nCourse Goals\n\n\n\nBy the end of this workshop, you should be able to apply the following concepts in the context of Next Generation Sequencing data: - Understand what is RDM and why it is important - Understand FAIR and Open Science Principles - Write a Data Management Plan for your NGS data - Structure and establish naming conventions for your files and folders - Add relevant metadata to your data - Version control your data analysis - Select a repository to archive your data - Make your data analysis and workflows reproducible\n\n\n\n\n\nUniversity of Copenhagen Research Data Management Team.\nMartin Proks and Sarah Lundregan, Brickman Lab, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nRichard Dennis, Data Steward, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nNBISweden.\nRDMkit, Elixir Research Data Management Platform."
  },
  {
    "objectID": "develop/01_RDM_intro.html",
    "href": "develop/01_RDM_intro.html",
    "title": "1. Introduction to RDM",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nFundamentals of Research Data Management\nEffective Research Data Management Guidelines\nData Lifecycle Management and phases\nFAIR principles and Open Science",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "develop/01_RDM_intro.html#what-is-research-data-management",
    "href": "develop/01_RDM_intro.html#what-is-research-data-management",
    "title": "Introduction",
    "section": "What is Research Data Management",
    "text": "What is Research Data Management\nWhile the meaning of Research Data Management might be obvious, it is a good idea to break down its components to make a good sense of what it implies. Let‚Äôs start with Management, which is rather simple!\n\n\n\n\n\n\nSemantics of RDM\n\n\n\n\n\nThe definition of Management is ‚Äúthe practice of managing; handling, supervision, or control.‚Äù.\nIn accordance with the UCPH Policy for Research Data Management, research data encompasses both physical material and digital information gathered, observed, produced, or formulated during research activities carried out during research. This broad definition includes various types of data serving as the foundation for the research, such as specimens, notebooks, interviews, texts, literature, digital raw data, recordings, computer code, and meticulous documentation of these materials and data, forming the core of the analysis that underlies the research outcomes.\n\n\n\nSo, our goal is to handle and control the data that we generate during our research, including both physical and digital data! This must be done throughout the whole life cycle of the data.\nNonetheless, since this is a workshop on bioinformatics data, NGS to be specific, we will focus on the digital part of our data. We will not talk much about wet lab RDM, such as protocols, instruments, reagents, ELM or LIMS systems, although this would ideally be integrated as part of the whole RDM process.\n\n\n\n\n\n\nWarning\n\n\n\nThis workshop focuses on RDM of digital data and how it was generated.\n\n\n\nResearch Data Cycle\nThe Research Data Life Cycle is a conceptual framework that illustrates the various stages that research data goes through during its lifetime, from its initial creation or collection to its eventual archiving or disposal. It provides a structured approach to managing research data effectively, ensuring data integrity, accessibility, and reusability. We will talk about the Research Data Life Cycle in the third lesson.\n\n\n\nResearch Data Life Cycle, University of Copenhagen RDM guidelines"
  },
  {
    "objectID": "develop/01_RDM_intro.html#why-is-research-data-management-important",
    "href": "develop/01_RDM_intro.html#why-is-research-data-management-important",
    "title": "Introduction",
    "section": "Why is Research Data Management important",
    "text": "Why is Research Data Management important\nEffective data management can significantly benefit research, providing advantages for individual researchers:\n\nCareful planning helps in the early identification and resolution of potential issues, aligns expectations among collaborators, and clarifies data rights and ownership.\nClear data documentation streamlines the process of locating and comprehending previous research, promoting efficiency and building upon existing knowledge.\nConducting risk assessments and devising robust data storage and security strategies mitigate the risk of data loss, breaches, or unauthorized use, safeguarding valuable research.\nSharing data with others beyond the project‚Äôs conclusion enhances research visibility and fosters increased citations, expanding the impact of your findings.\nDeveloping a data preservation plan ensures the long-term availability of research data well after the project‚Äôs completion, contributing to data accessibility and continued research relevance.\n\n\nThe cost of poor RDM practices\nSeveral surveys have shown that data scientists spend between 40-60% of their time loading and cleaning data, becoming the most consuming, and what many would call tedious, tasks of their jobs (‚ÄúThe State of Data Science 2020 Moving from Hype Toward Maturity‚Äù 2020; ‚ÄúCleaning Big Data: Most Time-Consuming, Least Enjoyable Data Science Task, Survey Says‚Äù 2016). Below we show the number figures from the Anaconda ‚ÄúState of data science 2020‚Äù report(‚ÄúThe State of Data Science 2020 Moving from Hype Toward Maturity‚Äù 2020).\n\n\n\nTime spent in different tasks by data scientists, Anaconda State of data science report 2020\n\n\nCould you think why we spend so much time? Maybe this pictures look familiar to you‚Ä¶\n\n\n\nPhoto by Wonderlane on Unsplash\n\n\n\n\n\nMessy folder structure, by Alex old-self\n\n\nOr ever felt like this?\n\n\n\nFrom Stanford Center for Reproducible Neuroscience\n\n\nWhat about these situations? Have you ever encounter any?\n\nImagine a researcher working on a project without a clear folder structure or meaningful file names for their data. As the project progresses and data accumulates, it becomes challenging for the researcher to locate specific data files quickly. This leads to wasted time searching for relevant information and delays in data analysis.\nSuppose a researcher does not document their data collection methods adequately. When another researcher attempts to analyze the data later, they struggle to understand the context in which the data was gathered, leading to misinterpretations and errors in the analysis.\nA researcher publishes a groundbreaking study, but their data and methods lack proper documentation and are not made available to others. As a result, other researchers find it challenging or impossible to reproduce the study‚Äôs results, leading to doubts about the validity of the findings.\nSuppose a research team spends weeks trying to clean and validate poorly organized data before they can start their analysis. This wasted time and effort could have been better spent on more productive research tasks.\n\nIneffective data management practices can have significant consequences that affect both your future self and colleagues who may have to deal with your data, as well as those handling other people‚Äôs data. The implications of poor data management include:\n\nDifficulty in Data Retrieval: Without proper organization and documentation, finding specific data files or understanding their content becomes challenging and time-consuming. This leads to inefficiency and frustration when attempting to retrieve relevant information.\nLoss of Data: Inadequate data backup and storage strategies increase the risk of data loss due to hardware failures, accidental deletions, or other unforeseen events. Losing valuable research data can be devastating and may result in the loss of months or even years of work.\nData Incompleteness and Errors: Insufficient data documentation can lead to ambiguity and errors in data interpretation and analysis. This can undermine the credibility and reliability of research outcomes.\nDifficulty in Reproducibility: Inability to reproduce research results due to poor data management hinders scientific progress and challenges the validity of research findings.\nDelayed or Compromised Collaboration: In collaborative research projects, disorganized or poorly managed data can slow down progress and hinder effective communication among team members.\nData Security and Privacy Risks: Inadequate data security measures can result in data breaches, compromising the confidentiality of sensitive information and exposing researchers and subjects to potential risks.\nWasted Time and Resources: Poor data management practices necessitate additional time and effort to clean, validate, and organize data, diverting valuable resources from actual research tasks.\nFinancial Implications: Time-consuming data management tasks translate to increased labor costs and potential project delays. Additionally, data loss may require costly data recovery attempts or, in extreme cases, data reconstruction.\nReputational Damage: Inaccurate or irreproducible research outcomes can damage a researcher‚Äôs reputation and credibility within the scientific community.\n\nBy not practicing proper data management, researchers risk impeding their own progress and the progress of others who rely on their data. Furthermore, dealing with poorly managed data from others can be equally time-consuming and resource-intensive. To address these challenges, researchers could prioritize effective data management practices, including proper data organization, documentation, backup strategies, and adherence to data security and preservation protocols. Investing time and effort into good data management can prevent unnecessary setbacks, ensure data integrity, and ultimately contribute to more robust and reliable scientific research.\n\n\nBenefits of effective RDM practices\nLet‚Äôs see what could we have done to avoid or fix the issues we have previously mentioned:\n\nTo avoid this, the researcher could have implemented a clear and consistent folder structure with descriptive file names. Additionally, using version control systems, such as Git, for code and analysis files can help track changes and facilitate easy retrieval of previous versions of analyses and results.\nProper data documentation, including detailed metadata, could have been maintained throughout the data collection process, providing necessary context and reducing the risk of incomplete or ambiguous data.\nThe researcher could have followed the FAIR principles (Findable, Accessible, Interoperable, Reusable) by making their data, along with detailed methods and documentation, openly accessible in a reputable data repository.\nBy implementing data management strategies from the outset of the research project, researchers can save time and resources later on, ensuring that data is well-organized and properly documented.\n\nAll in all, good data management absolutely have a positive impact on research. For individual researchers:\n\nGood planning can help identify issues before they become a problem, and help align expectations between collaborators, for examples concerning rights to data.\nGood data documentation can facilitate finding and understanding past research.\nConducting risk assessments and making a strategy for data storage and security can prevent data loss, data breaches or data misuse.\nSharing data with others after project end can enhance the visibility of research and lead to an increase in citations.\nDrafting a data preservation plan can help ensure the availability of research data for years after the end of the project.‚Äã"
  },
  {
    "objectID": "develop/01_RDM_intro.html#wrap-up",
    "href": "develop/01_RDM_intro.html#wrap-up",
    "title": "1. Introduction to RDM",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we‚Äôve covered the definition of RDM, the advantages of effective RDM practices the phases of the research data life cycle, and the FAIR principles and Open Science. While much of the guidelines are in the context of omics data, it‚Äôs worth noting its applicability to other fields and institutions. Nonetheless, we recommend exploring these guidelines further at your institution (links provided above).\nIn the next lessons, we will explore different resources, tools, and guidelines that can be applied to all kinds of data and how to apply them specifically to biological (with a focus on NGS) data.\n\nSources\n\nRDMkit: ELIXIR (2021) Research Data Management Kit. A deliverable from the EU-funded ELIXIR-CONVERGE project (grant agreement 871075).",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "index.html#introduction-to-research-data-management",
    "href": "index.html#introduction-to-research-data-management",
    "title": "Research Data Management for NGS data",
    "section": "",
    "text": "Research Data Management (RDM) for Next Generation Sequencing (NGS) data is a foundational course aimed at providing participants with fundamental knowledge and practical skills in handling the extensive data generated through modern NGS studies in the context of Open Science and FAIR principles. This course covers essential principles of RDM practices, such as data organization, metadata annotation, version control and archiving, enabling researchers to manage NGS data with confidence. Participants will also gain insights into FAIR principles and Open Science, fostering collaboration and reproducibility in NGS research. By the end of the course, attendees will be equipped with essential tools and techniques to navigate the data challenges prevalent in today‚Äôs NGS research landscape.\n\n\n\n\n\n\n\n\nCourse Overview\n\n\n\n\nüìñ Syllabus:\n\nWhat is Research Data Management and why it is important\nWhat is NGS data\nData Life Cycle\nOpen Science and FAIR principles\nData Management plans\nFolder and file structures applied to NGS data\nMetadata applied to NGS data\nCreate a database of your data and projects\nVersion control of your data analysis\nArchiving and repositories\n\n‚è∞ Total Time Estimation: X hours\nüìÅ Supporting Materials:\nüë®‚Äçüíª Target Audience: PhD, MsC, anyone interested in RDM for NGS data.\nüë©‚Äçüéì Level: Beginner.\nüîí License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\nüí∞ Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).\n\n\n\n\n\n\n\n\n\nCourse Requirements\n\n\n\n\nBasic understanding Next Generation Sequencing data and formats.\nCommand Line experience\nBasic programming experience\nQuarto or Mkdocs tools\n\n\n\nThis course provides participants with an overall introduction to effectively manage the vast amounts of data generated in modern NGS studies. Participants will gain a practical understanding of RDM principles and the significance of handling NGS data efficiently. The course covers the unique characteristics of NGS data, its life cycle, and the importance of adopting Open Science and FAIR principles for data accessibility and reusability.\nThroughout the course, participants will learn useful skills for organizing NGS data, including creating folder and file structures and implementing metadata to enhance data discoverability and interpretation. Data management plans (DMPs) tailored to NGS data will be explored, ensuring data integrity and compliance with institutional and funding agency requirements. Attendees will also gain insights into setting up simple databases and using version control systems to track changes in data analysis, promoting collaboration and reproducibility.\nThe course concludes with a focus on archiving and data repositories, enabling participants to preserve and share NGS data for long-term scientific usage. By the end of the course, attendees will be equipped with the necessary tools and techniques to navigate the challenges prevalent in today‚Äôs NGS research landscape, fostering successful data management practices and enhancing collaboration in the scientific community.\n\n\n\n\n\n\nCourse Goals\n\n\n\nBy the end of this workshop, you should be able to apply the following concepts in the context of Next Generation Sequencing data: - Understand what is RDM and why it is important - Understand FAIR and Open Science Principles - Write a Data Management Plan for your NGS data - Structure and establish naming conventions for your files and folders - Add relevant metadata to your data - Version control your data analysis - Select a repository to archive your data - Make your data analysis and workflows reproducible\n\n\n\n\n\nUniversity of Copenhagen Research Data Management Team.\nMartin Proks and Sarah Lundregan, Brickman Lab, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nRichard Dennis, Data Steward, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nNBISweden.\nRDMkit, Elixir Research Data Management Platform.",
    "crumbs": [
      "Course material",
      "Research Data Management for NGS data"
    ]
  },
  {
    "objectID": "use_cases.html",
    "href": "use_cases.html",
    "title": "RDM use cases",
    "section": "",
    "text": "Here, you will find practical examples demonstrating the application of omics data.\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "Use cases",
      "RDM use cases"
    ]
  },
  {
    "objectID": "index.html#research-data-management-for-ngs-data",
    "href": "index.html#research-data-management-for-ngs-data",
    "title": "Computational Research Data Management",
    "section": "",
    "text": "The course ‚ÄúResearch Data Management (RDM) for Next Generation Sequencing (NGS) data‚Äù is designed to provide participants with foundational knowledge and practical skills in handling the extensive data generated by modern NGS studies. It emphasizes the importance of Open Science and FAIR principles in managing NGS data effectively. This course covers essential principal and best practices guidelines in data organization, metadata annotation, version control, and data preservation. These principles are explored from a computational perspective, ensuring participants gain hands-on experience in applying them to real-world scenarios in their research labs. Additionally, the course delves into FAIR principles and Open Science, promoting collaboration and reproducibility in research endeavors. By the course‚Äôs conclusion, attendees will possess essential tools and techniques to address the data challenges prevalent in today‚Äôs NGS research landscape.\n\n\n\n\n\n\nCourse Overview\n\n\n\n\nüìñ Syllabus:\n\n\nData Lifecycle Management\nData Management Plans (DMPs)\nData Organization and Documentation\nMetadata standards and data description\nVersion Control and Collaboration\nCode and Pipelines for Data Analysis\nData sharing and data preservation\n\n\n‚è∞ Total Time Estimation: X hours\n\nüìÅ Supporting Materials:\n\nüë®‚Äçüíª Target Audience: PhD, MsC, anyone interested in RDM for NGS data.\nüë©‚Äçüéì Level: Beginner.\nüîí License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\nüí∞ Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).\n\n\n\n\n\n\n\n\n\n\nCourse Requirements\n\n\n\n\nBasic understanding Next Generation Sequencing data and formats.\nCommand Line experience\nBasic programming experience\nQuarto or Mkdocs tools\n\n\n\nThis course offers participants with an in-depth introduction to effectively managing the vast amounts of data generated in modern studies. Throughout the program, emphasis is placed on practical understanding of RDM principles and the importance of efficient handling of large datasets. In this context, participants will learn the necessity of adopting Open Science and FAIR principles for enhancing data accessibility and reusability.\nParticipants will acquire practical skills for organizing data, including the creation of folder and file structures, and the implementation of metadata to facilitate data discoverability and interpretation. Special attention is given to the development of Data Management Plans (DMPs) with examples tailored to omics data, ensuring compliance with institutional and funding agency requirements while maintaining data integrity. Attendees will also gain insights into the establishment of simple databases and the use of version control systems to track changes in data analysis, thereby promoting collaboration and reproducibility.\nThe course concludes with a focus on archiving and data repositories, enabling participants to learn strategies for preserving and sharing data for long-term scientific usage. By the end of the course, attendees will be equipped with essential tools and techniques to effectively navigate the challenges prevalent in today‚Äôs research landscape. This will not only foster successful data management practices but also enhance collaboration within the scientific community.\n\n\n\n\n\n\nCourse Goals\n\n\n\nBy the end of this workshop, you should be able to apply the following concepts in the context of Next Generation Sequencing data:\n\nUnderstand the Importance of Research Data Management (RDM)\nFamiliarize Yourself with FAIR and Open Science Principles\nDraft a Data Management Plan for your own Data\nEstablish File and Folder Naming Conventions\nEnhance Data with Descriptive Metadata\nImplement Version Control for Data Analysis\nSelect an Appropriate Repository for Data Archiving\nMake your data analysis and workflows reproducible and FAIR\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is a computational workshop that focuses primarily on the digital aspect of our data. While wet lab Research Data Management (RDM) involving protocols, instruments, reagents, ELM or LIMS systems is integral to the entire RDM process, it won‚Äôt be covered in this course.\nAs part of effective data management, it‚Äôs crucial to prioritize strategies that ensure security and privacy. While these aspects are important, please note that they won‚Äôt be covered in our course. However, we highly recommend enrolling in the GDPR course offered by Center for Health Data Science, specially if you‚Äôre working with sensitive data. This course specifically focuses on GDPR compliance and will provide you with valuable insights and skills in managing data privacy and security.\n\n\n\n\n\nUniversity of Copenhagen\nUniversity Library of Southern Denmark\nTechnical University of Denmark\nAalborg University\nAarhus University\n\n\n\n\n\nRDMkit, ELIXIR (2021) Research Data Management Kit. A deliverable from the EU-funded ELIXIR-CONVERGE project (grant agreement 871075).\nUniversity of Copenhagen Research Data Management Team.\nMartin Proks and Sarah Lundregan, Brickman Lab, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nRichard Dennis, Data Steward, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nNBISweden."
  },
  {
    "objectID": "develop/02_NGS_data.html",
    "href": "develop/02_NGS_data.html",
    "title": "Next Generation Sequencing Data",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nNext Generation Sequencing Concepts\nNext Generation Sequencing data types\nRDM challenges for NGS data\n\n\n\nNext Generation Sequencing (NGS) technology yields extensive data volumes. We‚Äôll explore the primary data types generated pre- and post-processing, and delve into RDM challenges unique to NGS (including managing genomic resources, workflows, analysis, preservation, and documentation). Additionally, we‚Äôll touch on concerns regarding sensitive data and GDPR compliance in safeguarding NGS data.\n\n\n\n\n\n\nNext Generation Sequencing\n\n\n\n\n\n\n\nNext Generation Sequencing (NGS), or high-throughput sequencing, has revolutionized genomics research. It encompasses advanced techniques for rapid and cost-effective analysis of DNA or RNA molecules. Unlike traditional methods, NGS can analyze millions of DNA fragments simultaneously, enhancing the speed, efficiency, and scale of sequencing and becoming integral to modern genomics and biomedical studies.As NGS technologies continue to advance and become more accessible, they will remain at the front of cutting-edge genomics research, driving innovations that contribute to our understanding of complex genetic interactions and their implications for human health and biology.\nApplications\nIt is widely utilized in various applications, including genomic sequencing, transcriptome analysis (RNA-Seq), epigenetic profiling (ChIP-Seq), metagenomics, and targeted sequencing. In addition, it plays a crucial role in fields such as oncology, infectious disease research, and personalized medicine.\nData production\nNGS workflows involve key steps, from sample preparation to data analysis. Samples undergo extraction and fragmentation, followed by the addition of unique identifiers, known as library preparation, for multiplexed sequencing. Then,fragments are amplified and sequences in parallel sequencing using state-of-the-art NGS platforms. Subsequent data analysis processes reconstruct the original sequence and identify genetic variations, structural changes, or functional elements. The unique identifiers are specific adapter sequences that allow future identification of individual samples within a multiplexed sequencing run.\n\n\n\n\n\n\n\n\n\nThoroughly document your datasets and the experimental setup to ensure reproducibility. Adhering to standards will ensure interoperability. Data types‚Äô examples:\n\nElectronic Laboratory Notebook (ELN): digital description of the experimental design, and measurement devices. ELNs offer features like data entry, text editing, file attachments, collaboration tools, and search capabilities.\nLaboratory protocols: methodologies to prepare and manage samples.\nSamples: refers to the biological material (extraction of DNA, RNA, or proteins). Specification of sample identifier, sample type, source organism etc.\nSequencing: details on the platform (e.g., Illumina, Oxford Nanopore), library preparation method, coverage, quality control metrics (e.g., Phred score)‚Ä¶\nRaw sequencing data: sequences and quality scores (e.g., FASTQ files)\n\n\n\n\n\n\n\nNote\n\n\n\nA metadata file is crucial during data analysis as it contains information about the experimental conditions (such as sequencing details, treatment, sample type, time points, tissue‚Ä¶).\n\n\n\n\n\nExamples of data types generated during processing:\n\nQuality control metrics: to filter out potential artifacts and ensure the reliability of downstream analyses (e.g., bioinformatics tool like FastQC or MultiQC for results‚Äô aggregation)\nData alignments: in genomics to determine the location of the read in the genome and in transcriptomics to identify gene expression levels.\nDNA analyses results: such as variant calling, genome annotation, functional genomics, phylogenetics, metagenomics etc. Results are usually presented in tabular format.\nRNA Expression Analyses results: from differential gene expression, gene ontology (GO) enrichment, alternative splicing, pathway analysis etc. Results are usually presented in tabular format.\nEpigenetic profiling outputs: to assess gene regulation and chromatin structure (e.g., ChIP-Seq). Usually presented in BED format.\n\nThe interpretation of NGS data relies heavily on the results of data analysis, which are pivotal for understanding the biological significance of the findings and formulating hypotheses for further exploration. Clear and effective visualization methods are crucial for communicating and interpreting the vast amount of information generated by NGS experiments.\n\n\n\n\n\n\nOther types of data: databases and visualizations\n\n\n\n\n\n\n\n\nKnowledge databases\nA knowledge database is a structured repository of biological information that categorizes and annotates genes, proteins, and their functions, facilitating comprehensive understanding and analysis of biological systems. Here are five examples of knowledge databases:\n\n\nGene Ontology (GO): A comprehensive resource that classifies gene functions into defined terms, allowing for standardized annotation and comparison of genes across different organisms.\nDisease Ontology: A database that provides structured, standardized terminology for various diseases and their relationships, aiding in the systematic analysis of disease-related data.\nKEGG Pathways: A collection of manually curated pathway maps representing molecular interactions and reaction networks within cells, enabling the interpretation of high-throughput data in the context of biological systems.\nReactome: An open-access database that offers curated descriptions of biological processes, including pathways, reactions, and molecular events, facilitating the interpretation of large-scale biological data.\nUniProt: An extensive protein knowledgebase that provides detailed information about proteins, including their sequences, functions, and related annotations, supporting a wide range of biological research endeavors.\n\n\nVisualizations\n\n\nHeatmaps: frequently used to visualize gene expression patterns, epigenetic modifications, or microbial abundances across samples/conditions.\nVolcano Plots: commonly used in differential gene expression analysis\nGenome Browser Snapshots: display alignments and genomic features in genomic regions (e.g., gene annotations, ChIP-Seq peaks)\nNetwork Visualizations:utilized to explore gene regulatory networks or protein-protein interaction\nGenomic Annotations: to annotate genetic variations (functional impact on genes, genomic regions, or regulatory element)\n\n\n\n\n\n\n\n\n\nOur summarized guidelines (don‚Äôt forget to read about FAIR software):\n\nCommenting your code: to enhance readability and comprehension\nMake your source code accessible using repository (GitHub, GitLab, Bitbucket, SourceForge, etc.) that provides version control (VC) solutions. This step is one of the most important ones as version control systems (Git or SVN) track changes in your code over time and enable collaboration and easy version management. Most danish institutions provide courses on Git/GitHub, check yours! We also highly recommend reading this paper (Perez-Riverol et al. 2016).\nREADME file: with the comprehensive information about the project and include installation instructions, usage examples or tutorials, licensing details, citation information, etc.\nRegister your code in a research software registry and include a clear and accessible software usage license: enabling other researcher to discover and reuse software packages (alongside a metadata). More recommendations here.\nUse domain-relevant community standards to ensure consistency and interoperability (e.g., CodeMeta).\n\n\n\n\n\n\n\nGit and Github courses and other resources\n\n\n\n\n\n\n\n\nUniversity of Copenhagen\nAarhus University\nAalborg University\nDTU Git guidelines Find more resources in Berkeley Library website\n\n\n\n\n\n\n\n\n\nYou might use standard workflows or generate new ones during data processing and data analyses steps.\n\nCode notebooks: tools for data documentation (e.g.¬†Jupyter Notebook, Rmarkdown) enabling the combination of code with descriptive text and visualizations.\n\nIntegrated development environments (knitr or MLflow).\nPipeline frameworks or workflow management systems: designed to streamline and automate various steps involved in data analysis (data extraction, transformation, validation, visualization, and more). Additionally, they contribute to ensuring interoperability by facilitating seamless integration and interaction between different components or stages. There are two very popular systems, Nextflow and Snakemake.\n\nA great example of community curated workflows is the nf-core community. Nf-core is a collaborative and open-source initiative comprising bioinformaticians and researchers dedicated to developing and maintaining a collection of curated and reproducible Nextflow-based pipelines for NGS data analysis, ensuring standardized and efficient data processing workflows.\n\n\n\n\n\n\nCourse on pipelines and workflows\n\n\n\nTake our course on Reproducible Research Practices LINK\n\n\n\n\nData types summary\n\n\nSelect appropriate file formats that balance data accessibility, storage efficiency, and compatibility with downstream analysis tools. Standardized file formats facilitates data sharing and collaboration among researchers in the scientific community.\n\nBAM/SAM: stores the alignment information (binary and text-based respectively)\nFASTA: store nucleotide or amino acid sequence, commonly used for reference sequences or assembled contigs.\nGene Transfer Format (GTF) and General Feature Format (GFF): annotates genomic features such as genes, exons, and transcripts.\nAlignment indexes: data structures for efficient and rapid mapping of sequencing reads to a reference.\nVariant Call Format (VCF): stores genetic variation such as single nucleotide variants (SNVs), insertions, deletions, and structural variants (and their position, quality score etc.)\nCount matrix: quantifies the abundance of RNA transcripts or genomic features across samples\nBED/BEDGraph: represent genomic intervals or coverage information (e.g., peak calling identifies regions of enriched signal intensity)\nWIG/BigWig: store genome-wide data\n\nGeneral formats\n\nTabular formats: File formats like CSV, TSV, and XLSX used to store data in rows and columns for easy data analysis and sharing\nImage formats: File formats such as PNG and SVG used to store graphical visualizations, making them easily viewable and shareable\nBinary formats: File formats like NPZ and H5 used to store large datasets, ensuring efficient data access and storage\nJSON: A lightweight data-interchange format for storing hierarchical data structures, commonly used in bioinformatics tools\nHTML: A format used to create interactive reports that include both visualizations and textual descriptions of analysis results\nCode notebooks: Interactive documents combining code, visualizations, and explanatory text, aiding in data analysis reproducibility and documentation\nScripts: Text files containing sets of commands or code instructions for automating data processing and analysis tasks\n\nExplore more data types at the UCSC webpage. Check out this tutorial for more detailed explanations.\n\n\n\n\n\n\nIn this lesson we have taken a look a the vast and diverse landscape of bioinformatics data. This should give you a start in understanding the type of data we will be focusing on for the next lessons."
  },
  {
    "objectID": "develop/02_NGS_data.html#what-is-next-generation-sequencing",
    "href": "develop/02_NGS_data.html#what-is-next-generation-sequencing",
    "title": "Next Generation Sequencing Data",
    "section": "",
    "text": "Next Generation Sequencing (NGS), also known as high-throughput sequencing, is a revolutionary technology that has transformed genomics research in recent years. NGS is a suite of advanced DNA sequencing techniques that enable rapid and cost-effective analysis of DNA or RNA molecules. Unlike traditional Sanger sequencing, which could only sequence a limited number of DNA fragments at a time, NGS can analyze millions of DNA fragments simultaneously in a single run. This massive parallel sequencing capacity has drastically increased the speed, efficiency, and scale of DNA sequencing, making it an indispensable tool in modern genomics and biomedical studies.\nNGS workflows encompass key steps, starting with sample preparation, where DNA or RNA is extracted and fragmented into smaller segments. Subsequently, unique identifiers are added to the fragments during library preparation, enabling multiplexed sequencing. The fragments are then amplified and sequenced in parallel using state-of-the-art NGS platforms. Finally, data analysis methods are applied to process the raw sequencing data, reconstruct the original DNA or RNA sequence, and identify genetic variations, structural changes, or functional elements.\nThe versatility of NGS extends far beyond genome sequencing. It is widely utilized in various applications, including transcriptome analysis (RNA-Seq), epigenetic profiling (ChIP-Seq), metagenomics, and targeted sequencing. NGS has revolutionized fields like oncology, infectious disease research, and personalized medicine, as its ability to generate vast amounts of data rapidly provides unprecedented insights into the genetic basis of diseases and biological processes. As NGS technologies continue to advance and become more accessible, they will remain at the front of cutting-edge genomics research, driving innovations that contribute to our understanding of complex genetic interactions and their implications for human health and biology."
  },
  {
    "objectID": "develop/02_NGS_data.html#data-produced-during-an-ngs-experiment",
    "href": "develop/02_NGS_data.html#data-produced-during-an-ngs-experiment",
    "title": "Next Generation Sequencing Data",
    "section": "",
    "text": "During an NGS experiment, various types of data are generated, including everything needed to create and prepare the samples used for sequencing. These compass, between others, laboratory protocols, lab notebooks, NGS libraries and the sequencing data itself.\n\n\nOnce the libraries are prepared, they undergo the sequencing process using NGS platforms such as Illumina, Oxford Nanopore Technologies (ONT), or Pacific Biosciences (PacBio). The choice of platform depends on factors such as read length, error rate, throughput, and cost. The sequencing generates millions to billions of short DNA or RNA fragments, known as reads.\nThe sequencing process produces raw data in the form of short reads, each representing a small segment of the genome or transcriptome. These reads are typically stored in FASTQ files, which contain nucleotide sequences and corresponding quality scores for each read.\n\n\n\n\n\n\nBioinformatics data from sequencing\n\n\n\n**FASTQ:** This is one of the primary file formats used to store raw NGS data. FASTQ files contain nucleotide sequences and their corresponding quality scores. Each read from the sequencing process is represented as a set of four lines: a read identifier, the sequence of nucleotides, a separator (usually a '+'), and the quality scores for each base in the sequence."
  },
  {
    "objectID": "develop/02_NGS_data.html#data-produced-during-preprocessing",
    "href": "develop/02_NGS_data.html#data-produced-during-preprocessing",
    "title": "Next Generation Sequencing Data",
    "section": "",
    "text": "Before proceeding with data analysis, quality control is performed to assess the accuracy and reliability of the raw sequencing data. Low-quality reads and potential sequencing artifacts are filtered out to ensure the reliability of downstream analyses. Most useful tools to track Quality Control are FastQC and MultiQC.\n:::{.callout-note title=‚ÄúBioinformatics data from QC‚Äù\n- *FastQC**: FastQC is a widely-used bioinformatics tool that provides a comprehensive assessment of the quality of sequencing data, generating valuable insights into potential issues or biases in the data. \n- **MultiQC**: MultiQC is a companion tool that simplifies the process of aggregating and visualizing quality control metrics from multiple samples or datasets, allowing for a quick overview of data quality across an entire project.\n:::\n\n\n\nIn DNA sequencing, the reads are aligned to a reference genome using alignment algorithms to determine the specific location of each read in the genome. For RNA-Seq experiments, reads are aligned to a reference transcriptome to identify gene expression levels.\n\n\n\n\n\n\nBioinformatics data from alignments\n\n\n\n-  **BAM/SAM**: These are binary and text versions, respectively, of Sequence Alignment/Map (SAM) files. SAM files store the alignment information of sequencing reads to a reference genome or transcriptome. BAM files are compressed and more space-efficient, making them the preferred format for storing aligned reads.\n- **FASTA**: FASTA files store nucleotide or amino acid sequences, and they are often used for reference sequences or assembled contigs. Each sequence in a FASTA file begins with a single-line description, followed by the actual sequence data.\n- **GTF/GFF**: These files are used to annotate genomic features, such as genes, exons, and transcripts. Gene Transfer Format (GTF) and General Feature Format (GFF) files include information on feature positions, names, and additional attributes.\n- **Alignment indexes**: These are reference data structures that facilitate efficient and rapid mapping of sequencing reads to a reference genome or transcriptome during Next Generation Sequencing (NGS) data analysis.\n\n\n\n\n\nFor genomic DNA sequencing, variant calling identifies differences (mutations, insertions, deletions) between the sequenced sample and the reference genome. This step is crucial for detecting genetic variations associated with diseases or phenotypic traits.\n!!! note ‚ÄúBioinformatics data from VC‚Äù\n**VCF**: Variant Call Format (VCF) files store genetic variations, such as single nucleotide variants (SNVs), insertions, deletions, and structural variants, identified during variant calling. VCF files include variant position, alleles, genotype information, and quality scores.\n\n\n\nIn RNA-Seq experiments, data analysis includes quantification of gene expression levels, detection of alternative splicing events, and identification of differentially expressed genes under different experimental conditions.\n:::{.callout-note title=‚ÄúBioinformatics data from Expression Analyses‚Äù\nCount matrix: represents the quantified number of times each gene or genomic feature is observed in a set of biological samples. Each row typically corresponds to a gene, while each column represents a sample. ::: ### Epigenetic Profiling\nEpigenetic analyses, such as ChIP-Seq, assess DNA modifications and protein-DNA interactions, providing insights into gene regulation and chromatin structure.\n\n\n\n\n\n\nBioinformatics data from Epigenetic profiling\n\n\n\n\nPeak Calling Results: For ChIP-Seq or other genomic profiling experiments, peak calling identifies regions with enriched signal intensity. The results are typically presented in BED or BEDGraph formats, indicating the genomic positions and signal intensities of detected peaks, or BigWig files for visualization in a genome browser.\nBED/BEDGraph: These files are used to represent genomic intervals, such as gene coordinates, regions of interest, or coverage information. BED files define genomic regions with start and end coordinates, while BEDGraph files represent continuous data (e.g., coverage) as a graph.\nWIG/BigWig: These files store genome-wide data, such as coverage, signal intensity, or ChIP-Seq peaks. Wiggle (WIG) files contain continuous data, while BigWig files are binary, compressed versions that enable efficient random access to large datasets.\n\n\n\n\n\n\nFor metagenomics, the NGS data is used to characterize microbial communities in environmental samples, including the identification of species and functional genes. Overall, the data generated during an NGS experiment is extensive and diverse, providing a wealth of information that is crucial for a wide range of biological and medical research applications. The interpretation and analysis of this data require sophisticated bioinformatics tools and pipelines, as well as domain-specific knowledge to derive meaningful biological insights.\n\n\n\nThere are many more data formats we have not explored here. You are welcome to check more bioinformatics data formats at the UCSC webpage.\n\n\n\nbioinfo_data_format\n\n\n!!! tip ‚ÄúBioinformatics data format tutorial‚Äù\nCheck out [this tutorial](https://bioinformatics.uconn.edu/resources-and-events/tutorials-2/file-formats-tutorial/) for a more in-depth explanation of different bioinformatics data formats"
  },
  {
    "objectID": "develop/02_NGS_data.html#data-produced-during-data-analysis",
    "href": "develop/02_NGS_data.html#data-produced-during-data-analysis",
    "title": "Next Generation Sequencing Data",
    "section": "",
    "text": "Results from NGS data analysis often involve a variety of visualizations and tabular outputs that provide valuable insights into the biological significance of the data. Here are some common types of results that researchers may obtain from NGS data analysis.\n\n\nAfter variant calling, the identified genetic variations are annotated with functional information, such as their impact on genes, regulatory elements, or protein products.\n\n\n\nDifferential Expression Analysis Tables: Differential expression analysis compares gene expression levels between different experimental conditions or groups. The results are typically presented in tabular format, listing genes with significant changes in expression, along with log-fold changes and statistical significance values.\n\n\n\nDifferential Binding Analysis Tables: Differential expression analysis compares chromatin accessibility or binding levels between different experimental conditions or groups. The results are typically presented in tabular format, listing genes with significant changes in binging or chromatin accesibility, along with log-fold changes and statistical significance values.\n\n\n\nFunctional analysis identifies functional categories, pathways, or gene sets that are significantly overrepresented in a list of differentially expressed genes. The results are presented as tables with enrichment scores and adjusted p-values, indicating the biological relevance of gene sets.\n\n\n\nMetagenomic Taxonomic Abundance Tables: In metagenomics, taxonomic abundance tables show the relative abundance of different microbial taxa across multiple samples. They are essential for understanding the composition of microbial communities.\nMetagenomic Functional Analysis Results: Metagenomic functional analysis identifies the functional capabilities of microbial communities. Results are typically presented as tables showing the relative abundance of functional genes or pathways."
  },
  {
    "objectID": "develop/02_NGS_data.html#other-types-of-data-1",
    "href": "develop/02_NGS_data.html#other-types-of-data-1",
    "title": "Next Generation Sequencing Data",
    "section": "",
    "text": "These results from NGS data analysis play a crucial role in interpreting the biological significance of the data, enabling researchers to draw meaningful conclusions and generate hypotheses for further investigation. Effective data visualization and clear tabular outputs are essential for effectively communicating and interpreting the wealth of information generated by NGS experiments.\n:::{.callout-note title=‚ÄúBioinformatics visualizations‚Äù - Heatmaps: Heatmaps are graphical representations of data matrices, where each cell‚Äôs color intensity reflects the value of a specific parameter. In NGS data analysis, heatmaps are frequently used to visualize gene expression patterns, epigenetic modifications, or microbial abundances across different samples or conditions. - Volcano Plots: Volcano plots display the relationship between fold change (usually log-fold change) and statistical significance (e.g., p-value) for each gene in a differential analysis. Genes with high significance and large fold changes are often represented further away from the plot‚Äôs center. - Genome Browser Snapshots: Genome browser snapshots display aligned sequencing reads and various genomic features (e.g., gene annotations, ChIP-Seq peaks) in a genomic region of interest. These snapshots provide a visual representation of NGS data aligned to a reference genome. - Network Visualizations: Network visualizations depict the interactions between genes, proteins, or other biological entities, providing insights into complex biological networks. These visualizations can help uncover regulatory relationships or functional modules. - Genomic Annotations: Results may include annotations for genetic variations, such as their functional impact on genes, genomic regions, or regulatory elements."
  },
  {
    "objectID": "develop/02_NGS_data.html#other-resources-used",
    "href": "develop/02_NGS_data.html#other-resources-used",
    "title": "Next Generation Sequencing Data",
    "section": "",
    "text": "A knowledge database is a structured repository of biological information that categorizes and annotates genes, proteins, and their functions, facilitating comprehensive understanding and analysis of biological systems. Here are five examples of knowledge databases:\n\nGene Ontology (GO): A comprehensive resource that classifies gene functions into defined terms, allowing for standardized annotation and comparison of genes across different organisms.\nDisease Ontology: A database that provides structured, standardized terminology for various diseases and their relationships, aiding in the systematic analysis of disease-related data.\nKEGG Pathways: A collection of manually curated pathway maps representing molecular interactions and reaction networks within cells, enabling the interpretation of high-throughput data in the context of biological systems.\nReactome: An open-access database that offers curated descriptions of biological processes, including pathways, reactions, and molecular events, facilitating the interpretation of large-scale biological data.\nUniProt: An extensive protein knowledgebase that provides detailed information about proteins, including their sequences, functions, and related annotations, supporting a wide range of biological research endeavors.\n\n\n\n\nCode notebooks and scripts are essential tools used to analyze Next Generation Sequencing (NGS) data, providing researchers with a documented and reproducible workflow for data processing, and analyses. Common programming languages (and notebooks) are: Python (Jupyter Notebooks), R (Rmarkdown), perl and bash.\n\n\n\nThese are structured sets of interconnected data analysis steps and bioinformatics tools that streamline and automate the transformation of raw sequencing reads into biologically meaningful information, ensuring reproducibility and efficiency in NGS data analysis. Some examples of workflow and pipeline languages are:\n\nNextflow: Nextflow is a popular workflow management system that enables scalable and portable NGS data analysis pipelines, allowing researchers to process data across various computing environments and platforms.\nSnakemake: Snakemake is a workflow management system that uses Python-based scripting to create flexible and automated NGS data analysis pipelines, facilitating parallel processing and easy integration with existing tools.\n\nA great example of community curated workflows is the nf-core community. Nf-core is a collaborative and open-source initiative comprising bioinformaticians and researchers dedicated to developing and maintaining a collection of curated and reproducible Nextflow-based pipelines for NGS data analysis, ensuring standardized and efficient data processing workflows."
  },
  {
    "objectID": "develop/02_NGS_data.html#data-formats-summary",
    "href": "develop/02_NGS_data.html#data-formats-summary",
    "title": "Next Generation Sequencing Data",
    "section": "",
    "text": "Data types\n\n\n\n\n\nSelect appropriate file formats that balance data accessibility, storage efficiency, and compatibility with downstream analysis tools. Standardized file formats facilitates data sharing and collaboration among researchers in the scientific community.\n\nBAM/SAM: stores the alignment information (binary and text-based respectively)\nFASTA: store nucleotide or amino acid sequence, commonly used for reference sequences or assembled contigs.\nGene Transfer Format (GTF) and General Feature Format (GFF): annotates genomic features such as genes, exons, and transcripts.\nAlignment indexes: data structures for efficient and rapid mapping of sequencing reads to a reference.\nVariant Call Format (VCF): stores genetic variation such as single nucleotide variants (SNVs), insertions, deletions, and structural variants (and their position, quality score etc.)\nCount matrix: quantifies the abundance of RNA transcripts or genomic features across samples\nBED/BEDGraph: represent genomic intervals or coverage information (e.g., peak calling identifies regions of enriched signal intensity)\nWIG/BigWig: store genome-wide data\n\nExplore more data types at the UCSC webpage. Check out this tutorial for more detailed explanations.\nGeneral files formats:\n\nTabular formats: File formats like CSV, TSV, and XLSX used to store data in rows and columns for easy data analysis and sharing.\nImage formats: File formats such as PNG and SVG used to store graphical visualizations, making them easily viewable and shareable.\nBinary formats: File formats like NPZ and H5 used to store large datasets, ensuring efficient data access and storage.\nJSON: A lightweight data-interchange format for storing hierarchical data structures, commonly used in bioinformatics tools.\nHTML: A format used to create interactive reports that include both visualizations and textual descriptions of analysis results.\nCode notebooks: Interactive documents combining code, visualizations, and explanatory text, aiding in data analysis reproducibility and documentation.\nScripts: Text files containing sets of commands or code instructions for automating data processing and analysis tasks."
  },
  {
    "objectID": "develop/02_NGS_data.html#wrap-up",
    "href": "develop/02_NGS_data.html#wrap-up",
    "title": "Next Generation Sequencing Data",
    "section": "",
    "text": "In this lesson we have taken a look a the vast and diverse landscape of bioinformatics data. This should give you a start in understanding the type of data we will be focusing on for the next lessons."
  },
  {
    "objectID": "develop/examples/transcriptomics.html",
    "href": "develop/examples/transcriptomics.html",
    "title": "FAIR Transcriptomics example",
    "section": "",
    "text": "Applied RDM to Transcriptomics\nData Lifecycle\n\n3. Process and analyse\nIn the data life cycle for NGS data, is a critical phase that involves transforming raw sequencing data into meaningful biological insights. During this stage, researchers apply computational methods and bioinformatics tools to extract valuable information from the vast amount of sequencing data generated in NGS experiments.\n\n\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "Use cases",
      "FAIR Transcriptomics example"
    ]
  },
  {
    "objectID": "develop/practical_workshop.html#aims",
    "href": "develop/practical_workshop.html#aims",
    "title": "DTU workshop 2023",
    "section": "",
    "text": "This project aims to investigate gene expression patterns across various human tissues using Next Generation Sequencing (NGS) data. By analyzing the transcriptomes of different tissues, we seek to uncover tissue-specific gene expression profiles and identify potential markers associated with specific biological functions or diseases."
  },
  {
    "objectID": "develop/practical_workshop.html#why-its-important",
    "href": "develop/practical_workshop.html#why-its-important",
    "title": "DTU workshop 2023",
    "section": "",
    "text": "Understanding tissue-specific gene expression is crucial for deciphering the molecular basis of health and disease. Identifying genes that are uniquely expressed in certain tissues can provide insights into tissue function, development, and potential therapeutic targets. This project contributes to our broader understanding of human biology and has implications for personalized medicine and disease research."
  },
  {
    "objectID": "develop/practical_workshop.html#datasets",
    "href": "develop/practical_workshop.html#datasets",
    "title": "Practical material",
    "section": "Datasets",
    "text": "Datasets\nDescribe the data,, including its sources, format, and how to access it. If the data has undergone preprocessing, provide a description of the processes applied or the pipeline used.\n\n\n\n\n\n\nExample text\n\n\n\n\n\n\n\nWe have used internal datasets with IDs: RNA_humanSkin_20201030, RNA_humanBrain_20210102, RNA_humanLung_20220304.\nIn addition, we utilized publicly available NGS datasets from the GTEx (Genotype-Tissue Expression) project, which provides comprehensive RNA-seq data across multiple human tissues. These datasets offer a wealth of information on gene expression levels and isoform variations across diverse tissues, making them ideal for our analysis."
  },
  {
    "objectID": "develop/practical_workshop.html#summary-of-results",
    "href": "develop/practical_workshop.html#summary-of-results",
    "title": "DTU workshop 2023",
    "section": "",
    "text": "Our analysis revealed distinct gene expression patterns among different human tissues. We identified tissue-specific genes enriched in brain tissues, highlighting their potential roles in neurodevelopment and function. Additionally, we found a set of genes that exhibit consistent expression across a range of tissues, suggesting their fundamental importance in basic cellular processes.\nFurthermore, our differential expression analysis unveiled significant changes in gene expression between healthy and diseased tissues, shedding light on potential molecular factors underlying various diseases. Overall, this project underscores the power of NGS data in unraveling intricate gene expression networks and their implications for human health.\n\nFor more details, refer to our Jupyter Notebook for the complete analysis pipeline and code. ```\n\n\nThe metadata file is a yml file, which is a text document that contains data formatted using a human-readable data format for data serialization.\n\n\n\nyaml file example\n\n\n\n\n\nThere is a ton of information you can collect regarding an NGS assay or a project. Some information fields are very general, such as author or date, while others are specific to the Assay or Project folder. Below, we will take a look at minimal information you should collect in each of the folders.\n\n\nHere you can find a list of suggestions for general metadata fields that can be used for both assays and project folders:\n\nTitle: A brief yet informative name for the dataset.\nAuthor(s): The individual(s) or organization responsible for creating the dataset. You can use your ORCID\nDate Created: The date when the dataset was originally generated or compiled. Use YYYY-MM-DD format!\nDescription: A short narrative explaining the content, purpose, and context.\nKeywords: A set of descriptive terms or phrases that capture the folder‚Äôs main topics and attributes.\nVersion: The version number or identifier for the folder, useful for tracking changes.\nLicense: The type of license or terms of use associated with the dataset/project.\n\n\n\n\nHere you will find a table with possible metadata fields that you can use to annotate and track your Assay folders:\n{{ read_table(‚Äò./assets/assay_metadata.tsv‚Äô) }}\n\n\n\nHere you will find a table with possible metadata fields that you can use to annotate and track your Project folders:\n{{ read_table(‚Äò./assets/project_metadata.tsv‚Äô) }}\n\n\n\n\nThe information provided in this lesson is not at all exhaustive. There might be many more fields and controlled vocabularies that could be useful for your NGS data. We recommend that you take a look at the following sources for more information!\n\nTranscriptomics metadata standards and fields\nBionty: Biological ontologies for data scientists.\n\n!!! question ‚ÄúExercise 2: modify the metadata.yml files in your cookiecutter templates‚Äù\nWe have seen some examples of metadata for NGS data. It is time now to customize your cookiecutter templates and modify the metadata.yml files so that they fit your needs! \n\n1. Think about what kind of metadata you would like to include.\n2. Modify the `cookiecutter.json` file so that when you create a new folder template, all the metadata is filled accordingly.\n![cookiecutter_json_example](./images/cookiecutter_json.png)\n3. Modify the `metadata.yml` file so that it includes the metadata recorded by the `cookiecutter.json` file.\n![assay_metadata_example](./images/assay_metadata.png)\n4. Modify the `README.md` file so that it includes the short description recorded by the `cookiecutter.json` file.\n5. Git add, commit and push the changes of your template.\n6. Test your folders by using the command `cookiecutter &lt;URL to your cookiecutter repository in GitHub&gt;`"
  },
  {
    "objectID": "develop/practical_workshop.html#question",
    "href": "develop/practical_workshop.html#question",
    "title": "DTU workshop 2023",
    "section": "",
    "text": "Using cookiecutter, create your own templates for your folders. You do not need to copy exactly our suggestions, adjust your template to your own needs!\nWe have prepared already two simple cookiecutter templates in GitHub repositories.\nAssay\n\nFirst, fork our Assay folder template from the GitHub page into your own account/organization. \nThen, use git clone &lt;your URL to the template&gt; to put it in your computer.\nModify the contents of the repository so that it matches the Assay example above. You are welcome to do changes as you please!\nModify the cookiecutter.json file so that it will include the Assay name template\nGit add, commit and push your changes\nTest your folder by using cookiecutter &lt;URL to your GitHub repository for \"assay-template&gt;\n\nProject\n\nFirst, fork our Project folder template from the GitHub page into your own account/organization. \nThen, use git clone &lt;your URL to the template&gt; to put it in your computer.\nModify the contents of the repository so that it matches the Project example above. You are welcome to do changes as you please!\nModify the cookiecutter.json file so that it will include the Project name template\nGit add, commit and push your changes\nTest your folder by using cookiecutter &lt;URL to your GitHub repository for \"project-template&gt;"
  },
  {
    "objectID": "develop/practical_workshop.html#metadata",
    "href": "develop/practical_workshop.html#metadata",
    "title": "Practical material",
    "section": "2. Metadata",
    "text": "2. Metadata\nMetadata is the behind-the-scenes information that makes sense of data and gives context and structure. For biodata, metadata includes information such as when and where the data was collected, what it represents, and how it was processed. Let‚Äôs check what kind of relevant metadata is available for NGS data and how to capture it in your Assay or Project folders. Both of these folders contain a metadata.yml file and a README.md file. In this section, we will check what kind of information you should collect in each of these files.\n\n\n\n\n\n\nMetadata and controlled vocabularies\n\n\n\nIn order for metadata to be most useful, you should try to use controlled vocabularies for all your fields. For example, tissue could be described with the UBERON ontologies, species using the NCBI taxonomy, diseases using the Mondo database, etc. Unfortunately, implementing a systematic way of using these vocabularies is rather complex and outside the scope of this workshop, but you are very welcome to try to implement them on your own!\n\n\n\nREADME.md file\nThe README.md file is a markdown file that allows you to write a long description of the data placed in a folder. Since it is a markdown file, you are able to write in rich text format (bold, italic, include links, etc) what is inside the folder, why it was created/collected, and how and when. If it is an Assay folder, you could include the laboratory protocol used to generate the samples, images explaining the experiment design, a summary of the results of the experiment, and any sort of comments that would help to understand the context of the experiment. On the other hand, a ‚ÄòProject‚Äô README file may contain a description of the project, what are its aims, why is it important, what ‚ÄòAssays‚Äô is it using, how to interpret the code notebooks, a summary of the results and, again, any sort of comments that would help to understand the project.\nHere is an example of a README file for a Project folder:\n# NGS Analysis Project: Exploring Gene Expression in Human Tissues\n\n## Aims\n\nThis project aims to investigate gene expression patterns across various human tissues using Next Generation Sequencing (NGS) data. By analyzing the transcriptomes of different tissues, we seek to uncover tissue-specific gene expression profiles and identify potential markers associated with specific biological functions or diseases.\n\n## Why It's Important\n\nUnderstanding tissue-specific gene expression is crucial for deciphering the molecular basis of health and disease. Identifying genes that are uniquely expressed in certain tissues can provide insights into tissue function, development, and potential therapeutic targets. This project contributes to our broader understanding of human biology and has implications for personalized medicine and disease research.\n\n## Datasets\n\nWe have used internal datasets with IDs: RNA_humanSkin_20201030, RNA_humanBrain_20210102, RNA_humanLung_20220304.\n\nIn addition, we utilized publicly available NGS datasets from the GTEx (Genotype-Tissue Expression) project, which provides comprehensive RNA-seq data across multiple human tissues. These datasets offer a wealth of information on gene expression levels and isoform variations across diverse tissues, making them ideal for our analysis.\n\n## Summary of Results\n\nOur analysis revealed distinct gene expression patterns among different human tissues. We identified tissue-specific genes enriched in brain tissues, highlighting their potential roles in neurodevelopment and function. Additionally, we found a set of genes that exhibit consistent expression across a range of tissues, suggesting their fundamental importance in basic cellular processes.\n\nFurthermore, our differential expression analysis unveiled significant changes in gene expression between healthy and diseased tissues, shedding light on potential molecular factors underlying various diseases. Overall, this project underscores the power of NGS data in unraveling intricate gene expression networks and their implications for human health.\n\n---\n\nFor more details, refer to our [Jupyter Notebook](link-to-jupyter-notebook.ipynb) for the complete analysis pipeline and code.\n\n\nmetadata.yml\nThe metadata file is a yml file, which is a text document that contains data formatted using a human-readable data format for data serialization.\n\n\n\nyaml file example\n\n\n\n\nMetadata fields\nThere is a ton of information you can collect regarding an NGS assay or a project. Some information fields are very general, such as author or date, while others are specific to the Assay or Project folder. Below, we will take a look at the minimal information you should collect in each of the folders.\n\nGeneral metadata fields\nHere you can find a list of suggestions for general metadata fields that can be used for both assays and project folders:\n\nTitle: A brief yet informative name for the dataset.\nAuthor(s): The individual(s) or organization responsible for creating the dataset. You can use your ORCID\nDate Created: The date when the dataset was originally generated or compiled. Use YYYY-MM-DD format!\nDescription: A short narrative explaining the content, purpose, and context.\nKeywords: A set of descriptive terms or phrases that capture the folder‚Äôs main topics and attributes.\nVersion: The version number or identifier for the folder, useful for tracking changes.\nLicense: The type of license or terms of use associated with the dataset/project.\n\n\n\nAssay metadata fields\nHere you will find a table with possible metadata fields that you can use to annotate and track your Assay folders:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nassay_ID\nIdentifier for the assay that is at least unique within the project\n&lt;Assay-ID\\&gt;_&lt;keyword\\&gt;_YYYYMMDD\nNA\nCHIP_Oct4_20200101\n\n\nassay_type\nThe type of experiment performed, eg ATAC-seq or seqFISH\nNA\nontology field- e.g. EFO or OBI\nChIPseq\n\n\nassay_subtype\nMore specific type or assay like bulk nascent RNAseq or single cell ATACseq\nNA\nontology field- e.g. EFO or OBI\nbulk ChIPseq\n\n\nowner\nOwner of the assay (who made the experiment?).\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nJose Romero\n\n\nplatform\nThe type of instrument used to perform the assay, eg Illumina HiSeq 4000 or Fluidigm C1 microfluidics platform\nNA\nontology field- e.g. EFO or OBI\nIllumina\n\n\nextraction_method\nTechnique used to extract the nucleic acid from the cell\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nlibrary_method\nTechnique used to amplify a cDNA library\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nexternal_accessions\nAccession numbers from external resources to which assay or protocol information was submitted\nNA\neg protocols.io, AE, GEO accession number, etc\nGSEXXXXX\n\n\nkeyword\nKeyword for easy identification\nwordWord\ncamelCase\nOct4ChIP\n\n\ndate\nDate of assay creation\nYYYYMMDD\nNA\n20200101\n\n\nnsamples\nNumber of samples analyzed in this assay\n&lt;integer\\&gt;\nNA\n9\n\n\nis_paired\nPaired fastq files or not\n&lt;single OR paired\\&gt;\nNA\nsingle\n\n\npipeline\nPipeline used to process data and version\nNA\nNA\nnf-core/chipseq -r 1.0\n\n\nstrandedness\nThe strandedness of the cDNA library\n&lt;+ OR - OR *\\&gt;\nNA\n*\n\n\nprocessed_by\nWho processed the data\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nSarah Lundregan\n\n\norganism\nOrganism origin\n&lt;Genus species\\&gt;\nTaxonomy name\nMus musculus\n\n\norigin\nIs internal or external (from a public resources) data\n&lt;internal OR external\\&gt;\nNA\ninternal\n\n\npath\nPath to files\n&lt;/path/to/file\\&gt;\nNA\nNA\n\n\nshort_desc\nShort description of the assay\nplain text\nNA\nOct4 ChIP after pERK activation\n\n\nELN_ID\nID of the experiment/assay in your Electronic Lab Notebook software, like labguru or benchling\nplain text\nNA\nNA\n\n\n\n\n\n\n\n\n\n\nProject metadata fields\nHere you will find a table with possible metadata fields that you can use to annotate and track your Project folders:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nproject\nProject ID\n&lt;surname\\&gt;_et_al_2023\nNA\nproks_et_al_2023\n\n\nauthor\nOwner of the project\n&lt;First name\\&gt; &lt;Surname\\&gt;\nNA\nMartin Proks\n\n\ndate\nDate of creation\nYYYYMMDD\nNA\n20230101\n\n\ndescription\nShort description of the project\nPlain text\nNA\nThis is a project describing the effect of Oct4 perturbation after pERK activation\n\n\n\n\n\n\n\n\n\n\n\nMore info\nThe information provided in this lesson is not at all exhaustive. There might be many more fields and controlled vocabularies that could be useful for your NGS data. We recommend that you take a look at the following sources for more information!\n\nTranscriptomics metadata standards and fields\nBionty: Biological ontologies for data scientists.\n\n\n\n\n\n\n\nExercise 2: modify the metadata.yml files in your Cookiecutter templates\n\n\n\n\n\n\n\nWe have seen some examples of metadata for NGS data. It is time now to customize your Cookiecutter templates and modify the metadata.yml files so that they fit your needs!\n\nThink about what kind of metadata you would like to include.\nModify the cookiecutter.json file so that when you create a new folder template, all the metadata is filled accordingly.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\n\n\ncookiecutter_json_example\n\n\n\n\n\n\n\n\nModify the metadata.yml file so that it includes the metadata recorded by the cookiecutter.json file.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\n\n\nassay_metadata_example\n\n\n\n\n\n\n\n\nModify the README.md file so that it includes the short description recorded by the cookiecutter.json file.\nGit add, commit, and push the changes to your template.\nTest your folders by using the command cookiecutter &lt;URL to your cookiecutter repository in GitHub&gt;"
  },
  {
    "objectID": "develop/01_RDM_intro.html#research-data-cycle",
    "href": "develop/01_RDM_intro.html#research-data-cycle",
    "title": "1. Introduction to RDM",
    "section": "Research Data Cycle",
    "text": "Research Data Cycle\nThe Research Data Life Cycle is a structured framework depicting the stages of data from its creation or collection to its eventual archiving or disposal. Comprising inception, collection, processing, analysis, sharing, and preservation stages, the cycle parallels a living organism‚Äôs growth, demanding tailored management at each phase. Mastering this cycle is vital for researchers to maintain data integrity, accessibility, and long-term (re)usability, as it fosters transparency, reproducibility, and collaboration in scientific research.\nThe data life cycle is described in 6 phases:\n\nPlan: definition of the objectives, and data requirements, and develop a data management plan outlining data collection, storage, sharing, and ethical/legal considerations.\nCollect and Document: data is gathered according to the plan, and important details such as source, collection methods, and modifications are documented to ensure quality and facilitate future use.\nProcess and Analyse: data is processed and analyzed using various methods and tools to extract meaningful insights. This involves transforming, cleaning, and formatting data for analysis.\nStore and Secure: data is stored securely to prevent loss, unauthorized access, or corruption. Researchers select appropriate storage solutions and implement security measures to protect sensitive information.\nPublish and Share: sharing data openly, following Open Science and FAIR principles, to foster collaboration, increase research visibility, and enable data reuse by others.\nPreserve: Valuable data is preserved in trusted repositories or archives to ensure long-term accessibility and usability for the scientific community.\n\n\n\n\nResearch Data Life Cycle, University of Copenhagen RDM guidelines\n\n\n\n\n\n\n\n\nRead more on your institution‚Äôs website\n\n\n\n\n\n\n\n\nUniversity of Copenhagen and pdf link\nAarhus University\nUniversity of Southern Denmark, SDU\nAalborg University and FAIR link\nUniversity of Southern Denmark, SDU\n\n\n\n\n\n\nTo delve deeper into this topic, click below and explore each phase of the data life cycle. You will find tips and links to future material.\n\n\n Phases of the data life cycle in detail\n\n\n\n1. Plan\nThe management of research data must be thoroughly considered before physical materials and digital data are collected, observed, generated, created, or reused. This includes developing and documenting data management plans (DMP) in electronic format.DMPs should be updated when significant changes occur and stored alongside the corresponding research data. It‚Äôs essential to discuss DMPs with project collaborators, research managers, and supervisors to establish responsibilities for data management activities during and after research projects.\n\n\n\n\n\n\nTip\n\n\n\nCheck out next lesson to learn more about creating effective DMPs.\n\n\n\n\n2. Collect and Document\nResearch data collection and processing should be in line with the best practices within the respective research discipline. Research projects should be documented in a way that enables reproducibility by others. This entails providing clear and accurate descriptions of project methodology, software, and code utilized. Additionally, workflows for data preprocessing and file structuring should be outlined.\nResearch data should be described in metadata to enable effective searching, identification, and interpretation of the data, with metadata linked to the research data for as long as they exist.\n\n\n\n\n\n\nTip\n\n\n\n\nWe will cover strategies for organizing your files and folder in lesson 3.\nWe will discuss different types of metadata in lesson 4\n\n\n\n\n\n3. Process and analyze\nDuring this phase, researchers employ computational methods and bioinformatics tools to extract meaningful information from the data. Good coding practices ensure well-documented and reproducible analyses. For example, code notebooks and version control tools, such as Git, are essential for transparency and sharing results with the scientific community.\nTo streamline and standardize the data analysis process, researchers often implement workflows and pipelines, automating multiple analysis steps to enhance efficiency and consistency while promoting reproducibility.\n\n\n\n\n\n\nTip\n\n\n\n\nCollaborative efforts by the nf-core community provide curated pipelines across different areas of bioinformatics and genomics.\nLearn more about version control in lesson 5\nIf you want to implement your own pipelines, we have the course for you [IN DEVELOPMENT].\n\n\n\n\n\n4. Store and Secure\nResearch data must be classified based on sensitivity and the potential impact to the research institution from unauthorized disclosure, alteration, or destruction. Risks to data security and data loss should be assessed accordingly. This includes evaluating:\n\nPhysical and digital access to research data\nRisks associated with data management procedures\nBackup requirements and backup procedures\nExternal and internal threats to data confidentiality, integrity and accessibility\nFinancial, regulatory, and technical consequences of working with data, data storage, and data preservation\n\n\n\n\n\n\n\nWarning\n\n\n\nThis step is very specific to the setup used in your environment so we cannot include it in a comprehensive guideline on this matter.\n\nEnroll in the next GDPR course offered by the Center for Health Data Science to learn more about data protection and GDPR compliance.\n\n\n\n\n\n5. Share and publish\nLegislation or agreements may restrict research data sharing and require obtaining relevant approvals and establishing agreements allowing sharing. By default, research data should be made openly available post-project, especially for data underlying research publications. This approach balances openness with considerations like intellectual property, personal data protection, and national interests in accordance with the principle of ‚Äòas open as possible, as closed as necessary‚Äô. When data cannot be openly shared, sharing associated metadata is encouraged.\nAdherence to FAIR principles (findable, accessible, interoperable, and reusable) is crucial, which includes:\n\nProviding open access to data (Open Data) by depositing data in a data repository, or by providing access to information on whether, when, how, and to what extent data can be accessed if data sets cannot be made openly available.\nUsing persistent identifiers (PID) and metadata (such as descriptive keywords) that help locate the data set.\nCommunicating terms for data reuse, for example by attaching a data license.\nOffering the necessary information to understand the process of data creation, purpose, and structure.\n\n\n\n\n\n\n\nTip\n\n\n\n\nMore on FAIR and OS principles in the next section\n\n\n\n\n\n6. Preserve\nArrangements for long-term preservation (data and metadata) must adhere to legislation and agreements. This should include:\n\nInformation on research data: At least the data sets supporting published research must be preserved to address objections or criticisms.\nPreservation duration: Retain data supporting research publications for at least five years post-project or publication.\nChoose preservation format and location: Determine format, location, and associated metadata.\nDelete/destroy data if excluded by legislation or agreements, or if preservation isn‚Äôt necessary or possible (for example, when research data can easily be reproduced or is too costly to store or when material quality will deteriorate over time).\nAssign responsibility: Appoint individuals or roles to safeguard data integrity post-project.\nDetermine access rights: Establish rights for accessing and using preserved data sets.\n\nCheck with your institution their requirements for data preservation, such as keeping copies accessible to research managers and peers at the institution‚Äôs premises.\n\n\n\n\n\n\nExample - University of Copenhagen\n\n\n\n\n\n\n\nFor example, the UCPH mandates that a copy of data sets and associated metadata must remain at UCPH after the project ends and/or when employment with the University ceases, in a way in which they are accessible to research managers and understandable for research managers and peers, unless legislation or agreements determine otherwise.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe will check about which repositories you can use to preserve your NGS data in the lesson 7\n\n\n\n\nTo guarantee effective RDM, researchers should follow the FAIR principles.",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "develop/01_RDM_intro.html#semantics-of-rdm",
    "href": "develop/01_RDM_intro.html#semantics-of-rdm",
    "title": "Introduction",
    "section": "Semantics of RDM",
    "text": "Semantics of RDM\nThe definition of Management is ‚Äúthe practice of managing; handling, supervision, or control.‚Äù.\nIn accordance with the UCPH Policy for Research Data Management, research data encompasses both physical material and digital information gathered, observed, produced, or formulated during research activities carried out during research. This broad definition includes various types of data serving as the foundation for the research, such as specimens, notebooks, interviews, texts, literature, digital raw data, recordings, computer code, and meticulous documentation of these materials and data, forming the core of the analysis that underlies the research outcomes."
  },
  {
    "objectID": "develop/01_RDM_intro.html#guidelines-and-benefits-of-effective-rdm",
    "href": "develop/01_RDM_intro.html#guidelines-and-benefits-of-effective-rdm",
    "title": "1. Introduction to RDM",
    "section": "Guidelines and benefits of effective RDM",
    "text": "Guidelines and benefits of effective RDM\nRDM ensures ethical and legal compliance with research requirements. Effective RDM can significantly benefit research and provide advantages for individual researchers:\n\nDetailed data management planning helps in identifying and addressing potential uses, alining expectations among collaborators, and clarifying data rights and ownership\nTransparent and Structured Data Management enhances the reliability and credibility of research findings\nData documentation and data sharing promotes discoverability and facilitates collaborations. Clear documentation of research also streamlines access to previous work, enhancing efficiency, building upon existing knowledge, maximizing research value, accelerating scientific discoveries, and improving visibility and impact\nRisk assessments and strategies for data storage and security can prevent data loss, breaches, or misuse and safeguard sensitive data\nLong-Term Preservation. Data accessibility well after the project‚Äôs completion contributes to data accessibility and continued research relevance\n\n\n\n\n\n\n\nConsequences of poor RDM\n\n\n\n\n\nSeveral surveys have shown that data scientists spend almost half of their time loading and cleaning data, becoming the most consuming, and what many would call tedious, tasks of their jobs (‚ÄúThe State of Data Science 2020 Moving from Hype Toward Maturity‚Äù 2020; ‚ÄúCleaning Big Data: Most Time-Consuming, Least Enjoyable Data Science Task, Survey Says‚Äù 2016).\nCan you consider why we dedicate such a significant amount of time? Perhaps these images look familiar to you!\n Caption: Top-left: Photo by Wonderlane on Unsplash; Top-right: From Stanford Center for Reproducible Neuroscience; Bottom: Messy folder structure, by J.A.HR\nIneffective data management practices can have significant consequences that affect your future self, colleagues, or collaborators who may have to deal with your data. The implications of poor data management include:\n\nDifficulty in Data Retrieval: Without proper organization and documentation, finding specific data files or understanding their content becomes challenging and time-consuming, leading to inefficiency.\nLoss of Data: Inadequate backup and storage strategies increase the risk of data loss (hardware failures, accidental deletions‚Ä¶), potentially erasing months or years of work.\nData Incompleteness and Errors: Insufficient documentation leads to ambiguity and errors in data interpretation undermining research credibility.\nDifficulty in Reproducibility: Poor data management hinders scientific progress by impeding the reproduction of research results.\nDelayed or Compromised Collaboration: disorganized data slows down collaborative research projects, hindering communication.\nData Security and Privacy Risks: Inadequate security measures measures expose sensitive information to breaches, risking privacy.\nWasted Time and Resources: Poor management diverts resources from research tasks, increasing labor costs (additional time on data management).\nFinancial Implications: Time-consuming data management tasks lead to increased labor costs and potential project delays. Data loss can also have negative implications.\nReputational Damage: Inaccurate or irreproducible research outcomes harm a researcher‚Äôs credibility in the scientific community.\n\nTo address these challenges, prioritizing and investing in effective RDM practices like organization, documentation, backup strategies, and data security and preservation protocols, can prevent setbacks, ensure data integrity, and enhance scientific research reliability.\n\n\n\n\n\n\n\n\n\nExercise 1: RDM practices\n\n\n\n\n\n\n\nThink about situations that you believe are consequences of poor data management that may have occurred in your research environment, or discuss if you have encountered any of the following.\n\nA researcher struggles over time with disorganized data, hindering efficient locating of files and causing delays in analysis.\nInadequate documentation of data collection leads to misinterpretations and errors in analysis by other researchers or colleagues.\nPoorly organized data requires extensive cleaning, wasting valuable research time.\nLack of proper documentation and data availability in a groundbreaking study raises doubts about the validity of its findings (from the lack of reproducibility).\n\nHow would you approach these issues differently or what steps would you take to address them?\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\nImplementation of a clear and consistent folder structure with descriptive file names. Additionally, using version control systems, such as Git, for code and analysis files can help track changes and facilitate easy retrieval of previous versions of analyses and results.\nProper data documentation, including detailed metadata, could have been maintained throughout the data collection process, providing necessary context and reducing the risk of incomplete or ambiguous data.\nFollowing FAIR principles (Findable, Accessible, Interoperable, Reusable) by making their data, along with detailed methods and documentation, openly accessible in a reputable data repository.\nImplementation of management strategies from the outset of the research project saves time and resources later on, ensuring that data is well-organized and properly documented.",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "develop/02_DMP.html",
    "href": "develop/02_DMP.html",
    "title": "2. Data Management Plan",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nLearn what is a DMP\nLearn about the different DMP templates\nHow to write a DMP focused on NGS data\nThe process of data management involves implementing tailored best practices for your data but how do you ensure comprehensive coverage of the decisions and that data is well-managed throughout its life cycle. To achieve this, a Data Management Plan (DMP) is essential.\nDMP are required for grant applications to ensure research data to be FAIR. A DMP serves as a comprehensive document detailing strategies for handling project data, code, and documentation across its life cycle. It includes plans for data collection, documentation, organization, and preservation.",
    "crumbs": [
      "Course material",
      "Basics",
      "2. Data Management Plan"
    ]
  },
  {
    "objectID": "develop/02_DMP.html#why-are-dmps-important",
    "href": "develop/02_DMP.html#why-are-dmps-important",
    "title": "Data Management Plans",
    "section": "",
    "text": "There are several reasons why writing a data management plan is a very good idea:\n\nThink of the DMP as a checklist. Going through the checklist allows you to identify gaps in current data management strategies. Identifying the gaps early on saves a lot of headache and time spent later. Going through the process of planning is more important than the actual plan itself.\nIn a project with several members, it is important to decide on standards that all collaborators should adhere to, e.g.¬†regarding how to organise the data, how to name it, which metadata standards to use, what vocabularies to use, etc.\nWriting a DMP also enables you to estimate costs regarding data production, storage, data management, etc.\nIt is also a good way to clarify responsibilities regarding the data and the data management, e.g.¬†who is responsible for the execution of the DMP.\nBy planning how the data will be managed, there‚Äôs greater chance that the research data will be well-managed (no guarantee, since you still need to have good strategies and actually implement them for this to happen). Of course there are many benefits with well-managed data but the main ones are:\n\nreproducibility, so that the results can be verified\nreusability, so that this data can be used for answering other scientific questions, thus reducing redundancy\nA DMP is the first step towards being FAIR in your project.\n\n\nIf the reasons above don‚Äôt persuade you, the last argument is that it is becoming more and more a requirement by funders and other stakeholders:\n\nFor transparency and openness: publicly funded research data must be discoverable, accessible, and reusable to the public\nReturn on investment: well planned data maximizes the research potential of the data and provides greater returns on public investments and research.",
    "crumbs": [
      "Course material",
      "Basics",
      "Data Management Plans"
    ]
  },
  {
    "objectID": "develop/02_DMP.html#when-to-write-a-dmp",
    "href": "develop/02_DMP.html#when-to-write-a-dmp",
    "title": "2. Data Management Plan",
    "section": "When to write a DMP",
    "text": "When to write a DMP\nA DMP is a living document, the initial version is written the same time as a new project idea is emerging, before e.g.¬†applying for funds, and then successively updated as the project continues and new decisions are made. Ideally it should be updated continuously, but there are three major time points:\n\nProject planning: The DMP should outline the strategies for data management in sufficient detail to be able to estimate the resources needed to implement the DMP, so that this can be included in the proposal for funding (e.g.¬†data production, data analysis, storage during and after project, costs related to publishing of data).\nProject start: The DMP is completed with more details e.g.¬†about documentation, data quality measures, file and folder strategies, etc.\nProject end: The DMP is updated a final time with e.g.¬†links to published data and details about archiving (what data and where), so that this document enables future re-use of the project (by yourself or others).",
    "crumbs": [
      "Course material",
      "Basics",
      "2. Data Management Plan"
    ]
  },
  {
    "objectID": "develop/02_DMP.html#the-main-parts-of-a-dmp",
    "href": "develop/02_DMP.html#the-main-parts-of-a-dmp",
    "title": "2. Data Management Plan",
    "section": "The main parts of a DMP",
    "text": "The main parts of a DMP\n\nDescription of data\n\nWhat types of data will be created and/or collected, in terms of data format and amount/volume of data?\n\nDocumentation\n\nHow will the material be documented and described, with associated metadata relating to structure, standards and format for descriptions of the content, collection method, etc.?\n\nStorage and backup\n\nHow is data security, storage and backup of data and metadata safeguarded during the research process?\n\nLegal and ethical aspects\n\nHow is data handling according to legal requirements safeguarded, e.g.¬†in terms of handling of personal data, confidentiality and intellectual property rights?\n\nAccessibility and long-term storage\n\nHow, when and where will research data or information about data (i.e.¬†metadata) be made accessible? E.g. via deposition to international public repositories.\nIn what way is long-term storage safeguarded, and by whom?\n\nResponsibility and resources\n\nWho are the responsible persons for data management?\nWhat resources (costs, labour input or other) will be required for data management?",
    "crumbs": [
      "Course material",
      "Basics",
      "2. Data Management Plan"
    ]
  },
  {
    "objectID": "develop/02_DMP.html#dmp-templates",
    "href": "develop/02_DMP.html#dmp-templates",
    "title": "2. Data Management Plan",
    "section": "DMP templates",
    "text": "DMP templates\nDifferent agencies and funders may have different DMP templates that are mandatory for researchers to comply with data management requirements. Here you can find some examples.\n\nUCPH template\nThe University of Copenhagen has developed a DMP template in alignment with the UCPH Policy on Research Data Manage‚Äãment. This is the recommended template for all researchers, guests and students involved in research activities at UCPH, unless other specific DMP requirements from a funding agency or a partner institution apply.\nThe UCPH DMP template is available within the DMPonline tool (see information below) or can be downloaded as a Word document.\nGuidance for all questions in this template can be found here.\n\n\nHorizon Europe and ERC templates\nAll projects funded in the Horizon Europe framework programme must prepare and submit a data management plan as deliverable according to the details described in the respective grant agreement.",
    "crumbs": [
      "Course material",
      "Basics",
      "2. Data Management Plan"
    ]
  },
  {
    "objectID": "develop/02_DMP.html#writing-a-dmp",
    "href": "develop/02_DMP.html#writing-a-dmp",
    "title": "2. Data Management Plan",
    "section": "Writing a DMP",
    "text": "Writing a DMP\nStandard DMP templates can typically be found at funder agencies, e.g.¬†Swedish Research Council and Science Europe, and it is of course possible to write in your favorite text editor. In addition, there are several tools that can assist you in writing a DMP as well as guidance.\n\n\n\n\n\n\nTools to write your DMP\n\n\n\n\nDMPonline: DMPOnline is an online tool for writing, sharing and reviewing data management plans. A Danish installation is available under dmponline.deic.dk. It is provided by the Danish e-Infrastructure Cooperation (DeiC) and jointly administered by the Royal Danish Library and DTU Library. DMPonline contains a number of different DMP templates, guidance texts and examples from Danish research institutions and relevant funders.\n- The tool most universities have chosen to offer (check with your institute) - Good guidance but typically generic and not Life Science specific - Most often free text answers - Contains guidance for several DMP templates\nData Stewardship wizard. DSW is a interactive tool to create data management plans based on questionnaires. - Provided by SciLifeLab - Gives Life Science specific guidance - Less free text answers, instead many questions with answer options",
    "crumbs": [
      "Course material",
      "Basics",
      "2. Data Management Plan"
    ]
  },
  {
    "objectID": "develop/02_DMP.html#example-of-a-dmp-for-ngs-data",
    "href": "develop/02_DMP.html#example-of-a-dmp-for-ngs-data",
    "title": "2. Data Management Plan",
    "section": "Example of a DMP for NGS data",
    "text": "Example of a DMP for NGS data\nWe are have written a DMP template that it is prefilled with repetitive information using DMPonline and the Horizon Europe guidelines. This template contains all the necessary information regarding common practices that we will use, the repositories we use for NGS, etc. The template is part of the project folder template, under documents. You can check the file here.\nThe Horizon Europe template is mostly focused on digital data and so, it is maybe not the best option if you are also interested in recording in your DMP physical data, such as samples, reagents, media, cultures, model organisms, etc.\n\n\n\n\n\n\nExercise: write a draft for a DMP\n\n\n\n\n\n\n\nYou can write a DMP draft from scratch, or you could use the template mentioned above and modify it to your own needs!",
    "crumbs": [
      "Course material",
      "Basics",
      "2. Data Management Plan"
    ]
  },
  {
    "objectID": "develop/02_DMP.html#wrap-up",
    "href": "develop/02_DMP.html#wrap-up",
    "title": "2. Data Management Plan",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we covered Data Management Plans (DMPs) and introduced tools to aid in their creation, along with a template for your projects. DMPs serve as valuable aids in project planning, data preservation, and facilitating future use by yourself, collaborators, or the wider scientific community. Next, we‚Äôll delve into organizing data efficiently and provide helpful tools for the task.\n\nSources\n\nNBISweden workshop on RDM practices\nStanford University Library Data Management Services website\nRDM Guide, Elixir Belgium",
    "crumbs": [
      "Course material",
      "Basics",
      "2. Data Management Plan"
    ]
  },
  {
    "objectID": "develop/01_RDM_intro.html#slide-title",
    "href": "develop/01_RDM_intro.html#slide-title",
    "title": "Introduction",
    "section": "Slide Title",
    "text": "Slide Title\n\n1. Plan\nThe management of research data must be thoroughly considered before physical materials (samples, model organisms, reagents or media, etc) and digital data (bioinformatics data) are collected, observed, generated, created or reused. Data management plans (DMP) must be developed and documented, preferably in electronic format. DMPs should be updated when significant changes to the management of research data occur and (references to) the DMP should be stored with the corresponding research data as long as they exist.\nThe DMP should be discussed with project collaborators, research managers and supervisors (if any), ensuring that agreements are reached regarding responsibilities for different research data management activities during and after research projects.\n\n\n\n\n\n\nTip\n\n\n\nWe will check about how to write a DMP for NGS data in the next lesson.\n\n\n\n\n2. Collect and Document\nResearch data should be collected and processed in line with best practice in the research discipline. For example, there will be important differences between a metagenomics project and a clinical trial involving human samples! Research projects should be documented in a way that allows them to be repeated by others. Among other things, this includes clearly and accurately describing project methodology and any equipment, software or code used. This includes workflows for data preprocessing and how you will structure and organize your files.\n\n\n\n\n\n\nTip\n\n\n\nWe will see about how to structure your files and folders in the 6th lesson.\n\n\nIn addition, research data should be described using appropriate metadata to facilitate searching for, the identification of, and the interpretation of the research data. Metadata should be linked to the research data as long as they exist, unless legislation or agreements state otherwise.\n\n\n\n\n\n\nTip\n\n\n\nWe will see about what kind of metadata you can use for your NGS data in the 7th lesson\n\n\n\n\n3. Process and analyse\nThis, in the data life cycle for NGS data, is a critical phase that involves transforming raw sequencing data into meaningful biological insights. During this stage, researchers apply computational methods and bioinformatics tools to extract valuable information from the vast amount of sequencing data generated in NGS experiments.\nThroughout this phase, researchers should adhere to good coding practices, ensuring well-documented and reproducible analyses. Code notebooks and version control tools, such as Git, help maintain transparency and facilitate the sharing of methods and results with the scientific community.\nTo streamline and standardize the data analysis process, researchers often employ workflows and pipelines. Workflows automate the execution of multiple analysis steps, enhancing efficiency and consistency while promoting reproducibility. The NGS research community benefits from collaborative efforts, such as the nf-core community, which provides pre-established and validated Nextflow-based pipelines for various NGS applications. Leveraging community-developed pipelines ensures adherence to best practices and accelerates the pace of research through shared expertise.\n\n\n\n\n\n\nTip\n\n\n\nWe will see more about version control in lesson 9\n\n\n\n\n4. Store and Secure\nResearch data must be classified at the start of a research project on the basis of the level of sensitivity and the impact to the University if data are disclosed, altered or destroyed without authorisation. Risks to data security and of data loss should be assessed in relation to the data classification. This includes evaluating:\n\nPhysical and digital access to research data\nRisks associated with data management procedures\nBackup requirements and backup procedures\nExternal and internal threats to data confidentiality, integrity and accessibility\nFinancial, regulatory and technical consequences of working with data, data storage and data preservation\n\n\n\n\n\n\n\nWarning\n\n\n\nThis step is very specific to the setup used in your environment. Maybe you are using a department server with storage, or you use a personal drive, or cloud solutions such as AWS. Thus, we cannot include in this course a comprehensive guideline on this matter.\n\n\n\n\n\n\n\n\nAbout GPDR and protected data\n\n\n\nThis course does not talk in detail about protected NGS data, such as human or patient samples. For more information about GPDR protected data, check out our course!\n\n\n\n\n5. Share and publish\nLegislation or agreements may preclude research data sharing or impose conditions for sharing. Before sharing research data, the relevant approvals need to be obtained and, if necessary, the appropriate agreements set up to allow data and material sharing.\nBy default, research data should be made openly available after project end, as a minimum for data sets underlying research publications. Concerns relating to intellectual property rights, personal data protection, information security as well as commercial and national interests and legislation must be taken into account in accordance with the principle of ‚Äòas open as possible, as closed as necessary‚Äô. If the research data cannot be made available, sharing the metadata associated with the research data should be considered. The FAIR principles (for findable, accessible, interoperable and reusable research objects) should be followed as much as possible when preparing digital data sets that can be shared. This includes as a minimum:\n\nProviding open access to data (Open Data) by depositing data in a data repository, or by providing access to information on whether, when, how, and to what extent data can be accessed if data sets cannot be made openly available.\nAs much as possible using persistent identifiers (PID) and metadata (such as descriptive keywords) that help locate the data set.\nCommunicating terms and conditions for data reuse by others, for example by attaching a data licence.\nProviding the information necessary to understand how data sets were created and structured, and for what purpose.\n\n\n\n\n\n\n\nTip\n\n\n\nWe will talk about FAIR and OS principles in the next lesson\n\n\n\n\n6. Preserve\nAppropriate arrangements for the long-term preservation of digital data, physical material and associated metadata must be made, adhering to legislation and/or agreements. This should include:\n\nDeciding which research data will be preserved. As a minimum, data sets underlying published research results must be preserved so that any objections or criticisms can be addressed.\nDeciding how long research data will be preserved. Data sets underlying research publications should be retained for at least five years after project completion or date of publication, whichever comes last.\nChoosing a format and location in which research data should be preserved, and deciding what metadata should be associated with the preserved data and material.\nDeleting/destructing research data if legislation or agreements exclude preservation, or when researchers and their managers determine that preservation is not required (for example when research data can easily be reproduced) or not possible (for example when research data are too costly to store or when material quality will deteriorate over time).\nAssigning a person, persons or role(s) responsible for the research data after project end. Responsibilities include safeguarding the long-term integrity of data sets.\nDetermining rights, for example, of access to and use of preserved data sets.\n\nIn addition, your institution may oblige you to preserve and keep a copy of your data at their premises. They will usually explain any requirement at their own data management department. For example, the UCPH mandates that a copy of data sets and associated metadata must remain at UCPH after project end and/or when employment with the University ceases, in a way in which they are accessible to research managers and understandable for research managers and peers, unless legislation or agreements determine otherwise.\n\n\n\n\n\n\nTip\n\n\n\nWe will check about which repositories you can use to preserve your NGS data in the 10th lesson"
  },
  {
    "objectID": "develop/01_RDM_intro.html#why-are-dmps-important",
    "href": "develop/01_RDM_intro.html#why-are-dmps-important",
    "title": "Introduction",
    "section": "Why are DMPs important",
    "text": "Why are DMPs important\nThere are several reasons why writing a data management plan is a very good idea:\n\nThink of the DMP as a checklist. Going through the checklist allows you to identify gaps in current data management strategies. Identifying the gaps early on saves a lot of headache and time spent later. Going through the process of planning is more important than the actual plan itself.\nIn a project with several members, it is important to decide on standards that all collaborators should adhere to, e.g.¬†regarding how to organise the data, how to name it, which metadata standards to use, what vocabularies to use, etc.\nWriting a DMP also enables you to estimate costs regarding data production, storage, data management, etc.\nIt is also a good way to clarify responsibilities regarding the data and the data management, e.g.¬†who is responsible for the execution of the DMP.\nBy planning how the data will be managed, there‚Äôs greater chance that the research data will be well-managed (no guarantee, since you still need to have good strategies and actually implement them for this to happen). Of course there are many benefits with well-managed data but the main ones are:\n\nreproducibility, so that the results can be verified\nreusability, so that this data can be used for answering other scientific questions, thus reducing redundancy\nA DMP is the first step towards being FAIR in your project.\n\n\nIf the reasons above don‚Äôt persuade you, the last argument is that it is becoming more and more a requirement by funders and other stakeholders:\n\nFor transparency and openness: publicly funded research data must be discoverable, accessible, and reusable to the public\nReturn on investment: well planned data maximizes the research potential of the data and provides greater returns on public investments and research."
  },
  {
    "objectID": "develop/01_RDM_intro.html#when-to-write-a-dmp",
    "href": "develop/01_RDM_intro.html#when-to-write-a-dmp",
    "title": "Introduction",
    "section": "When to write a DMP",
    "text": "When to write a DMP\nA DMP is a living document, the initial version is written the same time as a new project idea is emerging, before e.g.¬†applying for funds, and then successively updated as the project continues and new decisions are made. Ideally it should be updated continuously, but there are three major time points:\n\nProject planning: The DMP should outline the strategies for data management in sufficient detail to be able to estimate the resources needed to implement the DMP, so that this can be included in the proposal for funding (e.g.¬†data production, data analysis, storage during and after project, costs related to publishing of data).\nProject start: The DMP is completed with more details e.g.¬†about documentation, data quality measures, file and folder strategies, etc.\nProject end: The DMP is updated a final time with e.g.¬†links to published data and details about archiving (what data and where), so that this document enables future re-use of the project (by yourself or others)."
  },
  {
    "objectID": "develop/01_RDM_intro.html#the-main-parts-of-a-dmp",
    "href": "develop/01_RDM_intro.html#the-main-parts-of-a-dmp",
    "title": "Introduction",
    "section": "The main parts of a DMP",
    "text": "The main parts of a DMP\n\nDescription of data\n\nWhat types of data will be created and/or collected, in terms of data format and amount/volume of data?\n\nDocumentation\n\nHow will the material be documented and described, with associated metadata relating to structure, standards and format for descriptions of the content, collection method, etc.?\n\nStorage and backup\n\nHow is data security, storage and backup of data and metadata safeguarded during the research process?\n\nLegal and ethical aspects\n\nHow is data handling according to legal requirements safeguarded, e.g.¬†in terms of handling of personal data, confidentiality and intellectual property rights?\n\nAccessibility and long-term storage\n\nHow, when and where will research data or information about data (i.e.¬†metadata) be made accessible? E.g. via deposition to international public repositories.\nIn what way is long-term storage safeguarded, and by whom?\n\nResponsibility and resources\n\nWho are the responsible persons for data management?\nWhat resources (costs, labour input or other) will be required for data management?"
  },
  {
    "objectID": "develop/01_RDM_intro.html#dmp-templates",
    "href": "develop/01_RDM_intro.html#dmp-templates",
    "title": "Introduction",
    "section": "DMP templates",
    "text": "DMP templates\nDifferent agencies and funders may have different DMP templates that are mandatory for researchers to comply with data management requirements. Here you can find some examples.\n\nUCPH template\nThe University of Copenhagen has developed a DMP template in alignment with the UCPH Policy on Research Data Manage‚Äãment. This is the recommended template for all researchers, guests and students involved in research activities at UCPH, unless other specific DMP requirements from a funding agency or a partner institution apply.\nThe UCPH DMP template is available within the DMPonline tool (see information below) or can be downloaded as a Word document.\nGuidance for all questions in this template can be found here.\n\n\nHorizon Europe and ERC templates\nAll projects funded in the Horizon Europe framework programme must prepare and submit a data management plan as deliverable according to the details described in the respective grant agreement."
  },
  {
    "objectID": "develop/01_RDM_intro.html#writing-a-dmp",
    "href": "develop/01_RDM_intro.html#writing-a-dmp",
    "title": "Introduction",
    "section": "Writing a DMP",
    "text": "Writing a DMP\nStandard DMP templates can typically be found at funder agencies, e.g.¬†Swedish Research Council and Science Europe, and it is of course possible to write in your favorite text editor. In addition, there are several tools that can assist you in writing a DMP as well as guidance.\n\n\n\n\n\n\nTools to write your DMP\n\n\n\n\nDMPonline: DMPOnline is an online tool for writing, sharing and reviewing data management plans. A Danish installation is available under dmponline.deic.dk. It is provided by the Danish e-Infrastructure Cooperation (DeiC) and jointly administered by the Royal Danish Library and DTU Library. DMPonline contains a number of different DMP templates, guidance texts and examples from Danish research institutions and relevant funders.\n- The tool most universities have chosen to offer (check with your institute) - Good guidance but typically generic and not Life Science specific - Most often free text answers - Contains guidance for several DMP templates\nData Stewardship wizard. DSW is a interactive tool to create data management plans based on questionnaires. - Provided by SciLifeLab - Gives Life Science specific guidance - Less free text answers, instead many questions with answer options"
  },
  {
    "objectID": "develop/01_RDM_intro.html#example-of-a-dmp-for-ngs-data",
    "href": "develop/01_RDM_intro.html#example-of-a-dmp-for-ngs-data",
    "title": "Introduction",
    "section": "Example of a DMP for NGS data",
    "text": "Example of a DMP for NGS data\nWe are have written a DMP template that it is prefilled with repetitive information using DMPonline and the Horizon Europe guidelines. This template contains all the necessary information regarding common practices that we will use, the repositories we use for NGS, etc. The template is part of the project folder template, under documents. You can check the file here.\nThe Horizon Europe template is mostly focused on digital data and so, it is maybe not the best option if you are also interested in recording in your DMP physical data, such as samples, reagents, media, cultures, model organisms, etc.\n\n\n\n\n\n\nExercise: write a draft for a DMP\n\n\n\n\n\n\n\nYou can write a DMP draft from scratch, or you could use the template mentioned above and modify it to your own needs!"
  },
  {
    "objectID": "develop/01_RDM_intro.html#wrap-up-1",
    "href": "develop/01_RDM_intro.html#wrap-up-1",
    "title": "Introduction",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we‚Äôve covered the definition of RDM, the advantages of effective RDM practices, and the phases of the research data life cycle. While much of the material is drawn from UCPH data management guidelines in the context of omics data, it‚Äôs worth noting its applicability to other fields and institutions. Nonetheless, we recommend exploring these guidelines further at your institution (links provided above).\nIn the next lessons, we will explore different resources, tools and guidelines that can be applied to all kinds of data and how to apply it specifically for NGS data."
  },
  {
    "objectID": "develop/01_RDM_intro.html#fair-and-open-science",
    "href": "develop/01_RDM_intro.html#fair-and-open-science",
    "title": "1. Introduction to RDM",
    "section": "FAIR and Open Science",
    "text": "FAIR and Open Science\nOpen Science and FAIR principles have become essential frameworks for promoting transparency, accessibility, and reusability in scientific research. While Open Science advocates for unrestricted access to research outputs, data, and methodologies, FAIR principles emphasize making data Findable, Accessible, Interoperable, and Reusable. Together, they foster collaboration, transcend disciplinary boundaries, and support long-term data preservation. However, they were not directly relevant to software until recently. Governments and funding agencies worldwide increasingly recognize their value and are actively promoting their adoption in academia. In this section, you will learn how to apply these principles to your research.\n\nOpen Science\nOpen Science facilitates wider accessibility of research outputs, fosters collaboration, and promotes transparency and reproducibility, thus enhancing responsible research conduct. Economically, promoting Open Access and data sharing maximizes the return on research investments and fuels innovation and entrepreneurship. This approach enables businesses and startups to leverage research findings, while researchers can integrate large datasets for new discoveries.\n\n\n\nPillars of the Open Science according to UNESCO‚Äôs 2021 Open Science recommendation.\n\n\n\n\n\n\n\n\nExamples of Open Science Initiatives\n\n\n\n\n\n\n\n\nNational Institutes of Health (NIH): the USA encourages Open Science practices, including data sharing, through policies like the NIH Data Sharing Policy.\nWellcome Trust: mandates open access globally to research outputs funded by the foundation.\nEuropean Molecular Biology Organization (EMBO): supports Open Access and provides guidelines for data sharing.\nBill & Melinda Gates Foundation: advocates for Open Access and data sharing to maximize the impact of its research.\nEuropean Research Council (ERC): promotes Open Access to research publications and adheres to the FAIR principles for data management.\n\n\n\n\n\n\n\n\n\n\n\n\nBenefits of Open Science for Researchers\n\n\n\n\nIncreased Visibility and Impact: more people can access and engage with your findings.\nFacilitated Collaboration: leading to the development of innovative ideas and impactful projects.\nEnhanced Credibility: sharing data and methods openly allows for validation of research findings by others.\nAccelerated Research Progress:: by enabling researchers to build upon each other‚Äôs work and leverage shared data.\nStimulation of New Research: shared data can inspire novel research questions and discoveries.\nAttract Funding Opportunities: adhering to Open Science principles may make you eligible for additional funding opportunities.\nTransparency and Accountability: promoting responsible conduct in research.\nPLong-Term Data Preservation: by archiving research data in repositories.\n\n\n\n\n\nFAIR principles\nThe FAIR principles complementing Open Science, aim to improve research data management, sharing, and usability. FAIR stands for Findable, Accessible, Interoperable, and Reusable, enhancing the value, impact, and sustainability of research data. Adhering to FAIR principles benefits individual researchers and fosters collaboration, data-driven discoveries, knowledge advancement, and long-term preservation. However, achieving FAIR compliance is nuanced, with some aspects being more complex, especially concerning metadata standards and controlled vocabularies.\nWe strongly endorse these recommendations for those developing software or performing data analyses: https://fair-software.nl/endorse.\n\n\nBreaking down the FAIR Principles\n\n\n\nFindable\n\nResearch data should be easily identifiable, achieved through the following key components:\n\nAssign Persistent and Unique Identifiers: such as Digital Object Identifiers (DOIs) or Uniform Resource Identifiers (URIs).\nCreate Rich and Standard Metadata: describing the content, context, and characteristics of the data (such as origin, format, version, licensing, and the conditions for reuse).\nUse a Data Repository or Catalog: following FAIR principles to deposit your data, enhancing data discoverability and access from a centralized and reliable source.\n\nClear and comprehensive metadata facilitates data discovery by researchers and machines through various search engines and data repositories.\n\n\nAccessible\n\nResearch data should be accessible with minimal restrictions on access and downloading, to facilitate collaboration, verification of findings, and ensuring transparency. Key elements to follow:\n\nOpen Access: Ensure data is freely accessible without unnecessary barriers. Choose suitable licenses for broad data reuse (such as MIT, and Apache-2.0).\nAuthentication and Authorization: Implement secure mechanisms for access control, especially for sensitive data\nMetadata: Deposit metadata even when data access is restricted, providing valuable information about the dataset (version control systems).\n\n\n\n\n\n\n\nImportant note: As open as possible, as closed as necessary\n\n\n\n\n\nEnsure data accessibility aligns with privacy regulations like GDPR, and when necessary, limit access to sensitive data. When dealing with sensitive data, share query details and data sources to maintain transparency.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\nYou are working with sensitive data from a national authority regarding personal information. You won‚Äôt be able to publish or deposit the data but you can describe what was the query you used to obtain the data and its source (e.g., sample population: people between 20-30 years old, smoker).\n\n\n\n\n\n\n\n\n\n\nInteroperable\n\nInteroperability involves structuring and formatting data to seamlessly integrate with other datasets. Utilizing standard data formats, widely accepted vocabularies and ontologies enables integration and comparison across various sources. Key components to follow:\n\nStandard Data Formats: facilitating data exchange and interoperability with various software tools and platforms.\nVocabularies and Ontologies: commonly used by the scientific community ensuring data can be understood and combined with other datasets more effectively.\nLinked Data: to related data and resources, to enrich contextualization of datasets, facilitating integration and discovery of interconnected information.\n\n\n\nReusable\n\nData should be thoroughly documented and prepared, with detailed descriptions of data collection, processing, and methodology provided for replication by other researchers. Clear statements on licensing and ethical considerations are essential for enabling data reuse. Key components to follow:\n\nDocumentation and Provenance: Comprehensive documentation on data collection, processing, and analysis. Provenance information elucidates data origin and processing history.\nEthical and Legal Considerations: related to data collection and use. Additionally, adherence to legal requirements ensures responsible and ethical data reuse.\nData Licensing: Clearly stated licensing terms facilitate data reuse, specifying usage, modification, and redistribution while respecting intellectual property rights and legal constraints.\n\n\n\n\n\n\n\n\n\nTest your knowledge: use cases\n\n\n\nBefore moving on to the next lesson, take a moment to explore this practical example if you‚Äôre working with NGS data. Here, you‚Äôll find exercises and examples to reinforce the knowledge you‚Äôve acquired in this lesson.\n\n\n\nKey online links\n\nHow to FAIR DK\nFAIR principles\nFAIR software\nOpen AIRE\nDeiC - RDMElearn",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "develop/01_RDM_intro.html#fair-principles",
    "href": "develop/01_RDM_intro.html#fair-principles",
    "title": "1. Introduction to RDM",
    "section": "FAIR principles",
    "text": "FAIR principles\nThe FAIR principles complementing Open Science, aim to improve research data management, sharing, and usability. FAIR stands for Findable, Accessible, Interoperable, and Reusable, enhancing the value, impact, and sustainability of research data. Adhering to FAIR principles benefits individual researchers and fosters collaboration, data-driven discoveries, knowledge advancement and long-term preservation. However, achieving FAIR compliance is nuanced, with some aspects being more complex, especially concerning metadata standards and controlled vocabularies.\nWe strongly endorse these recommendations for those developing software or performing data analyses: https://fair-software.nl/endorse.\n\n\nBreaking down the FAIR Principles\n\n\n\nFindable\n Research data should be easily identifiable, achieved through the following key components:\n\nAssign Persistent and Unique Identifiers: such as Digital Object Identifiers (DOIs) or Uniform Resource Identifiers (URIs).\nCreate Rich and Standard Metadata: describing the content, context, and characteristics of the data (such as origin, format, version, licensing, and the conditions for reuse).\nUse a Data Repository or Catalog: following FAIR principles to deposit your data, enhancing data discoverability and access from a centralized and reliable source.\n\nClear and comprehensive metadata facilitates data discovery by both researchers and machines through various search engines and data repositories.\n\n\nAccessible\n\n\n\nFAIR accessible\n\n\nResearch data should be accessible with minimal restrictions on access and downloading, to facilitate collaboration, verification of findings, and ensuring transparency. Key elements to follow:\n\nOpen Access: Ensure data is freely accessible without unnecessary barriers. Choose suitable licenses for broad data reuse (such as MIT, Apache-2.0).\nAuthentication and Authorization: Implement secure mechanisms for access control, especially for sensitive data\nMetadata: Deposit metadata even when data access is restricted, providing valuable information about the dataset (version control systems).\n\n\n\n\n\n\n\nImportant note: As open as possible, as closed as necessary\n\n\n\n\n\nEnsure data accessibility aligns with privacy regulations like GDPR, and when necessary, limit access to sensitive data. When dealing with sensitive data, share query details and data sources to maintain transparency.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\nYou are working with sensitive data from a national authority regarding personal information. You won‚Äôt be able to publish or deposit the data but you can describe what was the query you used to obtain the data and its source (e.g., sample population: people between 20-30 years old, smoker).\n\n\n\n\n\n\n\n\n\n\nInteroperable\n\n\n\nFAIR interoperable\n\n\nResearch data ###",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "practical_workflows.html",
    "href": "practical_workflows.html",
    "title": "FAIR computational pipelines and environments",
    "section": "",
    "text": "Course Overview\n\n\n\n\n‚è∞ Total Time Estimation: X hours\n\nüìÅ Supporting Materials:\n\nüë®‚Äçüíª Target Audience: Ph.D., MSc, anyone interested in workflow management systems for High-Throughput data or other related fields within bioinformatics.\nüë©‚Äçüéì Level: Advanced.\nüîí License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\nüí∞ Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268)."
  },
  {
    "objectID": "develop/examples/NGS_OS_FAIR.html",
    "href": "develop/examples/NGS_OS_FAIR.html",
    "title": "Applied Open Science and FAIR principles to NGS",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n1. Apply Open Science and FAIR principles to your data\n\n\nYou should consider revisiting these exercises and examples after completing lesson 1 in the course material.\n\n\n\n\n\n\nExercise 1: FAIR principles in Research practice\n\n\n\n\n\n\n\nThink about your current research project or a hypothetical one. How could you apply Open Science and FAIR principles to improve the transparency, accessibility, and reusability of your research data and/or data analyses? Consider aspects such as data sharing, documentation, metadata standards, and licensing. Write down three specific actions you could take to implement Open Science and FAIR principles into your research workflow.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nExperiments/input/raw data\n\nStandardized file formats and data structure\nStandardized metadata of raw files and results (describing origin, format, etc.)\nDepositing raw data in public repositories or data archives\n\nPipelines/workflows\n\nUse data repositories and document your workflows (software tools, parameters/settings, versions‚Ä¶)\nDocumenting all preprocessing scripts and employed pipelines\nDevelopment environments\nUsing version control systems (git, GitHub, GitLab)\n\nSoftware/code\n\nPipeline scripts and software code openly accessible (commented code)\nUse data repositories (and version control)\nProvide a README file (installation and usage instructions, license and citation information, etc.)\nOpen-source licensing\nFollow these recommendations: FAIR software\n\n\n\n\n\n\n\n\n\n\n\nNote that not all data needs to be archived and deposited. NGS data processing generates vast amounts of data that might not need to be shared publicly, as long as you describe how you produced the data. For example, in a usual bulk RNAseq experiment, FASTQ reads are cleaned and subsequently aligned to a reference genome, creating a subset of cleaned FASTQ files and BAM files. After transcript/gene quantification, you can obtain a final count matrix that can be used for data analysis, such as Differential Expression Analysis. If you provide enough documentation on how these files were processed (which software, which versions, and which options), you won‚Äôt need to deposit either the cleaned or aligned reads, only the original FASTQ files and the final result of your preprocessing. This will save quite the computational resources and metadata needed to preserve the intermediary data. Providing documentation on how the data was generated is much simpler if you are using community-curated pipelines such as the ones created by the nf-core community.\n\nOpen Science\nUnless your data is of sensitive nature (human individual samples, patient data, or anything protected), you should always deposit your data with as few restrictions as possible. This includes publishing your manuscript in Open Access journals as well! For your generated NGS data, our suggestion is that you use a license such as Creative Commons license like CC-BY license, which only requires users to attribute the source of the data, but this also depends on the repository that you use for your data. We will see more about it in the lesson 7\n\n\n\nCC licenses\n\n\n\n\nFAIR principles\nNext, we will see how we can apply each of the FAIR principles to your NGS data.\n\nFindable\nTo make your NGS data easy to find, you should deposit it in a domain-specific repository, such as the Gene Expression Omnibus or Annotare. We will see more about them in lesson 7. Both of these repositories will help you give your data a unique identifier, and provide information (metadata) on how the data was generated. The metadata you include in your submission should contain, at least, the minimum necessary information to understand what kind of data it is submitted, and how it was generated. This includes:\n\nSample metadata in tabular format, containing information about the samples used in the experiment as well as variables of interest for the analysis.\nExperiment metadata, including data provenance, that is, how the samples were obtained, from which organism, following what protocols, kits, sequencing libraries, sequencing method, and data preprocessing workflows/pipelines. This is usually submitted as part of a submission form and it depends on the repository.\nKeywords, such as type of NGS data, conditions or diseases studied in the experiment, organisms used, genes studied, etc.\n\n\n\nAccessible\nBoth GEO and Annotare repositories promote the use of unrestricted access to the data. In the case of Annotare, deposited data is under CC0 license, while GEO states deposited data is public domain. Depositing your data will require you to have an account but it does not require authentication from the user to access and download the data.\n\n\n\nAnnotare main page\n\n\n\n\n\n\n\n\nOn sensitive data\n\n\n\nIf you would like to deposit sensitive data that need controlled access, it is possible to do so through the European Genome-phenome Archive (EGA).\n\n\nIn addition, if you have deposited your data with rich metadata, as explained in the previous step, it will be easier for users to query your data by date, author, organism, type of NGS data, etc etc.\n\n\nInteroperable\nBy using standard bioinformatics formats, such as fastq files for raw NGS data, count matrices in tabular format, BED files for peak calling results, etc., you are already complying to this section. In addition, GEO and Annotare repositories are compliant to NGS data standards, such as MIAME/MINSEQE/MINSCE guidelines.\nNonetheless, this is the easy part! Adhering to controlled vocabularies seems to be the most difficult part of the FAIR principles. Here are some cases:\n\nUsing organism names instead of their taxonomy. For example: mouse instead of Mus musculus, or human, instead of Homo sapiens. Even better, we should use a taxonomy ID, such as the NCBI taxonomy ID for humans NCBITaxon_9606, which will unequivocally refer to humans.\nUsing gene names or symbols instead of gene IDs. For example: the gene POU5F1 has many synonyms, like OCT4, OCT, and OTF4. To be explicit, it is better to reference the gene ID, like an ENSEMBL gene ID ENSG00000204531.\nUsing disease names instead of disease IDs. Again, this will reference specifically the disease you mention.\n\nThere are many more stances where you can use controlled vocabularies for other variables of interest, like cell type, tissue, cell cycle, etc. We will see in the metadata lesson where you can find controlled vocabularies for different variables of interest in NGS data.\n\n\nReusable\nIn order for your NGS data to be reusable, you will have to provide thorough documentation on how it was generated, as well as the terms (that is a license) on how the data can be used/retrieved. We have talked already about collecting metadata on how the samples were generated (laboratory protocols, sequencing library, kits, technology, etc) and processed (workflows or pipelines along with the software used, which versions, and options). We also talked about what type of standard file formats you should use, such as fastq files for raw data and tabular formats for sample metadata. Finally, we have discussed in the Open Science section that you should try to license your data as freely as possible, like a CC0 license or CC-BY license. If your data is of sensitive nature and has restricted access or conditions, you should instead provide information on how other people can access the data, as well as any agreements or ethical approvals necessary for its reuse.\n\n\n\nSources\n\nElixir Belgium: https://rdm.elixir-belgium.org/omics_data\n\n\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "Use cases",
      "NGS data",
      "Applied Open Science and FAIR principles to NGS"
    ]
  },
  {
    "objectID": "develop/02_DMP.html#benefits-from-writing-a-dmp",
    "href": "develop/02_DMP.html#benefits-from-writing-a-dmp",
    "title": "2. Data Management Plan",
    "section": "Benefits from writing a DMP",
    "text": "Benefits from writing a DMP\nA DMP is a living document that serves as the initial step toward achieving FAIR principles in a project.\n\nIt helps identifying gaps in current data management strategies early on, thereby saving time and effort in the long run.\nIt establishes standards for all collaborators to adhere to, ensuring consistency in data organization, naming conventions, metadata standards, and vocabularies.\nEnsures the estimation of costs associated with data production, storage, and management.\nIt clarifies responsibilities regarding the data management process, specifying who is accountable for executing the DMP.\nIt enhances the likelihood of well-managed data, promoting reproducibility and reusability of research results (facilitating the results‚Äô verification and reducing redundancy).\n\nMoreover, writing a DMP is increasingly becoming a requirement by funders and other stakeholders:\n\nTransparency and openness: publicly funded research data must be discoverable, accessible, and reusable by the public\nReturn on investment: well-planned data maximizes the research potential of the data leading to greater returns on public investments in research.",
    "crumbs": [
      "Course material",
      "Basics",
      "2. Data Management Plan"
    ]
  },
  {
    "objectID": "develop/02_NGS_data.html#practical-tips-for-computational-research",
    "href": "develop/02_NGS_data.html#practical-tips-for-computational-research",
    "title": "Next Generation Sequencing Data",
    "section": "",
    "text": "Thoroughly document your datasets and the experimental setup to ensure reproducibility. Adhering to standards will ensure interoperability. Data types‚Äô examples:\n\nElectronic Laboratory Notebook (ELN): digital description of the experimental design, and measurement devices. ELNs offer features like data entry, text editing, file attachments, collaboration tools, and search capabilities.\nLaboratory protocols: methodologies to prepare and manage samples.\nSamples: refers to the biological material (extraction of DNA, RNA, or proteins). Specification of sample identifier, sample type, source organism etc.\nSequencing: details on the platform (e.g., Illumina, Oxford Nanopore), library preparation method, coverage, quality control metrics (e.g., Phred score)‚Ä¶\nRaw sequencing data: sequences and quality scores (e.g., FASTQ files)\n\n\n\n\n\n\n\nNote\n\n\n\nA metadata file is crucial during data analysis as it contains information about the experimental conditions (such as sequencing details, treatment, sample type, time points, tissue‚Ä¶).\n\n\n\n\n\nExamples of data types generated during processing:\n\nQuality control metrics: to filter out potential artifacts and ensure the reliability of downstream analyses (e.g., bioinformatics tool like FastQC or MultiQC for results‚Äô aggregation)\nData alignments: in genomics to determine the location of the read in the genome and in transcriptomics to identify gene expression levels.\nDNA analyses results: such as variant calling, genome annotation, functional genomics, phylogenetics, metagenomics etc. Results are usually presented in tabular format.\nRNA Expression Analyses results: from differential gene expression, gene ontology (GO) enrichment, alternative splicing, pathway analysis etc. Results are usually presented in tabular format.\nEpigenetic profiling outputs: to assess gene regulation and chromatin structure (e.g., ChIP-Seq). Usually presented in BED format.\n\nThe interpretation of NGS data relies heavily on the results of data analysis, which are pivotal for understanding the biological significance of the findings and formulating hypotheses for further exploration. Clear and effective visualization methods are crucial for communicating and interpreting the vast amount of information generated by NGS experiments.\n\n\n\n\n\n\nOther types of data: databases and visualizations\n\n\n\n\n\n\n\n\nKnowledge databases\nA knowledge database is a structured repository of biological information that categorizes and annotates genes, proteins, and their functions, facilitating comprehensive understanding and analysis of biological systems. Here are five examples of knowledge databases:\n\n\nGene Ontology (GO): A comprehensive resource that classifies gene functions into defined terms, allowing for standardized annotation and comparison of genes across different organisms.\nDisease Ontology: A database that provides structured, standardized terminology for various diseases and their relationships, aiding in the systematic analysis of disease-related data.\nKEGG Pathways: A collection of manually curated pathway maps representing molecular interactions and reaction networks within cells, enabling the interpretation of high-throughput data in the context of biological systems.\nReactome: An open-access database that offers curated descriptions of biological processes, including pathways, reactions, and molecular events, facilitating the interpretation of large-scale biological data.\nUniProt: An extensive protein knowledgebase that provides detailed information about proteins, including their sequences, functions, and related annotations, supporting a wide range of biological research endeavors.\n\n\nVisualizations\n\n\nHeatmaps: frequently used to visualize gene expression patterns, epigenetic modifications, or microbial abundances across samples/conditions.\nVolcano Plots: commonly used in differential gene expression analysis\nGenome Browser Snapshots: display alignments and genomic features in genomic regions (e.g., gene annotations, ChIP-Seq peaks)\nNetwork Visualizations:utilized to explore gene regulatory networks or protein-protein interaction\nGenomic Annotations: to annotate genetic variations (functional impact on genes, genomic regions, or regulatory element)\n\n\n\n\n\n\n\n\n\nOur summarized guidelines (don‚Äôt forget to read about FAIR software):\n\nCommenting your code: to enhance readability and comprehension\nMake your source code accessible using repository (GitHub, GitLab, Bitbucket, SourceForge, etc.) that provides version control (VC) solutions. This step is one of the most important ones as version control systems (Git or SVN) track changes in your code over time and enable collaboration and easy version management. Most danish institutions provide courses on Git/GitHub, check yours! We also highly recommend reading this paper (Perez-Riverol et al. 2016).\nREADME file: with the comprehensive information about the project and include installation instructions, usage examples or tutorials, licensing details, citation information, etc.\nRegister your code in a research software registry and include a clear and accessible software usage license: enabling other researcher to discover and reuse software packages (alongside a metadata). More recommendations here.\nUse domain-relevant community standards to ensure consistency and interoperability (e.g., CodeMeta).\n\n\n\n\n\n\n\nGit and Github courses and other resources\n\n\n\n\n\n\n\n\nUniversity of Copenhagen\nAarhus University\nAalborg University\nDTU Git guidelines Find more resources in Berkeley Library website\n\n\n\n\n\n\n\n\n\nYou might use standard workflows or generate new ones during data processing and data analyses steps.\n\nCode notebooks: tools for data documentation (e.g.¬†Jupyter Notebook, Rmarkdown) enabling the combination of code with descriptive text and visualizations.\n\nIntegrated development environments (knitr or MLflow).\nPipeline frameworks or workflow management systems: designed to streamline and automate various steps involved in data analysis (data extraction, transformation, validation, visualization, and more). Additionally, they contribute to ensuring interoperability by facilitating seamless integration and interaction between different components or stages. There are two very popular systems, Nextflow and Snakemake.\n\nA great example of community curated workflows is the nf-core community. Nf-core is a collaborative and open-source initiative comprising bioinformaticians and researchers dedicated to developing and maintaining a collection of curated and reproducible Nextflow-based pipelines for NGS data analysis, ensuring standardized and efficient data processing workflows.\n\n\n\n\n\n\nCourse on pipelines and workflows\n\n\n\nTake our course on Reproducible Research Practices LINK\n\n\n\n\nData types summary\n\n\nSelect appropriate file formats that balance data accessibility, storage efficiency, and compatibility with downstream analysis tools. Standardized file formats facilitates data sharing and collaboration among researchers in the scientific community.\n\nBAM/SAM: stores the alignment information (binary and text-based respectively)\nFASTA: store nucleotide or amino acid sequence, commonly used for reference sequences or assembled contigs.\nGene Transfer Format (GTF) and General Feature Format (GFF): annotates genomic features such as genes, exons, and transcripts.\nAlignment indexes: data structures for efficient and rapid mapping of sequencing reads to a reference.\nVariant Call Format (VCF): stores genetic variation such as single nucleotide variants (SNVs), insertions, deletions, and structural variants (and their position, quality score etc.)\nCount matrix: quantifies the abundance of RNA transcripts or genomic features across samples\nBED/BEDGraph: represent genomic intervals or coverage information (e.g., peak calling identifies regions of enriched signal intensity)\nWIG/BigWig: store genome-wide data\n\nGeneral formats\n\nTabular formats: File formats like CSV, TSV, and XLSX used to store data in rows and columns for easy data analysis and sharing\nImage formats: File formats such as PNG and SVG used to store graphical visualizations, making them easily viewable and shareable\nBinary formats: File formats like NPZ and H5 used to store large datasets, ensuring efficient data access and storage\nJSON: A lightweight data-interchange format for storing hierarchical data structures, commonly used in bioinformatics tools\nHTML: A format used to create interactive reports that include both visualizations and textual descriptions of analysis results\nCode notebooks: Interactive documents combining code, visualizations, and explanatory text, aiding in data analysis reproducibility and documentation\nScripts: Text files containing sets of commands or code instructions for automating data processing and analysis tasks\n\nExplore more data types at the UCSC webpage. Check out this tutorial for more detailed explanations."
  },
  {
    "objectID": "develop/02_NGS_data.html#other-types-of-data",
    "href": "develop/02_NGS_data.html#other-types-of-data",
    "title": "Next Generation Sequencing Data",
    "section": "",
    "text": "These results from NGS data analysis play a crucial role in interpreting the biological significance of the data, enabling researchers to draw meaningful conclusions and generate hypotheses for further investigation. Effective data visualization and clear tabular outputs are essential for effectively communicating and interpreting the wealth of information generated by NGS experiments.\n:::{.callout-note title=‚ÄúBioinformatics visualizations‚Äù - Heatmaps: Heatmaps are graphical representations of data matrices, where each cell‚Äôs color intensity reflects the value of a specific parameter. In NGS data analysis, heatmaps are frequently used to visualize gene expression patterns, epigenetic modifications, or microbial abundances across different samples or conditions. - Volcano Plots: Volcano plots display the relationship between fold change (usually log-fold change) and statistical significance (e.g., p-value) for each gene in a differential analysis. Genes with high significance and large fold changes are often represented further away from the plot‚Äôs center. - Genome Browser Snapshots: Genome browser snapshots display aligned sequencing reads and various genomic features (e.g., gene annotations, ChIP-Seq peaks) in a genomic region of interest. These snapshots provide a visual representation of NGS data aligned to a reference genome. - Network Visualizations: Network visualizations depict the interactions between genes, proteins, or other biological entities, providing insights into complex biological networks. These visualizations can help uncover regulatory relationships or functional modules. - Genomic Annotations: Results may include annotations for genetic variations, such as their functional impact on genes, genomic regions, or regulatory elements. :::\n\nSoftware and code\n\n\n\n\n\n\n\nKnowledge databases\n\n\n\n\n\n\n\nA knowledge database is a structured repository of biological information that categorizes and annotates genes, proteins, and their functions, facilitating comprehensive understanding and analysis of biological systems. Here are five examples of knowledge databases:\n\nGene Ontology (GO): A comprehensive resource that classifies gene functions into defined terms, allowing for standardized annotation and comparison of genes across different organisms.\nDisease Ontology: A database that provides structured, standardized terminology for various diseases and their relationships, aiding in the systematic analysis of disease-related data.\nKEGG Pathways: A collection of manually curated pathway maps representing molecular interactions and reaction networks within cells, enabling the interpretation of high-throughput data in the context of biological systems.\nReactome: An open-access database that offers curated descriptions of biological processes, including pathways, reactions, and molecular events, facilitating the interpretation of large-scale biological data.\nUniProt: An extensive protein knowledgebase that provides detailed information about proteins, including their sequences, functions, and related annotations, supporting a wide range of biological research endeavors.\n\n\n\n\n\n\n\n\nCode notebooks and scripts are essential tools used to analyze Next Generation Sequencing (NGS) data, providing researchers with a documented and reproducible workflow for data processing, and analyses. Common programming languages (and notebooks) are: Python (Jupyter Notebooks), R (Rmarkdown), perl and bash.\n\n\n\nThese are structured sets of interconnected data analysis steps and bioinformatics tools that streamline and automate the transformation of raw sequencing reads into biologically meaningful information, ensuring reproducibility and efficiency in NGS data analysis. Some examples of workflow and pipeline languages are:\n\nNextflow: Nextflow is a popular workflow management system that enables scalable and portable NGS data analysis pipelines, allowing researchers to process data across various computing environments and platforms.\nSnakemake: Snakemake is a workflow management system that uses Python-based scripting to create flexible and automated NGS data analysis pipelines, facilitating parallel processing and easy integration with existing tools.\n\nA great example of community curated workflows is the nf-core community. Nf-core is a collaborative and open-source initiative comprising bioinformaticians and researchers dedicated to developing and maintaining a collection of curated and reproducible Nextflow-based pipelines for NGS data analysis, ensuring standardized and efficient data processing workflows.\n\n\nData types summary\n\n\nSelect appropriate file formats that balance data accessibility, storage efficiency, and compatibility with downstream analysis tools. Standardized file formats facilitates data sharing and collaboration among researchers in the scientific community.\n\nBAM/SAM: stores the alignment information (binary and text-based respectively)\nFASTA: store nucleotide or amino acid sequence, commonly used for reference sequences or assembled contigs.\nGene Transfer Format (GTF) and General Feature Format (GFF): annotates genomic features such as genes, exons, and transcripts.\nAlignment indexes: data structures for efficient and rapid mapping of sequencing reads to a reference.\nVariant Call Format (VCF): stores genetic variation such as single nucleotide variants (SNVs), insertions, deletions, and structural variants (and their position, quality score etc.)\nCount matrix: quantifies the abundance of RNA transcripts or genomic features across samples\nBED/BEDGraph: represent genomic intervals or coverage information (e.g., peak calling identifies regions of enriched signal intensity)\nWIG/BigWig: store genome-wide data\n\nGeneral formats\n\nTabular formats: File formats like CSV, TSV, and XLSX used to store data in rows and columns for easy data analysis and sharing\nImage formats: File formats such as PNG and SVG used to store graphical visualizations, making them easily viewable and shareable\nBinary formats: File formats like NPZ and H5 used to store large datasets, ensuring efficient data access and storage\nJSON: A lightweight data-interchange format for storing hierarchical data structures, commonly used in bioinformatics tools\nHTML: A format used to create interactive reports that include both visualizations and textual descriptions of analysis results\nCode notebooks: Interactive documents combining code, visualizations, and explanatory text, aiding in data analysis reproducibility and documentation\nScripts: Text files containing sets of commands or code instructions for automating data processing and analysis tasks\n\nExplore more data types at the UCSC webpage. Check out this tutorial for more detailed explanations."
  },
  {
    "objectID": "develop/examples/NGS_management.html",
    "href": "develop/examples/NGS_management.html",
    "title": "Effective RDM Practices in NGS Analysis",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nNGS data strategies\nFile naming conventions examples",
    "crumbs": [
      "Use cases",
      "NGS data",
      "Effective RDM Practices in NGS Analysis"
    ]
  },
  {
    "objectID": "develop/examples/NGS_management.html#practical-tips-for-computational-research",
    "href": "develop/examples/NGS_management.html#practical-tips-for-computational-research",
    "title": "Effective RDM Practices in NGS Analysis",
    "section": "Practical tips for computational research",
    "text": "Practical tips for computational research\n\n1. Experiments / raw data\nThoroughly document your datasets and the experimental setup to ensure reproducibility. Adhering to standards will ensure interoperability. Data types‚Äô examples:\n\nElectronic Laboratory Notebook (ELN): digital description of the experimental design, and measurement devices. ELNs offer features like data entry, text editing, file attachments, collaboration tools, and search capabilities.\nLaboratory protocols: methodologies to prepare and manage samples.\nSamples: refers to the biological material (extraction of DNA, RNA, or proteins). Specification of sample identifier, sample type, source organism, etc.\nSequencing: details on the platform (e.g., Illumina, Oxford Nanopore), library preparation method, coverage, quality control metrics (e.g., Phred score)‚Ä¶\nRaw sequencing data: sequences and quality scores (e.g., FASTQ files)\n\n\n\n\n\n\n\nNote\n\n\n\nA metadata file is crucial during data analysis as it contains information about the experimental conditions (such as sequencing details, treatment, sample type, time points, tissue‚Ä¶).\n\n\n\n\n2. Input / Pre- and post-processing data\nExamples of data types generated during processing:\n\nQuality control metrics: to filter out potential artifacts and ensure the reliability of downstream analyses (e.g., bioinformatics tool like FastQC or MultiQC for results‚Äô aggregation)\nData alignments: in genomics to determine the location of the read in the genome and in transcriptomics to identify gene expression levels.\nDNA analysis results: such as variant calling, genome annotation, functional genomics, phylogenetics, metagenomics, etc. Results are usually presented in tabular format.\nRNA Expression analysis results: from differential gene expression, gene ontology (GO) enrichment, alternative splicing, pathway analysis, etc. Results are usually presented in tabular format.\nEpigenetic profiling outputs: to assess gene regulation and chromatin structure (e.g., ChIP-Seq). Usually presented in BED format.\n\nThe interpretation of NGS data relies heavily on the results of data analysis, which are pivotal for understanding the biological significance of the findings and formulating hypotheses for further exploration. Clear and effective visualization methods are crucial for communicating and interpreting the vast amount of information generated by NGS experiments.\n\n\n\n\n\n\nOther types of data: databases and visualizations\n\n\n\n\n\n\n\n\nKnowledge databases\nA knowledge database is a structured repository of biological information that categorizes and annotates genes, proteins, and their functions, facilitating comprehensive understanding and analysis of biological systems. Here are five examples of knowledge databases:\n\n\nGene Ontology (GO): A comprehensive resource that classifies gene functions into defined terms, allowing for standardized annotation and comparison of genes across different organisms.\nDisease Ontology: A database that provides structured, standardized terminology for various diseases and their relationships, aiding in the systematic analysis of disease-related data.\nKEGG Pathways: A collection of manually curated pathway maps representing molecular interactions and reaction networks within cells, enabling the interpretation of high-throughput data in the context of biological systems.\nReactome: An open-access database that offers curated descriptions of biological processes, including pathways, reactions, and molecular events, facilitating the interpretation of large-scale biological data.\nUniProt: An extensive protein knowledgebase that provides detailed information about proteins, including their sequences, functions, and related annotations, supporting a wide range of biological research endeavors.\n\n\nVisualizations\n\n\nHeatmaps: frequently used to visualize gene expression patterns, epigenetic modifications, or microbial abundances across samples/conditions.\nVolcano Plots: commonly used in differential gene expression analysis\nGenome Browser Snapshots: display alignments and genomic features in genomic regions (e.g., gene annotations, ChIP-Seq peaks)\nNetwork Visualizations:utilized to explore gene regulatory networks or protein-protein interaction\nGenomic Annotations: to annotate genetic variations (functional impact on genes, genomic regions, or regulatory element)\n\n\n\n\n\n\n\n\n3. Software and code:\nBest practices for software and code management (don‚Äôt forget to read about FAIR software):\n\nCommenting your code: to enhance readability and comprehension\nMake your source code accessible using a repository (GitHub, GitLab, Bitbucket, SourceForge, etc.) that provides version control (VC) solutions. This step is one of the most important ones as version control systems (Git or SVN) track changes in your code over time and enable collaboration and easy version management. Most Danish institutions provide courses on Git/GitHub, check yours! We also highly recommend reading this paper (Perez-Riverol et al. 2016).\nREADME file: with comprehensive information about the project including installation instructions, usage examples or tutorials, licensing details, citation information, etc.\nRegister your code in a research software registry and include a clear and accessible software usage license: enabling other researchers to discover and reuse software packages (alongside metadata). More recommendations here.\nUse domain-relevant community standards to ensure consistency and interoperability (e.g., CodeMeta).\n\n\n\n\n\n\n\nGit and Github courses and other resources\n\n\n\n\n\n\n\n\nUniversity of Copenhagen\nAarhus University\nAalborg University\nDTU Git guidelines Find more resources on the Berkeley Library website\n\n\n\n\n\n\n\n\n4. Pipelines and workflows\nYou might use standard workflows or generate new ones during data processing and data analysis steps.\n\nCode notebooks: tools for data documentation (e.g.¬†Jupyter Notebook, Rmarkdown) enabling the combination of code with descriptive text and visualizations.\n\nIntegrated development environments (knitr or MLflow).\nPipeline frameworks or workflow management systems: designed to streamline and automate various steps involved in data analysis (data extraction, transformation, validation, visualization, and more). Additionally, they contribute to ensuring interoperability by facilitating seamless integration and interaction between different components or stages. There are two very popular systems, Nextflow and Snakemake.\n\nA great example of community-curated workflows is the nf-core community. Nf-core is a collaborative and open-source initiative comprising bioinformaticians and researchers dedicated to developing and maintaining a collection of curated and reproducible Nextflow-based pipelines for NGS data analysis, ensuring standardized and efficient data processing workflows.\n\n\n\n\n\n\nCourse on pipelines and workflows\n\n\n\nTake our course on Reproducible Research Practices LINK",
    "crumbs": [
      "Use cases",
      "NGS data",
      "Effective RDM Practices in NGS Analysis"
    ]
  },
  {
    "objectID": "develop/examples/NGS_management.html#wrap-up",
    "href": "develop/examples/NGS_management.html#wrap-up",
    "title": "Effective RDM Practices in NGS Analysis",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we have taken a look a the vast and diverse landscape of bioinformatics data.",
    "crumbs": [
      "Use cases",
      "NGS data",
      "Effective RDM Practices in NGS Analysis"
    ]
  },
  {
    "objectID": "develop/06_pipelines.html",
    "href": "develop/06_pipelines.html",
    "title": "6. Processing and analyzing biodata",
    "section": "",
    "text": "In this section, we explore essential elements of reproducibility and efficiency in computational research, highlighting techniques and tools for creating robust and transparent coding and workflows. By prioritizing reproducibility and replicability, researchers can enhance the credibility and impact of their findings while fostering collaboration and knowledge dissemination within the scientific community.\n\n\n\n\n\n\nBefore you start‚Ä¶\n\n\n\n\nChoose a folder structure (e.g., using cookiecutter)\nChoose a file naming system\nAdd a README describing the project (and the naming conventions)\nInstall and set up version control (e.g., Git and Github)\nChoose a coding style!\n\n\nPython: Python‚Äôs PEP or Google‚Äôs style guide\nR: Google‚Äôs style guide or Tidyverse‚Äôs style guide\n\n\n\n\n\nThrough techniques such as scripting, containerization (e.g., Docker), and virtual environments, researchers can create reproducible analyses that enable others to validate and build upon their work. Emphasizing the documentation of data processing steps, parameters, and results ensures transparency and accountability in research outputs. To write clear and reproducible code, take the following approach: write functions, code defensively (such as input validation, error handling, etc.), add comments, conduct testing, and maintain proper documentation.\nTools for reproducibility:\n\nCode notebooks: Utilize tools like Jupyter Notebook and R Markdown to combine code with descriptive text and visualizations, enhancing data documentation.\n\nIntegrated development environments: Consider using platforms such as (knitr or MLflow) to streamline code development and documentation processes.\nPipeline frameworks or workflow management systems: Implement systems like Nextflow and Snakemake to automate data analysis steps (including data extraction, transformation, validation, visualization, and more). Additionally, they contribute to ensuring interoperability by facilitating seamless integration and interaction between different components or stages.\n\n\n\nComputational notebooks (e.g., Jupyter, R Markdown) provide researchers with a versatile platform for exploratory and interactive data analysis. These notebooks facilitate sharing insights with collaborators and documentation of analysis procedures.\n\n\n\nTools such as Nextflow and Snakemake streamline and automate various data analysis steps, enabling parallel processing and seamless integration with existing tools. Remember to create portable code and use relative paths to ensure transferability between users.\n\nNextflow: offers scalable and portable NGS data analysis pipelines, facilitating data processing across diverse computing environments.\nSnakemake: Utilizing Python-based scripting, Snakemake allows for flexible and automated NGS data analysis pipelines, supporting parallel processing and integration with other tools.\n\nOnce your scientific computational workflow is ready to be shared, publish your scientific computational workflow on WorkflowHub.\n\n\n\nEach computer or HPC (High-Performance Computing) platform has a unique computational environment that includes its operating system, installed software, versions of software packages, and other features. If a research project is moved to a different computer or platform, the analysis might not run or produce consistent results if it depends on any of these factors.\nFor research to be reproducible, the original computational environment must be recorded so others can replicate it. There are several methods to achieve this:\n\nContainerization platforms (e.g., Docker, Singularity): allow the researcher to package their software and dependencies into a standardized container image.\nVirtual Machines (e.g., VirtualBox): can share an entire virtualized computing environment (OS, software and dependencies)\nEnvironment managers: provide an isolated environment with specific packages and dependencies that can be installed without affecting the system-wide configuration. These environments are particularly useful for managing conflicting dependencies and ensuring reproducibility. Configuration files can automate the setup of the computational environment:\n\nconda: allows users to export environment specifications (software and dependencies) to YAML files enabling easy recreation of the environment on another system\nPython virtualenv: is a tool for creating isolated environments to manage dependencies specific to a project\nrequirements.txt: may contain commands for installing packages (such as pip for Python packages or apt-get for system-level dependencies), configuring system settings, and setting environment variables. Package managers can be used to install, upgrade and manage packages.\nR‚Äôs renv: The ‚Äòrenv‚Äô package creates isolated environments in R.\n\nEnvironment descriptors\n\nsessionInfo() or devtools::session_info(): In R, these functions provide detailed information about the current session\nsessionInfo(), similarly, in Python. Libraries like NumPy and Pandas have show_versions() methods to display package versions.\n\n\nWhile environment managers are very easy to use and share across different systems, and are lightweight and efficient, offering fast start-up times, Docker containers provide a full env isolation (including the operating system) which ensures consistent behavior across different systems.\n\n\n\n\nTo maintain clarity and organization in the data analysis process, adopt best practices such as:\n\nData documentation: create a README.md file to provide an overview of the project and its structure, and metadata for understanding the context of your analysis.\nAnnotate your pipelines and comment your code (look for tutorials and templates such as this one from freeCodeCamp).\nUse coding style guides (code lay-out, whitespace in expressions, comments, naming conventions, annotations‚Ä¶) to maintain consistency.\nLabel files numerically to organize the entire data analysis process (scripts, notebooks, pipelines, etc.).\n\n00.preprocessing., 01.data_analysis_step1., etc.\n\nProvide environment files for reproducing the computational environment (such as ‚Äòrequirements.txt‚Äô for Python or ‚Äòenvironment.yml‚Äô for Conda). The simplest way is to document the dependencies by reporting the packages and their versions used to run your analysis.\nData versioning: use version control systems (e.g., Git) and upload your code to a code repository Lesson 5.\nIntegrated development environments (e.g., RStudio, PyCharm) offer tools and features for writing, testing, and debugging code\nUse git submodule for code and software that is reused in several projects\nLeverage curated pipelines such as the ones developed by the nf-core community, further ensuring adherence to community standards and guidelines.\nUse Software Heritage an archive for software source code are essential for long-term accessibility and reproducibility\nAdd a LICENSE file and perform regular updates: clarifying usage permissions and facilitating collaboration.\n\n\n\n\n\n\n\nPractical HPC pipes\n\n\n\nWe provide a hand-on workshop on computational environments and pipelines. Keep an eye on the upcoming events on the Sandbox website. If you‚Äôre interested in delving deeper, check out the HPC best practices module we‚Äôve developed here.",
    "crumbs": [
      "Course material",
      "Key practices",
      "6. Processing and analyzing biodata"
    ]
  },
  {
    "objectID": "develop/03_DOD.html",
    "href": "develop/03_DOD.html",
    "title": "3. Data organization and storage",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nOrganize your data and external resources efficiently\nApply naming conventions for files and folders\nDefine rules for naming results and figures accurately\nSo far, we have covered how to adhere to FAIR and Open Science standards, which primarily focus on data sharing post-project completion. However, effective data management is essential while actively working on the project. Organizing data folders, raw and processed data, analysis scripts and pipelines, and results ensure long-term project success. Without a clear structure, future access and understanding of data become challenging, even more so for collaborators, leading to potential chaos down the line.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#folder-organization",
    "href": "develop/03_DOD.html#folder-organization",
    "title": "3. Data organization and storage",
    "section": "Folder organization",
    "text": "Folder organization\nHere we suggest the use of three main folders:\n\nShared project data folders:\n\n\nThis shared directory is designated for storing unprocessed sequencing data files, with each subfolder representing a separate project.\nEach project folder contains raw data, corresponding metadata, and optionally pre-processed data like quality control reports and processed data.\n\nInclude the pipeline or workflow used for data processing, along with a metadata file.\n\nData in these folders should be locked and set to read-only to prevent unauthorized (‚Äúunwanted‚Äù) modifications.\n\n\nIndividual project folders:\n\n\nThis directory typically belongs to the researcher conducting bioinformatics analyses and encompasses all essential files for a specific research project (data, scripts, software, workflows, results, etc.).\nA project may utilize data from various assays or results obtained from other projects. It‚Äôs important to avoid duplicating datasets; instead, link them from the original source to maintain data integrity and avoid redundancy.\n\n\nResources and databases folders:\n\n\nThis (commonly) shared directory contains common repositories or curated databases that facilitate research (genomics, clinical data, imaging data, and more!). For instance, in genomics, it includes genome references (fasta files), annotations (gtf files) for different species, and indexes for various alignment algorithms.\nEach folder corresponds to a unique reference or database version, allowing for multiple references from the same organism or different species.\n\nEnsure each contains the version of the reference and a metadata file.\nMore subfolders can be created for different data formats.\n\n\n\n\n\n\n\n\nVerify the integrity of downloaded files!\n\n\n\nEnsure that the person downloading the files employs checksums (MD5, SHA1, SHA256) or cryptographic hash functions to verify the integrity and ascertain that files are neither corrupted nor tampered with.\n\nMD5 Checksum: Files with names ending in ‚Äú.md5‚Äù contain MD5 checksums. For instance, ‚Äúfilename.txt.md5‚Äù holds the MD5 checksum of ‚Äúfilename.txt‚Äù.‚Äù\n\n\n\n\n\n\n\n\n\nDatabase\n\n\n\n\n\n\n\nA database is a structured repository for storing, managing, and retrieving information, forming the cornerstone of efficient data organization.\n\n\n\n\n\n\n\n\n\n\n\nCreate shortcuts to public datasets and assays!\n\n\n\nThe use of symbolic links, also referred to as softlinks, is a key practice in large labs where data might used for different purposes and by multiple people.\n\nThey act as pointers, containing the path to the location of the target files/directories.\nThey avoid duplication and they are flexible and lightweight (do not occupy much disk space).\nThey simplify directory structures.\n\nExtra use case: create symbolic links to executable files and libraries!\n\n\n\n\n\n\n\n\n\nExercise: create a softlink link\n\n\n\n\n\n\n\nOpen your terminal and create a softlink using the following command. The first path is the target (directory or file) and the second one is where the symbolic link will be created.\nln -s path/to/dataset/&lt;ASSAY_ID&gt; /path/to/user/&lt;PROJECT_ID&gt;/data/\nNow, access the target file/directory through the symbolic link:\nls /path/to/user/&lt;PROJECT_ID&gt;/data/\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nFollow this example if need extra guidance (change paths!):\n\nCreate the target/original file\n\necho \"This is the content of the original file.\" &gt; /home/users/Documents/original_file.txt\n\nCreate the symbolic link\n\nln -s /home/users/Documents/original_file.txt /home/users/Desktop/original_file.txt\n\nVerify the symbolic link\n\nls -s /home/users/Desktop/original_file.txt\n\nAccess the file through the symbolic link:\n\ncat /home/users/Desktop/original_file.txt\nThe last command will display the contents of the original file.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#assay-folder",
    "href": "develop/03_DOD.html#assay-folder",
    "title": "Best Practices for Data Storage",
    "section": "Assay folder",
    "text": "Assay folder\nFor each NGS experiment there should be an Assay folder that will contain all experimental datasets, that is, an Assay (raw files and pipeline processed files). Raw files should not be modified at all, but you should probably lock modifications to the final results once you are done with preprocessing the data. This will help you prevent unwanted modifications to the data. Each Assay subfolder should be named in a way that it is unique, easily readable, distinguishable and understood at a glance. For example, you could name an NGS assay using an acronym for the type of NGS assay (RNAseq, ChIPseq, ATACseq), a keyword that represents a unique descriptive element of that assay, and the date. Like this:\n&lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD\nFor example CHIP_Oct4_20230101 is a ChIPseq assay made on 1st January 2023 with the keyword Oct4, so it is easily identifiable by eye.\n\nAssay ID code names\nBelow is a example list of different Assay-ID code names. You are welcome to use it and expand it as you wish!\n\nCHIP: ChIP-seq\nRNA: RNA-seq\nATAC: ATAC-seq\nSCR: scRNA-seq\nPROT: Mass Spectrometry Assay\nCAT: Cut&Tag\nCAR: Cut&Run\nRIME: Rapid Immunoprecipitation Mass spectrometry of Endogenous proteins\n‚Ä¶\n\n\n\nAssay folder structure\nNext, let‚Äôs take a look at a possible folder structure and what kind of files you can find there.\nCHIP_Oct4_20230101/\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ metadata.yml\n‚îú‚îÄ‚îÄ pipeline.md\n‚îú‚îÄ‚îÄ processed\n‚îî‚îÄ‚îÄ raw\n   ‚îú‚îÄ‚îÄ .fastq.gz\n   ‚îî‚îÄ‚îÄ samplesheet.csv\n\nREADME.md: Long description of the assay in markdown format. It should contain provenance of the raw NGS data (samples, laboratory protocols used, aim of the assay, etc)\nmetadata.yml: metadata file for the assay describing different keys and important information regarding that assay (see this lesson).\npipeline.md: description of the pipeline used to process raw data, as well as the commands used to run the pipeline.\nprocessed: folder with results of the preprocessing pipeline. Contents depend on the pipeline used.\nraw: folder with the raw data.\n\n.fastq.gz:In the case of NGS assays, there should be fastq files.\nsamplesheet.csv: file that contains metadata information for the samples. This file is used to run the nf-core pipelines. You can also add extra columns with info regarding the experimental variables and batches so it can be used for downstream analysis as well.\n\n\nThis folder structure is simple and straight forward, and files will be exactly where you expect them to be! By including a description and metadata file, you will always be able to understand what this experiment was and how it was generated. This information will be specially useful once you need to submit your data for archiving or write a manuscript. The metadata file will be really handy if you want to collect information on all your NGS datasets and create a database (see this lesson)!\nNote that the processed folder is not expanded upon. This folder will be very dependant on the workflows/pipelines that you use. We strongly recommend that you use reproducible and community curated pipelines, such as the ones developed by the nf-core community, which have a through documentation on the results they produce. For example, imagine you have run an RNAseq experiment and processed the data using the nf-core:rnaseq pipeline with the pseudoalignment and quantification option:\nprocessed/\n‚îú‚îÄ‚îÄ fastqc/\n‚îú‚îÄ‚îÄ multiqc/\n‚îú‚îÄ‚îÄ pipeline_info/\n‚îú‚îÄ‚îÄ salmon/\n‚îî‚îÄ‚îÄ trimgalore/\n\nfastqc: Quality Control results of the raw fastq files.\nmultiqc: Full compilation of the Quality Control checks for all your samples for the entire pipeline.\npipeline_info: Information and logs about the pipeline used for each of the steps of the workflow.\nsalmon: Pseudoalignment and quantification results of the salmon algorithm.\ntrimgalore: Cleaned fastq files and Quality Control results of the cleaned files.\n\nBy using a standardized pipeline, you should be able to always find the preprocessing results where you expect them to be! No need to come up with your own file organization (and document it) or worry that your colleagues (or your future self) won‚Äôt be able to find the data."
  },
  {
    "objectID": "develop/03_DOD.html#project-folder",
    "href": "develop/03_DOD.html#project-folder",
    "title": "3. Best Practices for Data Storage",
    "section": "Project folder",
    "text": "Project folder\nOn the other hand, we have the other type of folder called Projects. In this folder you will save a subfolder for each project that you (or your lab) works on. Each Project subfolder will contain project information and all the data analysis notebooks and scripts used in that project.\nProjects and Assays are separated from each other because a project may use one or more assays to answer a scientific question, and assays may be reused several times in different projects. This could be, for example, all the data analysis related to a publication (a RNAseq and a ChIPseq experiment), or a comparison between a previous ATACseq experiment (which was used for a older project) with a new laboratory protocol.\nAs like for an Assay folder, the Project folder should be named in a way that it is unique, easily readable, distinguishable and understood at a glance. For example, you could name it after the main author initials, a keyword that represents a unique descriptive element of that assay, and the date:\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\nFor example, JARH_Oct4_20230101, is a project about the gene Oct4 owned by Jose Alejandro Romero Herrera, created on the 1st of January of 2023.\n\nProject folder structure\nNext, let‚Äôs take a look at a possible folder structure and what kind of files you can find there.\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n‚îú‚îÄ‚îÄ data\n‚îÇ  ‚îî‚îÄ‚îÄ &lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD/\n‚îú‚îÄ‚îÄ documents\n‚îÇ  ‚îî‚îÄ‚îÄ Non-sensitive_NGS_research_project_template.docx\n‚îú‚îÄ‚îÄ notebooks\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis.rmd\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ reports\n‚îÇ  ‚îú‚îÄ‚îÄ figures\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis/\n‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ heatmap_sampleCor_20230102.png\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis.html\n‚îú‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ results\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis/\n‚îÇ      ‚îî‚îÄ‚îÄ DEA_treat-control_LFC1_p01.tsv\n‚îú‚îÄ‚îÄ scripts\n‚îú‚îÄ‚îÄ results\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis/\n‚îÇ      ‚îî‚îÄ‚îÄ DEA_treat-control_LFC1_p01.tsv\n‚îî‚îÄ‚îÄ metadata.yml\n\ndata: folder that contains symlinks or shortcuts to where the data is, avoiding copying and modification of original files.\ndocuments: folder containing word documents, slides or pdfs related to the project, such as explanations of the data or project, papers, etc. It also contains your Data Management Plan.\n\nNon-sensitive_NGS_research_project_template.docx. This is a pre-filled Data Management Plan based on the Horizon Europe guidelines.\n\nnotebooks: folder containing Jupyter, R markdown or Quarto notebooks with the actual data analysis.\npipelines: folder containing snakemake or Nextflow pipelines. Tip: labeled them numerically indicating the sequential order\nREADME.md: detailed description of the project in markdown format.\nreports: notebooks rendered as html/docx/pdf versions, ideal for sharing with colleagues and also as a formal report of the data analysis procedure.\n\nfigures: figures produced upon rendering notebooks. The figures will be saved under a subfolder named after the notebook that created them. This is for provenance purposes so we know which notebook created which figures.\n\nrequirements.txt: file explaining what software and libraries/packages and their versions are necessary to reproduce the code.\nresults: results from the data analysis, such as tables with differentially expressed genes, enrichment results, etc.\nscripts: folder containing helper scripts needed to run data analysis or reproduce the work of the folder\ndescription.yml: short description of the project.\nmetadata.yml: metadata file for the assay describing different keys (see this lesson).\n\n\n\n\n\n\n\nNote on the results folder\n\n\n\nResults from your code notebooks should be saved under this folder. Create a subfolder named after the notebook that created them, so you can always identify which notebook created which results!",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Best Practices for Data Storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#template-engine",
    "href": "develop/03_DOD.html#template-engine",
    "title": "3. Data organization and storage",
    "section": "Template engine",
    "text": "Template engine\nSetting up folder structures manually for each new project can be time-consuming. Thankfully, tools like Cookiecutter offer a solution by allowing users to create project templates easily. These templates can ensure consistency across projects and save time. Additionally, using cruft alongside Cookiecutter can assist in maintaining older templates when updates are made (by synchronizing them with the latest version).\n\n\n\n\n\n\nCookiecutter templates\n\n\n\n\nSandbox Project/Data analysis template\nSandbox Data/Assay template\nCookiecutter template for Data science projects\nBrickmanlab template for NGS data: similar to the folder structures in the examples above. You can download and modify it to suit your needs.\n\n\n\n\nQuick tutorial on cookiecutter\n\n\n\n\n\n\nSandbox Tutorial\n\n\n\nLearn how to create your own template here.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#genomic-resources-folder",
    "href": "develop/03_DOD.html#genomic-resources-folder",
    "title": "3. Best Practices for Data Storage",
    "section": "Genomic resources folder",
    "text": "Genomic resources folder\nPreprocessing NGS data usually requires different genomic resources in order to align and and annotate fastq files. First and most importantly, you will need reference genomes (human and mouse are the most common ones) in FASTA format. In addition, aligner tools such as STAR, Bowtie, etc., require indexed fasta files to perform alignment of reads. Moreover, GTF or GFF files are necessary in order to quantify reads into genomic regions (such as genes or promoters). Nonetheless, these genomic resources are often updated and are released periodically under different versions from different sources. In order to make your data reproducible, you will need to control and manage these genomic resources and specify their versions and sources. For example, the latest mouse reference genome is GRCm39, but may studies still align their reads to the version GRCm38.\nHow do you keep track of your resources? You could, again, set up a folder structure that hosts reference genomes, annotations and indexes per species and per version, or use a reference genome manager such as refgenie. We will take a look at both options.\n\nRefGenie\nRefgenie manages storage, access, and transfer of reference genome resources. It provides command-line and Python interfaces to download pre-built reference genome ‚Äúassets‚Äù, like indexes used by bioinformatics tools. It can also build assets for custom genome assemblies. Refgenie provides programmatic access to a standard genome folder structure, so software can swap from one genome to another.\nCheck their tutorial if you want to start managing your genomic resources with Refgenie!\n\n\nManual download\nIf you want to keep manual track and manage yourself and not depend on a third party tool, you can always download the genomic resources directly from the source. We will show you how in this section:\n\nCreate a new folder, called genomic_resources.\nInside this folder, separate each species with a subfolder.\nInside the species subfolder, separate it by the resource version\nInside the version subfolder, separate genomic sequences (FASTA) and annotations (GTF/GFF) from different alignment indexes\n\nIn the example below, we are downloading human (version GRCh38) and mouse (version GRCm39) genomic resources from the ensembl FTP server\ngenomic_resources/\n‚îú‚îÄ‚îÄ homo_sapiens/\n‚îÇ  ‚îî‚îÄ‚îÄ GRCh38/\n‚îÇ     ‚îú‚îÄ‚îÄ Homo_sapiens.GRCh38.109.gtf.gz\n‚îÇ     ‚îú‚îÄ‚îÄ Homo_sapiens.GRCh38.109.dna_sm.primary_assembly.fa.gz\n‚îÇ     ‚îî‚îÄ‚îÄ indexes/\n‚îÇ        ‚îú‚îÄ‚îÄ salmon/\n‚îÇ        ‚îî‚îÄ‚îÄ STAR/\n‚îú‚îÄ‚îÄ mus_musculus/\n‚îÇ  ‚îî‚îÄ‚îÄ GRCm39/\n‚îÇ     ‚îú‚îÄ‚îÄ Mus_musculus.GRCm39.109.gtf.gz\n‚îÇ     ‚îú‚îÄ‚îÄ Mus_musculus.GRCm39.109.dna_sm.primary_assembly.fa.gz\n‚îÇ     ‚îî‚îÄ‚îÄ indexes/\n‚îÇ        ‚îú‚îÄ‚îÄ salmon/\n‚îÇ        ‚îî‚îÄ‚îÄ STAR/\n‚îú‚îÄ‚îÄ ref_genomes.sh\n‚îî‚îÄ‚îÄ create_indexes.sh\nAs you can see, we have also keep here a bash script that downloads and sets up the folders. The contents of the bash script should also indicate a date of when was the data downloaded. For example:\n#!/bin/bash\n\n# Download human (GRCh38) and mouse (GRCm39) genome and annotations\n# Release 109\n# 2023-07-10\n\n# Go to home\ncd\n# Create homo_sapiens version folder and go to that folder\nmkdir -p genomic_resources/homo_sapiens/GRCh38\ncd  genomic_resources/homo_sapiens/GRCh38\n\n# Download FASTA file and GTF file from the ensembl release 109\nwget -L ftp://ftp.ensembl.org/pub/release-109/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna_sm.primary_assembly.fa.gz\nwget -L ftp://ftp.ensembl.org/pub/release-109/gtf/homo_sapiens/Homo_sapiens.GRCh38.109.gtf.gz\n\n# Go to home\ncd\n# Create mus_musculus version folder and go to that folder\nmkdir -p genomic_resources/mouse/GRCm39\ncd  genomic_resources/mouse/GRCm39\n\n# Download FASTA file and GTF file from the ensembl release 109\nwget -L ftp://ftp.ensembl.org/pub/release-109/fasta/mus_musculus/dna/Mus_musculus.GRCm39.dna_sm.primary_assembly.fa.gz\nwget -L https://ftp.ensembl.org/pub/release-109/gtf/mus_musculus/Mus_musculus.GRCm39.109.gtf.gz\nAfter that is all set up, you can run another bash script that will create a index folder inside each version subfolder and generate indexes for different alignment tools!",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Best Practices for Data Storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#naming-conventions",
    "href": "develop/03_DOD.html#naming-conventions",
    "title": "3. Data organization and storage",
    "section": "Naming conventions",
    "text": "Naming conventions\nConsistent naming conventions play a crucial role in scientific research by enhancing organization and data retrieval. By adopting standardized naming conventions, researchers ensure that files, experiments, or datasets are labeled logically, facilitating easy location and comparison of similar data. The importance of uniform naming conventions extends to various fields, in fields like genomics or health data science, uniform naming conventions for files associated with particular experiments or samples allow for swift identification and comparison of relevant data, streamlining the research process and contributing to the reproducibility of findings. Overall, promotes efficiency, collaboration, and the integrity of scientific work.\n\n\n\n\n\n\nGeneral tips for file and folder naming\n\n\n\nRemember to keep the folder structure simple.\n\nKeep it short and meaningful (use understandable abbreviation only, e.g., Cor for correlations or LFC for Log Fold Change)\nConsider including one of these elements: project name, category, descriptor, content, author‚Ä¶\n\nAuthor-based: use initials\n\nUse alphanumeric characters: letters (A-Z) and numbers (0-9)\nAvoid special characters: ~!@#$%^&*()`‚Äú|\nDate-based format: use YYYYMMDD format (year/month/day format helps with sorting and listing files in chronological order)\nUse underscores and hyphens as delimiters and avoid spaces.\n\nNot all search tools may work well with spaces (messy to indicate paths)\nIf the length is a concern, use capital letters to delimit words camelCase.\n\nSequential numbering: Use a two-‚Äëdigit format for single-digit numbers (0‚Äì9) to ensure correct numerical sequence order (for example, 01 and not, 1 if your sequence only goes up to 99)\nVersion control: Indicate the version (‚ÄúV‚Äù) or revision (‚ÄúR‚Äù) as the last element, using the two-digit format (e.g., v01, v02)\nWrite down your naming convention pattern and document it in the README file\n\n\n\n\n\n\n\n\n\nCreate your own naming conventions\n\n\n\n\n\n\n\nConsider the most common types of files and folders you will be working with, such as visualizations, results tables, and processed files. Develop a logical and clear naming system for these files based on the tips provided above. Aim for concise and straightforward names to avoid complexity.\n\n\n\n\n\n\n\n\n\n\n\nWhich naming conventions should not be used and why?\n\n\n\n\n\n\n\nA. data_processing_carlo's.py\nB. raw_sequences_V#20241111.fasta\nC. differential_expression_results_clara.csv\nD. Grant proposal final.doc\nE. sequence_alignment$v1.py\nF. data/gene_annotations_20201107.gff\nG. alpha~1.0/beta~2.0/reg_2024-05-98.tsv\nH. alpha=1.0/beta=2.0/reg_2024-05-98.tsv\nI. run_pipeline:20241203.sh\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nA, B, D, E, H, I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich file name is more readable?\n\n\n\n\n\n\n\n1a. forecast2000122420240724.tsv\n1b. forecast_2000-12-24_2024-07-24.tsv\n1c. forecast_2000_12_24_2024_07_24.tsv\n2a. 01_data_preprocessing.R\n2b. 1_data_preProcessing.R\n2c. 01_d4t4_pr3processing.R\n3a. B1_2024-12-12_cond~pH7_temp~37C.fastq\n3b. B1.20241212.pH7.37C.fastq\n3c. b1_2024-12-12_c0nd~pH7_t3mp~37C.fastq\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n1b: easier for human & machine, _ separates dates, - separates within time information (year/month/day). This is important, for example, when using wildcards in Snakemake for building pipelines.\n2a: start with 0 for sorting, consistently with upper/lower and the use of separators (_ separates metadata)\n3a: indicates variable temperature is set to 37 Celsius (temperature could be negative - and is better used to separate values in time)\n\n\n\n\n\n\n\n\n\n\nRegular expressions are an incredibly powerful tool for string manipulation. We recommend checking out RegexOne to learn how to create smart file names that will help you parse them more efficiently. To learn more about naming conventions for NGS analysis and see additional examples, click here.\n\n\n\n\n\n\nWhich of the following regexps match the following filenames?\n\n\n\n\n\n\n\n(in bold filenames that SHOULD be matched):\n\nrna_seq/2021/03/results/Sample_A123_gene_expression.tsv\nproteomics/2020/11/Sample_B234_protein_abundance.tsv\nrna_seq/2021/03/results/Sample_C345_normalized_counts.tsv\nrna_seq/2021/03/results/Sample_D456_quality_report.log\nmetabolomics/2019/05/Sample_E567_metabolite_levels.tsv\nrna_seq/2019/12/Sample_F678_raw_reads.fastq\nrna_seq/2021/03/results/Sample_G789_transcript_counts.tsv\nproteomics/2021/02/Sample_H890_protein_quantification.TSV\n\nRegular Expressions:\nrna_seq.*\\.tsv\n.*\\.csv\n.*/2021/03/.*\\.tsv\n.*Sample_.*_gene_expression.tsv\nrna_seq/2021/03/results/Sample_.*_.*\\.tsv\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n.*rna_seq.*\\.tsv and rna_seq/2021/03/results/Sample_.*_.*\\.tsv match the exact same files",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#wrap-up",
    "href": "develop/03_DOD.html#wrap-up",
    "title": "3. Data organization and storage",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we have learned some practical tips and examples about how to organize your data and bring some order to chaos! It is now your responsibility to use and implement them in a reasonable way. Complete the practical tutorial on using cookiecutter as a template engine to be able to create your own templates and reuse them as much as you need.\n\nSources\n\nThe Turing way https://the-turing-way.netlify.app/project-design/project-repo/project-repo-advanced.html\nRDMkit Elixir Europe: https://rdmkit.elixir-europe.org/data_organisation\nCoderefinery: https://coderefinery.github.io/reproducible-research/organizing-projects/#directory-structure-for-projects\nUK Data Service: https://ukdataservice.ac.uk/learning-hub/research-data-management/format-your-data/organising/\nOakland University: https://library.oakland.edu/services/research-data/file-org.html\nCessda guidelines: https://dmeg.cessda.eu/Data-Management-Expert-Guide/2.-Organise-Document/File-naming-and-folder-structure.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#shared-project-data-folders-assays-datasets",
    "href": "develop/03_DOD.html#shared-project-data-folders-assays-datasets",
    "title": "3. Best Practices for Data Storage",
    "section": "Shared project data folders (assays, datasets)",
    "text": "Shared project data folders (assays, datasets)\nFor each NGS experiment there should be an Assay folder that will contain all experimental datasets, that is, an Assay (raw files and pipeline processed files). Raw files should not be modified at all, but you should probably lock modifications to the final results once you are done with preprocessing the data. This will help you prevent unwanted modifications to the data. Each Assay subfolder should be named in a way that it is unique, easily readable, distinguishable and understood at a glance. For example, you could name an NGS assay using an acronym for the type of NGS assay (RNAseq, ChIPseq, ATACseq), a keyword that represents a unique descriptive element of that assay, and the date. Like this:\n&lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD\nFor example CHIP_Oct4_20230101 is a ChIPseq assay made on 1st January 2023 with the keyword Oct4, so it is easily identifiable by eye.\n\nAssay ID code names\nBelow is a example list of different Assay-ID code names. You are welcome to use it and expand it as you wish!\n\nCHIP: ChIP-seq\nRNA: RNA-seq\nATAC: ATAC-seq\nSCR: scRNA-seq\nPROT: Mass Spectrometry Assay\nCAT: Cut&Tag\nCAR: Cut&Run\nRIME: Rapid Immunoprecipitation Mass spectrometry of Endogenous proteins\n‚Ä¶\n\n\n\nAssay folder structure\nNext, let‚Äôs take a look at a possible folder structure and what kind of files you can find there.\nCHIP_Oct4_20230101/\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ metadata.yml\n‚îú‚îÄ‚îÄ pipeline.md\n‚îú‚îÄ‚îÄ processed\n‚îî‚îÄ‚îÄ raw\n   ‚îú‚îÄ‚îÄ .fastq.gz\n   ‚îî‚îÄ‚îÄ samplesheet.csv\n\nREADME.md: Long description of the assay in markdown format. It should contain provenance of the raw NGS data (samples, laboratory protocols used, aim of the assay, etc)\nmetadata.yml: metadata file for the assay describing different keys and important information regarding that assay (see this lesson).\npipeline.md: description of the pipeline used to process raw data, as well as the commands used to run the pipeline.\nprocessed: folder with results of the preprocessing pipeline. Contents depend on the pipeline used.\nraw: folder with the raw data.\n\n.fastq.gz:In the case of NGS assays, there should be fastq files.\nsamplesheet.csv: file that contains metadata information for the samples. This file is used to run the nf-core pipelines. You can also add extra columns with info regarding the experimental variables and batches so it can be used for downstream analysis as well.\n\n\nThis folder structure is simple and straight forward, and files will be exactly where you expect them to be! By including a description and metadata file, you will always be able to understand what this experiment was and how it was generated. This information will be specially useful once you need to submit your data for archiving or write a manuscript. The metadata file will be really handy if you want to collect information on all your NGS datasets and create a database (see this lesson)!\nNote that the processed folder is not expanded upon. This folder will be very dependant on the workflows/pipelines that you use. We strongly recommend that you use reproducible and community curated pipelines, such as the ones developed by the nf-core community, which have a through documentation on the results they produce. For example, imagine you have run an RNAseq experiment and processed the data using the nf-core:rnaseq pipeline with the pseudoalignment and quantification option:\nprocessed/\n‚îú‚îÄ‚îÄ fastqc/\n‚îú‚îÄ‚îÄ multiqc/\n‚îú‚îÄ‚îÄ pipeline_info/\n‚îú‚îÄ‚îÄ salmon/\n‚îî‚îÄ‚îÄ trimgalore/\n\nfastqc: Quality Control results of the raw fastq files.\nmultiqc: Full compilation of the Quality Control checks for all your samples for the entire pipeline.\npipeline_info: Information and logs about the pipeline used for each of the steps of the workflow.\nsalmon: Pseudoalignment and quantification results of the salmon algorithm.\ntrimgalore: Cleaned fastq files and Quality Control results of the cleaned files.\n\nBy using a standardized pipeline, you should be able to always find the preprocessing results where you expect them to be! No need to come up with your own file organization (and document it) or worry that your colleagues (or your future self) won‚Äôt be able to find the data.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Best Practices for Data Storage"
    ]
  },
  {
    "objectID": "develop/04_metadata.html",
    "href": "develop/04_metadata.html",
    "title": "4. Documentation for biodata",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nThe role of documentation in effective management\nBest practices to create metadata and README files\nSources for controlled vocabularies\n(Optional) Create a database and a catalog browser\nIn bioinformatics data management, documentation plays a critical role in ensuring clarity and reproducibility. Documentation and metadata are essential components in ensuring your data adheres to FAIR principles.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Documentation for biodata"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#what-is-metadata-and-why-it-is-important",
    "href": "develop/04_metadata.html#what-is-metadata-and-why-it-is-important",
    "title": "4. Metadata for NGS data",
    "section": "What is metadata and why it is important",
    "text": "What is metadata and why it is important\n From ontotext.com\nImagine you have a batch of DNA sequences from different people. The raw sequences are like jigsaw puzzle pieces, and metadata is a cheat sheet that tells you which pieces fit where. It could include details like when and where the samples were collected, the lab procedures used, who did created them, and even the equipment involved, providing context and making sense of the who, what, when, where, and why.\n\n\n\n\n\n\nDefinition of metadata\n\n\n\nMetadata refers to data that provides information about other data. It describes various aspects of the data, such as its origin, structure, format, and context. Metadata is typically used to facilitate the organization, management, and interpretation of data, making it easier to locate, access, and understand. In essence, metadata adds valuable context and attributes to the primary data, enhancing its usability and ensuring efficient data management.\"\n\n\nLet‚Äôs think of an example that shows why metadata is extremelly important. Imagine you‚Äôre in a big lab with a plethora of different datasets all saved under generic folder names. Without metadata, it would be like searching for a needle in a haystack. You‚Äôd have folders labeled ‚ÄòExperiment123,‚Äô ‚ÄòDataBatch42,‚Äô and so on, but zero clues about what‚Äôs inside. With metadata, ‚ÄòExperiment123‚Äô is not just that anymore, but ‚ÄòDNA Sequencing of Human Cells, March 2023‚Äô. Providing relevant metadata converts data chaos into an organized repository, turning data exploration, interpretation, and insight extraction into a much easier journey. This is true not only for yourself or your lab, but to other researchers that might want to reuse your data. Collecting metadata from the very beginning of the research project will help tremendously to alleviate future efforts to understand the data. It can also help you make an organised collection of data, so that you do not lose any information regarding the data, or that you do not repeat an unnecessary experiment that someone else has done it before, but you could not find. That is, it helps you save money and time!\n\n\n\n\n\n\nBenefits of collecting proper metadata\n\n\n\n1. **Data Context and Interpretation**: Metadata provides crucial context to NGS data, offering insights into the experimental conditions, sample origins, and processing methods. This context is vital for understanding data variations, drawing accurate conclusions, and interpreting results correctly.\n2. **Data Discovery and Access**: With metadata, researchers can easily locate and access specific NGS datasets within large repositories. Details like sample identifiers, experimental parameters, and timestamps help researchers quickly identify relevant data for their analyses.\n3. **Reproducibility and Collaboration**: Metadata ensures that NGS experiments can be replicated and validated by others. By sharing comprehensive metadata, researchers enable colleagues to reproduce analyses, compare results, and collaborate effectively, bolstering the integrity of scientific findings.\n4. **Quality Control and Validation**: Metadata supports data quality assessment by allowing researchers to track the origin and handling of NGS data. It enables the identification of potential errors or biases, helping researchers validate the accuracy and reliability of their analyses.\n5. **Long-Term Data Preservation**: Properly documented metadata is essential for preserving NGS data over time. As research evolves, metadata ensures that future generations can understand and utilize archived NGS datasets, ensuring the continued impact of scientific discoveries.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Metadata for NGS data"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#how-to-collect-metadata-in-your-folders",
    "href": "develop/04_metadata.html#how-to-collect-metadata-in-your-folders",
    "title": "4. Metadata for NGS data",
    "section": "How to collect metadata in your folders",
    "text": "How to collect metadata in your folders\nIn our previous lesson, we learnt about how to organize your data into different types of folders: Assays and Projects. Both of these folders contain a metadata.yml file and a README.md file. In this section, we will check what kind of information you should collect in each of these files.\n\nREADME.md file\nThe README.md file is a markdown file that allows you to write a long description of the data placed in a folder. Since it is a markdown file, you are able to write in rich text format (bold, italic, include links, etc) what is inside the folder, why it was created/collected, how and when. If it is an Assay folder, you could include the laboratory protocol used to generate the samples, images explaining the experiment design, a summary of the results of the experiment and any sort of comments that would help to understand the context of the experiment. On the other hand, a ‚ÄòProject‚Äô README file may contain a description of the project, what are its aims, why is it important, what ‚ÄòAssays‚Äô is it using, how to interpret the code notebooks, a summary of the results and, again, any sort of comments that would help to understand the project.\nHere is an example of a README file for a `Project`` folder:\n# NGS Analysis Project: Exploring Gene Expression in Human Tissues\n\n## Aims\n\nThis project aims to investigate gene expression patterns across various human tissues using Next Generation Sequencing (NGS) data. By analyzing the transcriptomes of different tissues, we seek to uncover tissue-specific gene expression profiles and identify potential markers associated with specific biological functions or diseases.\n\n## Why It's Important\n\nUnderstanding tissue-specific gene expression is crucial for deciphering the molecular basis of health and disease. Identifying genes that are uniquely expressed in certain tissues can provide insights into tissue function, development, and potential therapeutic targets. This project contributes to our broader understanding of human biology and has implications for personalized medicine and disease research.\n\n## Datasets\n\nWe have used internal datasets with IDs: RNA_humanSkin_20201030, RNA_humanBrain_20210102, RNA_humanLung_20220304.\n\nIn addition, we utilized publicly available NGS datasets from the GTEx (Genotype-Tissue Expression) project, which provides comprehensive RNA-seq data across multiple human tissues. These datasets offer a wealth of information on gene expression levels and isoform variations across diverse tissues, making them ideal for our analysis.\n\n## Summary of Results\n\nOur analysis revealed distinct gene expression patterns among different human tissues. We identified tissue-specific genes enriched in brain tissues, highlighting their potential roles in neurodevelopment and function. Additionally, we found a set of genes that exhibit consistent expression across a range of tissues, suggesting their fundamental importance in basic cellular processes.\n\nFurthermore, our differential expression analysis unveiled significant changes in gene expression between healthy and diseased tissues, shedding light on potential molecular factors underlying various diseases. Overall, this project underscores the power of NGS data in unraveling intricate gene expression networks and their implications for human health.\n\n---\n\nFor more details, refer to our [Jupyter Notebook](link-to-jupyter-notebook.ipynb) for the complete analysis pipeline and code.\n\n\nmetadata.yml\nThe metadata file is a yml file, which is a text document that contains data formatted using a human-readable data format for data serialization. It is not mandatory that you use yml format, any other structured file format will work too, such as json files. We recommend using yml format because it is easily readable for humans, so non-coding people will have an easier time checking, writing or modifying the file if they need to.\n\n\n\nyaml file example",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Metadata for NGS data"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#metadata-fields",
    "href": "develop/04_metadata.html#metadata-fields",
    "title": "4. Metadata for NGS data",
    "section": "Metadata fields",
    "text": "Metadata fields\nThere is a ton of information you can collect regarding an NGS assay or a project. Some information fields are very general, such as author or date, while others are specific to the Assay or Project folder. Below, we will take a look at minimal information you should collect in each of the folders. But before that, we need to talk about controlled vocabularies/ontologies.\nImagine this scenario: a researcher in genomics us excited to explore various NGS datasets from different human tissue samples to study gene expression patterns. However, she encounters a significant hurdle: the tissue names used in the datasets are inconsistent and lack standardized terms. Some datasets refer to ‚Äúbrain,‚Äù while others use ‚Äúcerebral cortex‚Äù or ‚Äúcortical tissue.‚Äù This lack of controlled vocabularies for tissue names complicates her data integration efforts, requiring her to spend additional time curating and mapping tissue labels to establish meaningful cross-dataset comparisons.\nSo, what happened here? If the original creators of the NGS datasets had adopted a standardized vocabulary for tissue names, Dr.¬†Smith could have seamlessly integrated the data without the need for extensive curation. By employing a widely accepted tissue ontology, like the Uberon Ontology, dataset contributors could have consistently used predefined terms, such as ‚Äúbrain‚Äù or ‚Äúcerebral cortex.‚Äù This practice would have not only simplified data integration but also facilitated accurate cross-dataset comparisons and enabled more reliable downstream analyses.\nIn the context of NGS data, ontologies and controlled vocabularies play a pivotal role in clarifying and categorizing many concepts and information about your data. More examples are: the Gene Ontology (GO), which provides a shared vocabulary to describe gene functions, molecular processes, and cellular components, enhancing the consistency and comparability of NGS results, and Ensembl gene IDs, which are used to uniquely represent individual genes and provide a standardized way of referencing and accessing gene-related information across various species. By leveraging ontologies, researchers ensure that metadata fields capture specific details consistently across experiments, from sample sources and protocols to experimental conditions.\n\n\n\n\n\n\nDefinition of ontology\n\n\n\nAn ontology is a formal representation of knowledge that encompasses concepts, their attributes, and the relationships between them within a particular domain or subject area. It serves as a structured framework for organizing and categorizing information, facilitating the interpretation, sharing, and integration of knowledge across diverse applications and disciplines. Ontologies employ standardized vocabularies and define the semantics of terms, enabling effective communication and reasoning among humans and computer systems. They play a pivotal role in knowledge representation, data integration, and semantic interoperability, contributing to enhanced understanding, collaboration, and analysis within complex domains.\n\n\nThis standardization not only enhances data discoverability and interoperability but also empowers robust data analysis, accelerates knowledge sharing, and enables meaningful cross-study comparisons. In essence, ontologies serve as the universal translators of the scientific language, fostering a harmonious symphony of data interpretation and collaboration.\n\nGeneral metadata fields\nHere you can find a list of suggestions for general metadata fields that can be used for both assays and project folders:\n\nTitle: A brief yet informative name for the dataset.\nAuthor(s): The individual(s) or organization responsible for creating the dataset. You can use your ORCID\nDate Created: The date when the dataset was originally generated or compiled. Use YYYY-MM-DD format!\nDate Modified: The date when the dataset was last updated or modified. Use YYYY-MM-DD format!\nObject ID: The project or assay ID for tracking and reference purposes.\nDescription: A short narrative explaining the content, purpose, and context.\nKeywords: A set of descriptive terms or phrases that capture the folder‚Äôs main topics and attributes.\nEthical and Legal Considerations: Information about ethical approvals, consent, and any legal restrictions.\nVersion: The version number or identifier for the folder, useful for tracking changes.\nRelated Publications: Links or references to any scientific publications associated with the folder. Try to add here the DOI!\nFunding Source: Details about the funding agency or source that supported the research and data generation.\nLicense: The type of license or terms of use associated with the dataset/project.\nContact Information: Contact details for individuals who can provide further information about the dataset/project.\n\n\n\nAssay metadata fields\nHere you will find a table with possible metadata fields that you can use to annotate and track your Assay folders:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nassay_ID\nIdentifier for the assay that is at least unique within the project\n&lt;Assay-ID\\&gt;_&lt;keyword\\&gt;_YYYYMMDD\nNA\nCHIP_Oct4_20200101\n\n\nassay_type\nThe type of experiment performed, eg ATAC-seq or seqFISH\nNA\nontology field- e.g. EFO or OBI\nChIPseq\n\n\nassay_subtype\nMore specific type or assay like bulk nascent RNAseq or single cell ATACseq\nNA\nontology field- e.g. EFO or OBI\nbulk ChIPseq\n\n\nowner\nOwner of the assay (who made the experiment?).\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nJose Romero\n\n\nplatform\nThe type of instrument used to perform the assay, eg Illumina HiSeq 4000 or Fluidigm C1 microfluidics platform\nNA\nontology field- e.g. EFO or OBI\nIllumina\n\n\nextraction_method\nTechnique used to extract the nucleic acid from the cell\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nlibrary_method\nTechnique used to amplify a cDNA library\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nexternal_accessions\nAccession numbers from external resources to which assay or protocol information was submitted\nNA\neg protocols.io, AE, GEO accession number, etc\nGSEXXXXX\n\n\nkeyword\nKeyword for easy identification\nwordWord\ncamelCase\nOct4ChIP\n\n\ndate\nDate of assay creation\nYYYYMMDD\nNA\n20200101\n\n\nnsamples\nNumber of samples analyzed in this assay\n&lt;integer\\&gt;\nNA\n9\n\n\nis_paired\nPaired fastq files or not\n&lt;single OR paired\\&gt;\nNA\nsingle\n\n\npipeline\nPipeline used to process data and version\nNA\nNA\nnf-core/chipseq -r 1.0\n\n\nstrandedness\nThe strandedness of the cDNA library\n&lt;+ OR - OR *\\&gt;\nNA\n*\n\n\nprocessed_by\nWho processed the data\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nSarah Lundregan\n\n\norganism\nOrganism origin\n&lt;Genus species\\&gt;\nTaxonomy name\nMus musculus\n\n\norigin\nIs internal or external (from a public resources) data\n&lt;internal OR external\\&gt;\nNA\ninternal\n\n\npath\nPath to files\n&lt;/path/to/file\\&gt;\nNA\nNA\n\n\nshort_desc\nShort description of the assay\nplain text\nNA\nOct4 ChIP after pERK activation\n\n\nELN_ID\nID of the experiment/assay in your Electronic Lab Notebook software, like labguru or benchling\nplain text\nNA\nNA\n\n\n\n\n\n\n\n\n\n\nSample metadata fields\nThere is some information that will be specific to your samples. For example, which samples are treated, which are control, which tissue do they come from, which cell type, etc. In this case, it would be beneficial if you include all this information in the samplesheet.csv that describes the fastq files. Here is a list of possible metadata fields that you can use:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nsample\nName of the sample\nNA\nNA\ncontrol_rep1, treat_rep1\n\n\nfastq_1\nPath to fastq file 1\nNA\nNA\nAEG588A1_S1_L002_R1_001.fastq.gz\n\n\nfastq_2\nPath to paired fastq file, if it is a paired experiment\nNA\nNA\nAEG588A1_S1_L002_R2_001.fastq.gz\n\n\nstrandedness\nThe strandedness of the cDNA library\n&lt;unstranded OR forward OR reverse \\&gt;\nNA\nunstranded\n\n\ncondition\nVariable of interest of the experiment, such as \"control\", \"treatment\", etc\nwordWord\ncamelCase\ncontrol, treat1, treat2\n\n\ncell_type\nThe cell type(s) known or selected to be present in the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\ntissue\nThe tissue from which the sample was taken\nNA\nUberon\nNA\n\n\nsex\nThe biological/genetic sex of the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\ncell_line\nCell line of the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\norganism\nOrganism origin of the sample\n&lt;Genus species&gt;\nTaxonomy\nMus musculus\n\n\nreplicate\nReplicate number\n&lt;integer\\&gt;\nNA\n1\n\n\nbatch\nBatch information\nwordWord\ncamelCase\n1\n\n\ndisease\nAny diseases that may affect the sample\nNA\nDisease Ontology or MONDO\nNA\n\n\ndevelopmental_stage\nThe developmental stage of the sample\nNA\nNA\nNA\n\n\nsample_type\nThe type of the collected specimen, eg tissue biopsy, blood draw or throat swab\nNA\nNA\nNA\n\n\nstrain\nStrain of the species from which the sample was collected, if applicable\nNA\nontology field - e.g. NCBITaxonomy\nNA\n\n\ngenetic variation\nAny relevant genetic differences from the specimen or sample to the expected genomic information for this species, eg abnormal chromosome counts, major translocations or indels\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n\nProject metadata fields\nHere you will find a table with possible metadata fields that you can use to annotate and track your Project folders:\nIn development.\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nproject\nProject ID\n&lt;surname\\&gt;_et_al_2023\nNA\nproks_et_al_2023\n\n\nauthor\nOwner of the project\n&lt;First name\\&gt; &lt;Surname\\&gt;\nNA\nMartin Proks\n\n\ndate\nDate of creation\nYYYYMMDD\nNA\n20230101\n\n\ndescription\nShort description of the project\nPlain text\nNA\nThis is a project describing the effect of Oct4 perturbation after pERK activation",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Metadata for NGS data"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#more-info",
    "href": "develop/04_metadata.html#more-info",
    "title": "4. Metadata for NGS data",
    "section": "More info",
    "text": "More info\nThe information provided in this lesson is not at all exhaustive. There might be many more fields and controlled vocabularies that could be useful for your NGS data. We recommend that you take a look at the following sources for more information!\n\nTranscriptomics metadata standards and fields\nBionty: Biological ontologies for data scientists.\n\n::: {.callout-tip appearance=‚Äúsimple‚Äù} # ‚ÄúExercise: modify the metadata files in your cookiecutter templates‚Äù We have seen some examples of metadata for NGS data. It is time now to customize your cookiecutter templates and modify the metadata.yml files so that they fit your needs!\n1. Think about what kind of metadata you would like to include.\n2. Modify the `cookiecutter.json` file so that when you create a new folder template, all the metadata is filled accordingly.\n![cookiecutter_json_example](./images/cookiecutter_json.png)\n3. Modify the `metadata.yml` file so that it includes the metadata recorded by the `cookiecutter.json` file.\n![assay_metadata_example](./images/assay_metadata.png)\n4. Modify the `README.md` file so that it includes the short description recorded by the `cookiecutter.json` file.\n5. Git add, commit and push the changes of your template.\n6. Test your folders by using the command `cookiecutter &lt;URL to your cookiecutter repository in GitHub&gt;`",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Metadata for NGS data"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#wrap-up",
    "href": "develop/04_metadata.html#wrap-up",
    "title": "4. Documentation for biodata",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we‚Äôve covered the importance of attaching metadata to your data for future reusability and comprehension. We briefly introduced various controlled vocabularies and provided several sources for inspiration. Implementing ontologies is optional, as their usage complexity varies.\nOptionally, if you‚Äôve gone through the lesson, you‚Äôve learned how to use the metadata YAML files to create a database and a catalog browser using Shiny apps. This makes it easy to manage all assays together.\n\nSources\n\nRMDKit: https://rdmkit.elixir-europe.org/data_brokering#collecting-and-processing-the-metadata-and-data\nFAIRsharing.org: provide a searchable database of metadata standards for a wide variety of disciplines\n\nOther sources:\n\nJohns Hopkins Sheridan libraries, RDM. They provide a list of medical metadata standards resources.\n\nKU Leuven Guidance: https://www.kuleuven.be/rdm/en/guidance/documentation-metadata\nTranscriptomics metadata standards and fields\nNIH standardizing data collection\nObservational Health Data Sciences and Informatics (OHDSI) OMOP Common Data Model\n\n\n\nTools and software\n\nRightfield: open source tool facilitates the integration of ontology terms into Excel spreadsheet.\nOwlready2: Python package, enables the loading of ontologies as Python objects. This versatile tool allows users to manipulate and store ontology classes, instances, and properties as needed.\nShiny Apps: easy interactive web apps for data science",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Documentation for biodata"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#shared-projectresources-data-folders",
    "href": "develop/03_DOD.html#shared-projectresources-data-folders",
    "title": "3. Best Practices for Data Storage",
    "section": "Shared project/resources data folders",
    "text": "Shared project/resources data folders\nLet‚Äôs focus on the shared folders containing both experimental datasets generated in-house and other downloaded resources.\n\nNaming Shared Folders Effectively\nCreate a folder for all your NGS experiments, for instance named Assay. Each subfolder, denoted by a unique Assay-ID, should be named clearly and comprehensibly. Assay-ID comprises raw files, processed files, and the pipeline used to generate them. Raw files should remain unchanged, while modifications to processed files should be restricted post-preprocessing (e.g., after quality control) to prevent unintended alterations. Check the exercise for efficient naming of Assay-ID:\n\n\n\n\n\n\nExercise: name your Assay-ID\n\n\n\n\n\n\n\n\nHow would you ensure its name is unique and understood at a glance?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nUse an acronym (1) that describes type of NGS assay (RNAseq, ChIPseq, ATACseq) a keyword (2) that represents a unique element to that assay, and the date (3).\n&lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther Assay ID code names\n\n\n\n\n\n\nCHIP: ChIP-seq\nRNA: RNA-seq\nATAC: ATAC-seq\nSCR: scRNA-seq\nPROT: Mass Spectrometry Assay\nCAT: Cut&Tag\nCAR: Cut&Run\nRIME: Rapid Immunoprecipitation Mass spectrometry of Endogenous proteins\n‚Ä¶\n\n\n\n\n\n\nAssay folder structure\nNext, let‚Äôs take a look at a possible folder structure and what kind of files you can find there.\nCHIP_Oct4_20230101/\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ metadata.yml\n‚îú‚îÄ‚îÄ pipeline.md\n‚îú‚îÄ‚îÄ processed\n‚îî‚îÄ‚îÄ raw\n   ‚îú‚îÄ‚îÄ .fastq.gz\n   ‚îî‚îÄ‚îÄ samplesheet.csv\n\nREADME.md: Long description of the assay in markdown format. It should contain provenance of the raw NGS data (samples, laboratory protocols used, aim of the assay, etc)\nmetadata.yml: metadata file for the assay describing different keys and important information regarding that assay (see this lesson).\npipeline.md: description of the pipeline used to process raw data, as well as the commands used to run the pipeline.\nprocessed: folder with results of the preprocessing pipeline. Contents depend on the pipeline used.\nraw: folder with the raw data.\n\n.fastq.gz:In the case of NGS assays, there should be fastq files.\nsamplesheet.csv: file that contains metadata information for the samples. This file is used to run the nf-core pipelines. You can also add extra columns with info regarding the experimental variables and batches so it can be used for downstream analysis as well.\n\n\nThis folder structure is simple and straight forward, and files will be exactly where you expect them to be! By including a description and metadata file, you will always be able to understand what this experiment was and how it was generated. This information will be specially useful once you need to submit your data for archiving or write a manuscript. The metadata file will be really handy if you want to collect information on all your NGS datasets and create a database (see this lesson)!\nNote that the processed folder is not expanded upon. This folder will be very dependant on the workflows/pipelines that you use. We strongly recommend that you use reproducible and community curated pipelines, such as the ones developed by the nf-core community, which have a through documentation on the results they produce. For example, imagine you have run an RNAseq experiment and processed the data using the nf-core:rnaseq pipeline with the pseudoalignment and quantification option:\nprocessed/\n‚îú‚îÄ‚îÄ fastqc/\n‚îú‚îÄ‚îÄ multiqc/\n‚îú‚îÄ‚îÄ pipeline_info/\n‚îú‚îÄ‚îÄ salmon/\n‚îî‚îÄ‚îÄ trimgalore/\n\nfastqc: Quality Control results of the raw fastq files.\nmultiqc: Full compilation of the Quality Control checks for all your samples for the entire pipeline.\npipeline_info: Information and logs about the pipeline used for each of the steps of the workflow.\nsalmon: Pseudoalignment and quantification results of the salmon algorithm.\ntrimgalore: Cleaned fastq files and Quality Control results of the cleaned files.\n\nBy using a standardized pipeline, you should be able to always find the preprocessing results where you expect them to be! No need to come up with your own file organization (and document it) or worry that your colleagues (or your future self) won‚Äôt be able to find the data.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Best Practices for Data Storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#navigating-shared-project-data-optimizing-folder-structures",
    "href": "develop/03_DOD.html#navigating-shared-project-data-optimizing-folder-structures",
    "title": "3. Best Practices for Data Storage",
    "section": "Navigating Shared Project Data: Optimizing Folder Structures",
    "text": "Navigating Shared Project Data: Optimizing Folder Structures\nLet‚Äôs focus on the shared folders containing both experimental datasets generated in-house and other downloaded resources.\n\nNaming Shared Folders Effectively\nCreate a folder for all your NGS experiments, for instance named Assay. Each subfolder, denoted by a unique Assay-ID, should be named clearly and comprehensibly. Assay-ID comprises raw files, processed files, and the pipeline used to generate them. Raw files should remain unchanged, while modifications to processed files should be restricted post-preprocessing (e.g., after quality control) to prevent unintended alterations. Check the exercise for efficient naming of Assay-ID:\n\n\n\n\n\n\nExercise: name your Assay-ID\n\n\n\n\n\n\n\n\nHow would you ensure its name is unique and understood at a glance?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nUse an acronym (1) that describes type of NGS assay (RNAseq, ChIPseq, ATACseq) a keyword (2) that represents a unique element to that assay, and the date (3).\n&lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther Assay ID code names\n\n\n\n\n\n\nCHIP: ChIP-seq\nRNA: RNA-seq\nATAC: ATAC-seq\nSCR: scRNA-seq\nPROT: Mass Spectrometry Assay\nCAT: Cut&Tag\nCAR: Cut&Run\nRIME: Rapid Immunoprecipitation Mass spectrometry of Endogenous proteins\n‚Ä¶\n\n\n\n\n\n\nAssay folder structure\nNext, let‚Äôs take a look at a possible folder structure and what kind of files you can find there.\nCHIP_Oct4_20230101/\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ metadata.yml\n‚îú‚îÄ‚îÄ pipeline.md\n‚îú‚îÄ‚îÄ processed\n‚îî‚îÄ‚îÄ raw\n   ‚îú‚îÄ‚îÄ .fastq.gz\n   ‚îî‚îÄ‚îÄ samplesheet.csv\n\nREADME.md: Long description of the assay in markdown format. It should contain provenance of the raw NGS data (samples, laboratory protocols used, aim of the assay, etc)\nmetadata.yml: metadata file for the assay describing different keys and important information regarding that assay (see this lesson).\npipeline.md: description of the pipeline used to process raw data, as well as the commands used to run the pipeline.\nprocessed: folder with results of the preprocessing pipeline. Contents depend on the pipeline used.\nraw: folder with the raw data.\n\n.fastq.gz:In the case of NGS assays, there should be fastq files.\nsamplesheet.csv: file that contains metadata information for the samples. This file is used to run the nf-core pipelines. You can also add extra columns with info regarding the experimental variables and batches so it can be used for downstream analysis as well.\n\n\nThis folder structure is simple and straight forward, and files will be exactly where you expect them to be! By including a description and metadata file, you will always be able to understand what this experiment was and how it was generated. This information will be specially useful once you need to submit your data for archiving or write a manuscript. The metadata file will be really handy if you want to collect information on all your NGS datasets and create a database (see this lesson)!\nNote that the processed folder is not expanded upon. This folder will be very dependant on the workflows/pipelines that you use. We strongly recommend that you use reproducible and community curated pipelines, such as the ones developed by the nf-core community, which have a through documentation on the results they produce. For example, imagine you have run an RNAseq experiment and processed the data using the nf-core:rnaseq pipeline with the pseudoalignment and quantification option:\nprocessed/\n‚îú‚îÄ‚îÄ fastqc/\n‚îú‚îÄ‚îÄ multiqc/\n‚îú‚îÄ‚îÄ pipeline_info/\n‚îú‚îÄ‚îÄ salmon/\n‚îî‚îÄ‚îÄ trimgalore/\n\nfastqc: Quality Control results of the raw fastq files.\nmultiqc: Full compilation of the Quality Control checks for all your samples for the entire pipeline.\npipeline_info: Information and logs about the pipeline used for each of the steps of the workflow.\nsalmon: Pseudoalignment and quantification results of the salmon algorithm.\ntrimgalore: Cleaned fastq files and Quality Control results of the cleaned files.\n\nBy using a standardized pipeline, you should be able to always find the preprocessing results where you expect them to be! No need to come up with your own file organization (and document it) or worry that your colleagues (or your future self) won‚Äôt be able to find the data.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Best Practices for Data Storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#navigating-shared-project-data",
    "href": "develop/03_DOD.html#navigating-shared-project-data",
    "title": "3. Data organization and storage",
    "section": "1. Navigating Shared Project Data",
    "text": "1. Navigating Shared Project Data\nLet‚Äôs focus on the shared folders containing experimental datasets generated in-house.\n\nNaming Shared Folders Effectively\nCreate a folder for all your NGS experiments, for instance, named Assay. Each subfolder, denoted by a unique Assay-ID, should be named clearly and comprehensibly. Assay-ID comprises raw files, processed files, and the pipeline used to generate them. Raw files should remain unchanged, while modifications to processed files should be restricted post-preprocessing (e.g., after quality control) to prevent unintended alterations. Check the exercise for efficient naming of Assay-ID:\n\n\n\n\n\n\nExercise: name your Assay-ID\n\n\n\n\n\n\n\n\nHow would you ensure its name is unique and understood at a glance?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nUse an acronym (1) that describes the type of NGS assay (RNAseq, ChIPseq, ATACseq) a keyword (2) that represents a unique element to that assay, and the date (3).\n&lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther Assay ID code names\n\n\n\n\n\n\nCHIP: ChIP-seq\nRNA: RNA-seq\nATAC: ATAC-seq\nSCR: scRNA-seq\nPROT: Mass Spectrometry Assay\nCAT: Cut&Tag\nCAR: Cut&Run\nRIME: Rapid Immunoprecipitation Mass spectrometry of Endogenous proteins\n‚Ä¶\n\n\n\n\nKeep in mind that these folders might be (re)used in different individual projects over many years.\n\n\nOptimizing Folder Structures\nThe provided folder structure is designed to be intuitive for NGS data. The description and metadata files aid in understanding the project‚Äôs origin and structure, crucial for archiving and manuscript preparation. There is a section dedicated to databases in lesson 4. Let‚Äôs explore the example and its folder contents:\n&lt;data_type&gt;_&lt;keyword&gt;_YYYYMMDD/\n‚îú‚îÄ‚îÄ README.md \n‚îú‚îÄ‚îÄ CHECKSUMS\n‚îú‚îÄ‚îÄ pipeline.md\n‚îú‚îÄ‚îÄ processed\n    ‚îú‚îÄ‚îÄ fastqc/\n    ‚îú‚îÄ‚îÄ multiqc/\n    ‚îú‚îÄ‚îÄ final_fastq/\n‚îî‚îÄ‚îÄ raw\n    ‚îú‚îÄ‚îÄ .fastq.gz \n    ‚îî‚îÄ‚îÄ samplesheet.csv\n\nREADME.md: This file contains general information about the project or experiment, usually in markdown or plain text format. It includes details such as such as the origin of the raw NGS data (including sample information, laboratory protocols used, and the assay‚Äôs objectives). Sometimes, it also outlines the basic directory structure and file naming conventions. README‚Äôs are great but the goal is to make everything as self-explanatory as possible!\nmetadata.yml: This serves as the metadata file for the project (see this lesson).\npipeline.md: This document describes the pipeline employed to process the raw data, along with the specific commands used to execute the pipeline. The specific format can vary depending on the workflow system employed (e.g., bash, Snakemake, Nextflow, Jupyter Notebooks, etc.) (see this lesson). Employing a standardized pipeline ensures a consistent file organization system (and the corresponding documentation)\nprocessed_data: folder with results of the preprocessing pipeline. The contents may vary depending on the pipeline utilized. For example,\n\nfastqc: quality Control results of the raw fastq files.\nmultiqc: aggregated quality control results across all samples\nfinal_fastq: cleaned and processed files\n\nraw_data: folder with the raw data.\n\n.fastq.gz or other file formats (depending on the field or the experiment)\nsamplesheet.csv: metadata information for the samples. It may contain additional columns that will facilitate downstream analysis. This file is key if are planning to use nf-core pipelines.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#personal-project-folder",
    "href": "develop/03_DOD.html#personal-project-folder",
    "title": "3. Best Practices for Data Storage",
    "section": "(Personal) Project folder",
    "text": "(Personal) Project folder\nIn the Projects folder, usually private to the individual performing the data analysis, each project has its own subfolder containing project information, data analysis scripts and pipelines, and results. It‚Äôs advisable to maintain folders for individual projects, separate from shared data folders, as project-specific files typically aren‚Äôt reused across multiple projects, and more than one dataset might be needed to answer a specific scientific question.\nThe Project folder should have a unique, easily readable, distinguishable, and instantly understandable name.For instance, consider naming it using the main author‚Äôs initials, a descriptive keyword, and the date:\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n\nProject folder structure\nNext, let‚Äôs take a look at a possible folder structure and what kind of files you can find there.\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n‚îú‚îÄ‚îÄ data (symbolic link)\n‚îÇ  ‚îî‚îÄ‚îÄ &lt;Assay-ID&gt;_&lt;description&gt;_YYYYMMDD/\n‚îú‚îÄ‚îÄ docs\n‚îÇ  ‚îî‚îÄ‚îÄ project_template.docx\n‚îú‚îÄ‚îÄ notebooks or pipelines/\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ logs\n‚îú‚îÄ‚îÄ tmp\n‚îú‚îÄ‚îÄ environment\n‚îÇ  ‚îî‚îÄ‚îÄ requirements.txt or environment.yml\n‚îú‚îÄ‚îÄ scripts/src\n‚îú‚îÄ‚îÄ reports\n‚îÇ  ‚îî‚îÄ‚îÄ figures/\n‚îÇ  ‚îî‚îÄ‚îÄ &lt;file_name&gt;.html\n‚îú‚îÄ‚îÄ results\n‚îÇ  ‚îî‚îÄ‚îÄ tables/\n‚îÇ  ‚îî‚îÄ‚îÄ figures/\n‚îî‚îÄ‚îÄ metadata.yml\n\ndata: contains symlinks or shortcuts to where the data is (raw, processed, external, etc.), avoiding duplication and modification of original files.\ndocuments: folder containing word documents, slides or pdfs related to the project. It also contains your Data Management Plan.\n\nproject_template.docx. This is a pre-filled Data Management Plan based on the Horizon Europe guidelines.\n\nnotebooks or pipelines: folder containing notebooks (Jupyter, R markdown, Quarto notebooks) or workflows (Snakemake or Nextflow) with the actual data analysis. Tip: labeled them numerically indicating the sequential order\nREADME.md: detailed description of the project in markdown format.\nreports: Generated analysis as HTML, PDF, LaTeX, etc. Great for sharing with colleagues and creating formal reports of the data analysis procedure.\n\nfigures: figures produced upon rendering notebooks. Tip: save the figures under a subfolder named after the notebook/pipeline that created them (you will appreciate this organization when you need to rerun analysis and know which script created each figure!).\n\nrequirements.txt: The requirements file for reproducing the analysis environment, e.g.¬†software and libraries/packages (and their versions!).\nresults: results from the data analysis, such as tables with differentially expressed genes, enrichment results, etc.\nscripts: folder containing helper scripts to run data analysis\nmetadata.yml: metadata file describing the dataset, samples, etc. (see this lesson).\n\n\n\n\n\n\n\nNote on the results folder\n\n\n\nResults from your code notebooks should be saved under this folder. Create a subfolder named after the notebook that created them, so you can always identify which notebook created which results!",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Best Practices for Data Storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#navigating-project-folder",
    "href": "develop/03_DOD.html#navigating-project-folder",
    "title": "3. Data organization and storage",
    "section": "2. Navigating Project folder",
    "text": "2. Navigating Project folder\nIn the Projects folder, usually private to the individual performing the data analysis, each project has its own subfolder containing project information, data analysis scripts and pipelines, and results. It‚Äôs advisable to maintain folders for individual projects, separate from shared data folders, as project-specific files typically aren‚Äôt reused across multiple projects, and more than one dataset might be needed to answer a specific scientific question.\n\nNaming Project Folders Effectively\nThe Project folder should have a unique, easily readable, distinguishable, and instantly understandable name. For instance, consider naming it using the main author‚Äôs initials, a descriptive keyword, and the date:\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n\n\nOptimizing Folder Structures\nNext, let‚Äôs take a look at a possible folder structure and what kind of files you can find there.\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n‚îú‚îÄ‚îÄ data (symbolic link)\n‚îÇ  ‚îî‚îÄ‚îÄ raw\n‚îÇ  ‚îî‚îÄ‚îÄ processed\n‚îÇ  ‚îî‚îÄ‚îÄ external (third party resources)\n‚îú‚îÄ‚îÄ docs\n‚îÇ  ‚îî‚îÄ‚îÄ project_template.docx\n‚îú‚îÄ‚îÄ notebooks or pipelines/\n‚îÇ  ‚îî‚îÄ‚îÄ data_analysis1.ipynb or data_analysis1.smk\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ logs\n‚îú‚îÄ‚îÄ tmp/scratch\n‚îú‚îÄ‚îÄ environment\n‚îÇ  ‚îî‚îÄ‚îÄ requirements.txt or environment.yml\n‚îú‚îÄ‚îÄ scripts/src\n‚îÇ  ‚îî‚îÄ‚îÄ step1.py \n‚îú‚îÄ‚îÄ reports\n‚îÇ  ‚îî‚îÄ‚îÄ figures/\n‚îÇ  ‚îî‚îÄ‚îÄ &lt;file_name&gt;.html\n‚îú‚îÄ‚îÄ results\n‚îÇ  ‚îî‚îÄ‚îÄ tables/\n‚îÇ  ‚îî‚îÄ‚îÄ figures/\n‚îî‚îÄ‚îÄ metadata.yml\n\ndata: contains symlinks or shortcuts to where the data is (raw, processed, external, etc.), avoiding duplication and modification of original files.\ndocs: a folder containing Word documents, slides, or PDFs related to the project. It also contains your Data Management Plan.\nnotebooks or pipelines: a folder containing notebooks (Jupyter, R markdown, Quarto notebooks) or workflows (Snakemake or Nextflow) with the actual data analysis. Tip: Label them numerically indicating the sequential order.\nREADME.md: detailed description of the project in markdown format.\nlogs: log files.\ntmp/scratch: store temporary or intermediate files (eg. testing).\nenvironment: files for reproducing the analysis environment to reproduce the results, such as a Dockerfile, conda yaml file, or a text file (See 6th lesson for more tips on making your pipelines reproducible). It includes software, libraries/packages, and dependencies (and their versions!).\nscripts: a folder containing helper scripts to run data analysis or source code. Other common directory names: src, source and code, pick one!\nreports: Generated analysis as HTML, PDF, LaTeX, etc. Great for sharing with colleagues and creating formal reports of the data analysis procedure.\n\nfigures: figures produced upon rendering notebooks. Tip: save the figures under a subfolder named after the notebook/pipeline that created them (you will appreciate this organization when you need to rerun analysis and know which script created each figure!).\n\nresults: results from the data analysis, such as tables and figures, etc. Tip: Create a subfolder named after the notebook or pipeline for storing the results generated by that specific notebook or pipeline.\nmetadata.yml: metadata file describing the dataset, samples, etc. (see this lesson).\n\nFor good managing project practices, version control everything with git and git-annex!\n\n\n\n\n\n\nExercise: Write your personal data structure\n\n\n\n\n\n\n\n\nCreate your own data structure for one of the projects you are currently working on. Consider how it is similar to the example provided and how it differs. Make sure the data structure is easily understandable and navigable.\nWhat improvements or modifications could be made to enhance clarity and efficiency? Check the following callout for more examples to get inspired.\n\n\n\n\n\n\n\n\n\n\n\n\nNeed more examples?\n\n\n\n\n\nIf you want to get inspired, here are two other templates proposed by A. The Turing way and B. Coderefinery:\n\n\n\nProject Folder/\n‚îú‚îÄ‚îÄ docs                     &lt;- documentation\n‚îÇ   ‚îî‚îÄ‚îÄ codelist.txt\n‚îÇ   ‚îî‚îÄ‚îÄ project_plan.txt\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îÇ   ‚îî‚îÄ‚îÄ deliverables.txt\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îî‚îÄ‚îÄ raw/\n‚îÇ       ‚îî‚îÄ‚îÄ my_data.csv\n‚îÇ   ‚îî‚îÄ‚îÄ clean/\n‚îÇ       ‚îî‚îÄ‚îÄ data_clean.csv\n‚îú‚îÄ‚îÄ analysis                 &lt;- scripts\n‚îÇ   ‚îî‚îÄ‚îÄ my_script.R\n‚îú‚îÄ‚îÄ results                  &lt;- analysis output     \n‚îÇ   ‚îî‚îÄ‚îÄ figures\n‚îú‚îÄ‚îÄ .gitignore               &lt;- files excluded from git version control\n‚îú‚îÄ‚îÄ install.R                &lt;- environment setup\n‚îú‚îÄ‚îÄ CODE_OF_CONDUCT          &lt;- Code of Conduct for community projects\n‚îú‚îÄ‚îÄ CONTRIBUTING             &lt;- Contribution guideline for collaborators\n‚îú‚îÄ‚îÄ LICENSE                  &lt;- software license\n‚îú‚îÄ‚îÄ README.md                &lt;- information about the repo\n‚îî‚îÄ‚îÄ report.md                &lt;- report of project\n\n\n\nproject_name/ \n‚îú‚îÄ‚îÄ README.md # overview of the project\n‚îú‚îÄ‚îÄ data/ # data files used in the project \n‚îÇ   ‚îú‚îÄ‚îÄ README.md # describes where data came from \n‚îÇ   ‚îî‚îÄ‚îÄ sub-folder/ # may contain subdirectories \n‚îú‚îÄ‚îÄ processed_data/ # intermediate files from the analysis \n‚îú‚îÄ‚îÄ manuscript/ # manuscript describing the results \n‚îú‚îÄ‚îÄ results/ # results of the analysis (data, tables, figures) \n‚îú‚îÄ‚îÄ src/ # contains all code in the project \n‚îÇ   ‚îú‚îÄ‚îÄ LICENSE # license for your code \n‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt # software requirements and dependencies \n‚îÇ   ‚îî‚îÄ‚îÄ ... \n‚îî‚îÄ‚îÄ doc/ # documentation for your project \n‚îú‚îÄ‚îÄ index.rst \n‚îî‚îÄ‚îÄ ...",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/07_repos.html",
    "href": "develop/07_repos.html",
    "title": "7. Storing and sharing biodata",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nRepositories for managing biological data\nArchive your GitHub data analysis repositories\nWhile platforms like GitHub excel in version control and collaborative coding, repositories like Zenodo, Gene Expression Omnibus, and Annotare specialize in archiving and sharing scientific data, ensuring long-term accessibility for the global research community.",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Storing and sharing biodata"
    ]
  },
  {
    "objectID": "develop/07_repos.html#what-is-a-repositoryarchive",
    "href": "develop/07_repos.html#what-is-a-repositoryarchive",
    "title": "7. Repositories for NGS data",
    "section": "What is a repository/archive?",
    "text": "What is a repository/archive?\nSpecialized repositories/archives are dedicated digital platforms designed for the secure storage, curation, and dissemination of scientific data. These repositories hold great importance in the research community as they serve as reliable archives for preserving valuable datasets. Their standardized formats and robust curation processes ensure the long-term accessibility and citability of research findings. Researchers worldwide rely on these repositories to share, discover, and validate scientific information, thereby fostering transparency, collaboration, and the advancement of knowledge across various domains of study.\n\n\n\n\n\n\nList of repositories for biological data\n\n\n\n\n\n\n\n\nNCBI Gene Expression Omnibus (GEO)\nEuropean Nucleotide Archive (ENA)\nSequence Read Archive (SRA)\nProtein Data Bank (PDB)\nKyoto Encyclopedia of Genes and Genomes (KEGG)\nUniversal Protein Resource (UniProt)\nHuman Protein Atlas\nArrayExpress\nModel Organism Databases (MODs)\nFunctional Annotation of Animal Genomes (FAANG) Data Repository",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Repositories for NGS data"
    ]
  },
  {
    "objectID": "develop/07_repos.html#why-are-they-important",
    "href": "develop/07_repos.html#why-are-they-important",
    "title": "7. Repositories for NGS data",
    "section": "Why are they important?",
    "text": "Why are they important?\nArchiving your data in these repositories offers a multitude of benefits. Firstly, it ensures the enduring accessibility and preservation of your research, safeguarding it for future generations of scientists. Additionally, repositories typically provide a unique Identifier (often a DOI) for your dataset, granting it a citable status in the academic world. This not only enhances the visibility and impact of your work but also facilitates proper attribution.\nMoreover, these repositories often encourage or require comprehensive metadata to accompany your data. This rich contextual information includes details about the methodology, experimental setup, and any other pertinent information. Such metadata greatly enhances the discoverability and interpretability of your dataset, enabling fellow researchers to effectively use and build upon your work. In essence, archiving your data in these repositories not only fulfills scholarly obligations but also amplifies the reach and influence of your research in the scientific community.\nIn addition, depositing data in these archives is often a mandatory requirement set by scientific journals and funding agencies. This reflects the growing recognition of the critical role these repositories play in ensuring transparency, reproducibility, and the integrity of research outcomes. By adhering to these guidelines, researchers contribute to the broader scientific community and increase the quality of their research.",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Repositories for NGS data"
    ]
  },
  {
    "objectID": "develop/07_repos.html#types-of-repositories",
    "href": "develop/07_repos.html#types-of-repositories",
    "title": "7. Repositories for bio data",
    "section": "Types of repositories",
    "text": "Types of repositories\n\nGeneral repositories: relevant to a wide range of disciplines.\n\nZenodo\n\nDomain-specific: repositories are customized for specific fields, providing specialized curation and context-specific features. This tailored approach ensures alignment with standards and maximizes utility and impact of research findings. By catering to particular research areas, these repositories offer researchers a more focused audience, deeper domain expertise, and increased visibility within their specific research community.\n\n\nDomain-specific repositories:\nExplore some examples of NGS data below:\n\n\n\n\n\n\nENA (European Nucleotide Archive)\n\n\n\n\n\n\n\nENA: hosted by the European Bioinformatics Institute (EBI), provides researchers with a platform to deposit and access nucleotide sequences along with associated metadata, ensuring data preservation and contextualization. ENA adheres to community standards and guidelines for data submission, including those established by the International Nucleotide Sequence Database Collaboration (INSDC).\n\n\n\n\n\n\n\n\n\n\n\nGEO (Gene Expression Omnibus)\n\n\n\n\n\n\n\nGEO (Gene Expression Omnibus): curated by the National Center for Biotechnology Information (NCBI), serves as a specialized repository for high-throughput functional genomic data sets, particularly gene expression data across diverse biological conditions and experimental designs. Researchers can easily deposit and access a variety of genomic data, fostering data transparency and reproducibility within the scientific community. GEO assigns unique accession numbers to each dataset, ensuring traceability and proper citation in research publications.\n\n\n\n\n\n\n\n\n\n\n\nAnnotare\n\n\n\n\n\n\n\nArrayExpress/Annotare: hosted by the European Bioinformatics Institute (EBI), is a specialized repository tailored for storing and submitting functional genomics experiments for high-throughput sequencing data. It offers researchers a platform to upload experimental data along with comprehensive metadata ensuring preservation and contextualization. Annotare provides a curated environment aligned with the standards and practices of the field. This specialization enhances data discoverability, promotes collaboration, and facilitates deeper insights into the functional aspects of the genome.\n\n\n\n\n\nThe repositories mentioned earlier adhere to established community standards for data submission and sharing in genomics research such as:\n\nMIAME (Minimum Information About a Microarray Experiment): These guidelines ensure comprehensive and standardized reporting of microarray experiments.\nMIxS (Minimum Information about a high-throughput SeQuencing Experiment): MIxS standards, developed by the Genomic Standards Consortium, ensure consistent reporting of metadata for high-throughput sequencing experiments.\nSequence Read Archive (SRA) Submission Guidelines: They include requirements for data formatting, metadata inclusion, and quality control.\nCommunity-Specific Standards designed to ensure that submitted data meets the specific requirements and expectations of the field.\n\nBy adhering to standards, repositories ensure that submitted data is high quality, well-documented, and compliant with community best practices, promoting data discovery, reproducibility, and interoperability within the scientific community.\nFollowing all the recommendations in this course makes it straightforward to provide the necessary documentation and information for these repositories. For instance, repositories specific to NGS data will require the raw FASTQ files, sample metadata and protocols as well as final pre-processing results (for instance, read count matrices in BED files).\n\n\n\n\n\n\nWarning\n\n\n\nKeep in mind that these repositories are not intended for downstream analysis data and associated code. However, you should already have those version controlled by GitHub, which eliminates any concerns. You can then archive such repositories in a general repository like Zenodo.\n\n\n\n\nGeneral repositories: Zenodo\nZenodo[https://zenodo.org/] is an open-access digital repository supported by the European Organization for Nuclear Research (CERN) and the European Commission. It caters to various research outputs, including datasets, papers, software, and multimedia files, making it a valuable resource for researchers worldwide.With its user-friendly platform, researchers can easily upload, share, and preserve their research data. Each deposited item receives a unique Digital Object Identifier (DOI), ensuring citability and long-term accessibility. Additionally, Zenodo offers robust metadata capabilities for enriching submissions with contextual information. Moreover, researchers can link their GitHub accounts to Zenodo, simplifying the process of archiving GitHub repository releases for long-term accessibility and citation.\n\nProject archiving in Zenodo\nOnce your accounts are linked, creating a Zenodo archive becomes as straightforward as tagging a release in your GitHub repository. Zenodo automatically detects the release and generates a corresponding archive, complete with a unique Digital Object Identifier (DOI) for citable reference. Therefore, before submitting your work to a journal, link your data analysis repository to Zenodo, obtain a DOI, and cite it in your manuscript which enhances reproducibility in research.\n\n\n\nStep-by-Step Setup Guide\nCheck the practical material where we demonstrate how to link Zenodo and Github (see Exercise 6 in the practical workshop).",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Repositories for bio data"
    ]
  },
  {
    "objectID": "develop/07_repos.html#wrap-up",
    "href": "develop/07_repos.html#wrap-up",
    "title": "7. Storing and sharing biodata",
    "section": "Wrap up",
    "text": "Wrap up\nIn this concluding lesson, we‚Äôve covered the process of submitting your data to a domain-specific repository and archiving your data analysis GitHub repositories in Zenodo. By applying the lessons from this workshop, you‚Äôll significantly enhance the FAIRness of your data and improve its organization for future use. These benefits extend beyond yourself to your teammates, group leader, and the wider scientific community.\nWe hope that you found this workshop useful. If you would like to leave us some comments or suggestions, feel free to contact us.\n\n\n\n\nClick to enlarge",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Storing and sharing biodata"
    ]
  },
  {
    "objectID": "develop/05_VC.html",
    "href": "develop/05_VC.html",
    "title": "5. Version Control with Git and GitHub",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nVersion control essentials and practices\nGit and Github repositories\nCreate repositories\nGitHub page to showcase your data analysis reports",
    "crumbs": [
      "Course material",
      "Key practices",
      "5. Version Control with Git and GitHub"
    ]
  },
  {
    "objectID": "develop/05_VC.html#version-control",
    "href": "develop/05_VC.html#version-control",
    "title": "5. Data Analysis with Version Control",
    "section": "",
    "text": "Version control systematically tracks project changes, documenting alterations for understanding project evolution. It holds significant importance in research data management, software development, and data analysis, offering numerous advantages.\n\n\n\n\n\n\nAdvantages of using version control\n\n\n\n\nDocument Progress: Detailed change history aids understanding of project development and modifications.\nEnsure Data Integrity: Prevents accidental data loss or corruption, with each change tracked for easy recovery.\nFacilitate Collaboration: Enables seamless collaboration among team members, allowing multiple individuals to work concurrently without conflicts.\nReproducibility: Preserves project state for accurate validation and analysis.\nBranching and Experimentation: Allows the creation of alternative project versions for experimentation, without altering the main branch.\nGlobal Accessibility: Platforms like GitHub provide visibility for sharing, feedback, and contribution to open science.\n\n\n\n\n\n\n\n\n\nTake our course on Git & Github\n\n\n\nif you‚Äôre interested in delving deeper, explore our course on Git and GitHub.\nAlternatively, here are some examples and online resources to expand your understanding:\n\nGit and GitHub online resources\nGitHub documentation\nGit documentation\n\n\n\n\n\nGit is a widely adopted version control system that empowers developers and researchers to efficiently manage their project‚Äôs history, collaborate seamlessly, track changes, and ensure data integrity. Git operates on core principles and mechanisms:\n\nLocal Repository: Each user maintains a local repository on their computer, storing the complete project history for independent work.\nSnapshots, Not Files: Git captures snapshots of the entire project at different points instead of tracking individual file changes, ensuring data consistency.\nCommits: Users create ‚Äòcommits‚Äô as snapshots of the project at specific moments, recording changes made to files along with explanatory commit messages.\nBranching: Git supports branching, enabling users to create separate lines of development for new features or bug fixes without affecting the main branch.\nMerging: Changes from one branch can be merged into another, facilitating the incorporation of new features or bug fixes back into the main project with a smooth merging process.\nDistributed Architecture: Git‚Äôs distributed nature means each user‚Äôs local repository is a complete copy of the project, enabling offline work and ensuring data redundancy.\nRemote Repositories: Users can connect and synchronize their local repositories with remote repositories hosted on platforms like GitHub, facilitating collaboration and project sharing.\nPush and Pull: Users ‚Äòpush‚Äô their local changes to a remote repository to share with others and ‚Äòpull‚Äô changes made by others into their local repository to stay updated.\nConflict Resolution: Git provides tools to resolve conflicts manually in cases of conflicting changes, ensuring data integrity during collaboration.\nVersioning and Tagging: Git offers versioning and tagging capabilities, allowing users to mark specific points in history such as major releases or significant milestones.\n\n\n\n\nIn addition to exploring Git, we will also explore GitHub, a collaborative platform for hosting Git repositories. GitHub enhances Git‚Äôs capabilities by offering features like issue tracking, security measures to protect repositories, and GitHub Pages for creating project websites. Additionally, GitHub provides the option to set repositories as private until you are ready to share your work publicly.\n\n\n\n\n\n\nAlternatives flows for collaborative projects\n\n\n\n\nGitLab\nBitBucket\n\nWe will focus on GitHub for the remainder of this lesson due to its widespread usage and compatibility.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe will discuss repositories for archiving experimental or large datasets in lesson 7.\n\n\n\n\nMoving from Git to GitHub involves transitioning from a local version control setup to a remote hosting platform. You will need a GitHub account for the exercise in this section.\n\n\n\n\n\n\nCreate a GitHub account\n\n\n\n\nIf you don‚Äôt have a GitHub account yet, click here\nInstall Git from Git webpage\n\n\n\nYou have two options when it comes to creating a repository for your project. First, you can start from scratch by creating a new repository and adding files to it as your project progresses. Alternatively, if you already have an existing folder structure for your project, you can initialize a repository directly from that folder. It is crucial to initiate version control in the early stages of a project to facilitate easy tracking of changes and effective management of the project‚Äôs version history from the beginning.\n\n\nIf you completed all the exercises in lesson 3, you should have a project data structure prepared. Otherwise, consider using one of your existing projects or creating a small toy example for practice using cookiecutter (see practical_workshop).\n\n\n\n\n\n\nGithub documentation link\n\n\n\n\nAdding locally hosted code to Github\n\n\n\n\n\n\n\n\n\nExercise 1: initialize a repository from an existing folder:\n\n\n\n\n\n\n\n\nInitialize the repository: Begin by running the command git init in your project directory. This command sets up a new Git repository in the current directory and is executed only once, even for collaborative projects. See (git init) for more details.\nCreate a remote repository: Once the local repository is initialized, create am empty new repository on GitHub.\nConnect the remote repository: Add the GitHub repository URL to your local repository using the command git remote add origin &lt;URL&gt;. This associates the remote repository with the name ‚Äúorigin.‚Äù\nCommit changes: If you have files you want to add to your repository, stage them using git add ., then create a commit to save a snapshot of your changes with git commit -m \"add local folder\".\nPush to GitHub: To synchronize your local repository with the remote repository and establish a tracking relationship, push your commits to the GitHub repository using git push -u origin main.\n\n\n\n\n\n\n\n\n\nAlternatively to converting folders to repositories, you can create a new repository remotely, and then clone (git clone) it locally. Here, git init is not needed. You can move the files into the repository locally (git add, git commit, and git push). If you are creating a collaborative repository, you can now share it with your colleagues.\n\n\n\n\n\n\nTips to write good commit messages\n\n\n\nWrite useful and clear Git commits. Check out this post for tips.\n\n\n\n\n\n\n\nAfter setting up your repository on GitHub, take advantage of the opportunity to enhance it by adding your data analysis reports. Whether they are in Jupyter Notebooks, R Markdown files, or HTML reports, you can showcase them on a GitHub Page.\nOnce you have created your repository (and put it in GitHub), you have now the opportunity to add your data analysis reports that you created, in either Jupyter Notebooks, R Markdown files, or HTML reports, in a GitHub Page website. Creating a GitHub page is very simple, and we recommend that you follow the nice tutorial that GitHub has put for you.\nFor simplicity, we recommend using Quarto or MkDocs. Visit their websites and follow the instructions to get started.\n\n\n\n\n\n\nTutorial links\n\n\n\n\nGet started in quarto: https://quarto.org/docs/get-started/. We recommend using the VS code tool, if you do, follow this tutorial.\nMkDocs materials to further customize MkDocs websites.\n\n\n\n\n\n\nWe provide an example of setting up Git, MkDocs, and a GitHub account, enabling you to replicate the process independently! (see Exercise 5 in the practical material)",
    "crumbs": [
      "Course material",
      "Key practices",
      "5. Data Analysis with Version Control"
    ]
  },
  {
    "objectID": "develop/05_VC.html#git-and-github",
    "href": "develop/05_VC.html#git-and-github",
    "title": "Data analysis version control",
    "section": "",
    "text": "!!! warning\nIn this section we will only talk briefly about what is Git and Github. Explaining how git works is beyond the scope of this course. If you want to know more, please check out our [course](https://heads.ku.dk/course/git_github/)! You can also check [GitHub documentation](https://docs.github.com/get-started), which cover all the basics to work with Git and GitHub.\n\n\nGit is a distributed version control system that enables developers and researchers to efficiently manage their project‚Äôs history, collaborate seamlessly, and ensure data integrity. At its core, Git operates through the following principles and mechanisms:\n\nLocal Repository: Each user maintains a local repository on their computer, allowing them to work on a project independently. This local repository stores the complete project history.\nSnapshots, Not Files: Git does not track individual file changes but rather captures snapshots of the entire project at different points in time. This ‚Äòsnapshot‚Äô approach ensures data consistency.\nCommits: Users create ‚Äòcommits,‚Äô which are snapshots of the project at a specific moment. Commits record changes made to files, along with a commit message explaining the modifications.\nBranching: Git supports branching, enabling users to create separate lines of development. Branches are useful for experimenting with new features or fixing bugs without affecting the main project.\nMerging: Changes made in one branch can be merged into another, allowing for the incorporation of new features or bug fixes back into the main project. Git ensures a smooth merging process.\nDistributed Architecture: Git‚Äôs distributed nature means that each user‚Äôs local repository is a complete copy of the project, including its entire history. This enables offline work and ensures data redundancy.\nRemote Repositories: Git allows users to connect and synchronize their local repositories with remote repositories hosted on platforms like GitHub. Remote repositories facilitate collaboration and provide a central hub for project sharing.\nPush and Pull: Users ‚Äòpush‚Äô their local changes to a remote repository to share with others. Conversely, they ‚Äòpull‚Äô changes made by others into their local repository to stay up-to-date.\nConflict Resolution: In cases of conflicting changes, Git provides tools to resolve conflicts manually, ensuring that data integrity is maintained during collaboration.\nVersioning and Tagging: Git offers versioning and tagging capabilities, allowing users to mark specific points in history, such as major releases or significant milestones.\n\n\n\n\nOn the other hand, GitHub is a web-based platform that enhances Git‚Äôs capabilities by providing a collaborative and centralized hub for hosting Git repositories. It offers several key functionalities, such as tracking issues, security features to safeguard your repos, and GitHub Pages that allows you to create websites to showcase your projects. We will see more about Github Pages in the section below. In addition, you can set your repository as private until you are ready to publish your work!\nIf you do not have an account in Github already, we recommend you to do one now! There are other alternatives to GitHub, such as BitBucket and GitLab, Some features might be different, so we will stick to Github for the rest of the lesson!"
  },
  {
    "objectID": "develop/05_VC.html#creating-a-repo-from-your-project-folder",
    "href": "develop/05_VC.html#creating-a-repo-from-your-project-folder",
    "title": "Data Analysis with Version Control",
    "section": "",
    "text": "We will show you two ways to make your Project folder into a Git repository. You will need a github account for this exercise\n!!! warning ‚ÄúWhat about my Assay folder?‚Äù\nAn assay folder contains very big files that are not suitable for version control, at least in GitHub. We recommend that you deposit the data into a domain specific archive such as [GEO](https://www.ncbi.nlm.nih.gov/geo/) or [ArrayExpress/Annotare](https://www.ebi.ac.uk/fg/annotare/login/). Even [Zenodo](https://zenodo.org/) would be a better option in this case. We will look at them in the [next session](./10_repos.qmd).\n\n\nUsing git init is probably the right choice for you if you have created a Project folder using the cookiecutter template we saw in the previous lesson.\nThe git init command should only be run once, even if other collaborators share will the project!\n\nFirst, initialize the repository (git init) and make at least one commit (git add and git commit).\nOnce you have initialized the repository, create a remote repository in GitHub\nThen, add the remote URL to your local git repository with git remote add origin \\&lt;URL\\&gt;. This stores the remote URL under a more human-friendly name, origin.\nShape your history into at least one commit by using git add to stage the existing files, and git commit to make the snapshot.\nOnce you have at least one commit, you can push to the remote and set up the tracking relationship for good with git push -u origin master.\n\n\n\n\nIf the repository already exists on a remote, you would choose to git clone and not git init. On the other hand, if you create a remote repository first with the intent of moving your project to it later, you may have a few other steps to follow. If there are no commits in the remote repository, you can follow the steps above for git init. If there are commits and files in the remote repository but you would still like it to contain your project files, git clone that repository. Then, move the project‚Äôs files into that cloned repository. git add, git commit, and git push to create a history that makes sense for the beginning of your project. Then, your team can interact with the repository without git init again.\n!!! tip ‚ÄúTips to write good commit messages‚Äù\nIf you would like to know more about Git commits and the best way to make clear git messages, check out [this post](https://www.conventionalcommits.org/en/v1.0.0/)!",
    "crumbs": [
      "Course material",
      "Key practices",
      "Data Analysis with Version Control"
    ]
  },
  {
    "objectID": "develop/05_VC.html#github-pages",
    "href": "develop/05_VC.html#github-pages",
    "title": "Data Analysis with Version Control",
    "section": "",
    "text": "Once you have created your repository (and put it in GitHub), you have now the opportunity to add your data analysis reports that you created, in either Jupyter Notebooks, Rmarkdowns or html reports, in a GitHub Page website. Creating a GitHub page is very simple, and we really recommend that you follow the nice tutorial that GitHub as put for you.\nThere are many different ways to create your webpages. We recommend using Mkdocs and Mkdocs materials as a framework to create a nice webpage in a simple manner. The folder templates that we used as an example in lesson 06 already contain everything you need to start a webpage. Nonetheless, you will need to understand the basics of MkDocs and MkDocs materials to design a webpage to your liking. MkDocs is a static webpage generator that is very easy to use, while MkDocs materials is an extension of the tool that gives you many more options to customize your website. Check out their webpages to get started!",
    "crumbs": [
      "Course material",
      "Key practices",
      "Data Analysis with Version Control"
    ]
  },
  {
    "objectID": "develop/05_VC.html#a-full-setup-example",
    "href": "develop/05_VC.html#a-full-setup-example",
    "title": "Data Analysis with Version Control",
    "section": "",
    "text": "In this section we will showcase a full example about how to setup Git, MkDocs and a github account so you can do it yourself!\n\n\nThere are several tools and softwares that you need to install. First you will need pip and install the following packages using pip in the command line:\npip install cookiecutter # cookiecutter to create folder templates\npip install cruft # cruft is used to version control your templates\npip install mkdocs # mkdocs to create your webpages\npip install mkdocs-material # mkdocs extension to customize your templates\npip install mkdocs-video # mkdocs extension to add videos or embed internet videos, like youtube, to your webpages\npip install mkdocs-bibtex # mkdocs extension to add references in your text from a bib file\npip install neoteroi-mkdocs # mkdocs extension to create author cards\npip install mkdocs-minify-plugin # mkdocs extension to minimize the html code created by mkdocs\npip install mkdocs-git-revision-date-localized-plugin  # mkdocs extension to show \"last updated\" date of your webpage\npip install mkdocs-jupyter # mkdocs extension to include jupyter notebooks without needing to convert them\npip install mkdocs-table-reader-plugin # mkdocs extension to embed tabular format files like tsv or csv\nLastly and very importantly, install Git from their webpage.\n\n\n\nGo to Github and create a new user.\n\n\n\nGitHub allows users to create organizations and teams that will collaborate together or create repositories under the same umbrella organization. If you would like to create an educational organization in GitHub, you can do so for free! For example, you could create a GitHub account for your lab.\nIn order to create a GitHub organization, follow these instructions\nAfter you have created the GitHub organization, make sure that you create your repositories under the organization space and not your own user!\n\n\n\nThe next step is to set up the main GitHub Page site and the repository that will host it. This is very simple, as you will only need to follow these steps.\nAfter you have created the organizationgithub.io, it is time to configure your webpage using MkDocs!\n\n\nFollow the steps on the MkDocs documentation to get started on your webpage! You can use a simple markdown file describing your organization (your lab or department), its main goals and missions and maybe a couple of images showcasing your research.\nWhen you are happy with your webpage and are ready too publish it, make sure to add, commit and push the changes to the remote! Instead of using the basic setup that GitHub offers, we recommend that you build up your webpage using MkDocs and the mkdocs gh-deploy command! This requires a couple of changes in your GitHub organization settings.\n\n\n\nGo to your GitHub organization settings and configure the Page section. Since you are using the mkdocs gh-deploy command to publish your site in the gh-pages branch (as explained the the mkdocs documentation), we need to change where GitHub is fetching the website from:\n\n\n\nGitHub Pages setup\n\n\n\nBranch should be gh-pages\nFolder should be root\n\nAfter a couple of minutes, your webpage should be ready!\n\n\n\n\nYour GitHub organization account and webpage is ready! Now it is time to create a cookiecutter template for your folders using what you learned in this lesson.\n\n\n\nUsing cookiecutter, create a new data analysis project. Remember to fill up your metadata and description files! After you have created the folder, it would be best to initialize a Git repo following the instructions from the previous section.\nNext, link your data of interest and make an example of data analysis notebook/report. Depending on your setup, you might be using Jupyter Notebooks or Rmarkdowns. The extensions that we have installed using pip allows you to directly add a Jupyter Notebook file to the mkdocs.yml navigation section. On the other hand, if you are using Rmarkdown, you will have to knit your document into either an html page or a github document.\n\n\n\nRemember to make sure that your markdowns, images, reports, etc., are included in the docs folder and properly set up in the navigation section of your mkdocs.yml file.\nGit add, commit and push your changes. Then, run mkdocs gh-deploy. You will still need to configure the settings of this repositories in GitHub, so that the Page is taken from the gh-pages branch and the root folder. You should be able to see your webpage through the link provided in the Page section!\nNow it is also possible to include this repository webpage in your main webpage organizationgithub.io by including the link of the repo website (https://organizationgithub.io/repo-name) in the navigation section of the mkdocs.yml file in the main organizationgithub.io repo.\n!!! question ‚ÄúExercise 5: make a project folder and publish a data analysis webpage‚Äù\n1. Configure your main GitHub Page and its repo\n\nThe first step is to set up the main GitHub Page site and the repository that will host it. This is very simple, as you will only need to follow [these steps](https://pages.github.com/).\nAfter you have created the *organization/username*github.io, it is time to configure your `Project` repository webpage using MkDocs!\n\n2. Start a new project from cookiecutter or use one from the previous exercise.\n\nIf you use a `Project` repo from the first exercise, go to the next paragraph. Using cookiecutter, create a new data analysis project. Remember to fill up your metadata and description files! After you have created the folder, it would be best to initialize a Git repo following the instructions from the [previous section](#creating-a-git-repo-online-and-copying-your-project-folder).\n\nNext, link your data of interest (or create a small fake dataset) and make an example of data analysis notebook/report (this could be just a scatter plot of a random matrix of values). Depending on your setup, you might be using Jupyter Notebooks or Rmarkdowns. The extensions that we have installed using `pip` allows you to directly add a Jupyter Notebook file to the `mkdocs.yml` navigation section. On the other hand, if you are using Rmarkdown, you will have to knit your document into either an html page or a github document.\n\nFor the purposes of this exercise, we have already included a basic `index.md` markdown file that can serve as the intro page of your repo, and a `jupyter_example.ipynb` with some code in it. You are welcome to modify them further to test them out!\n\n3. Use MkDocs to create your webpage\n\nWhen you are happy with your files and are ready too publish them, make sure to add, commit and push the changes to the remote. Then, build up your webpage using MkDocs and the [`mkdocs gh-deploy`](https://www.mkdocs.org/user-guide/deploying-your-docs/) command from the same directory where the `mkdocs.yml` file is. For example, if your `mkdocs.yml` for your `Project` folder is in `/Users/JARH/Projects/project1_JARH_20231010/mkdocs.yml`, do `cd /Users/JARH/Projects/project1_JARH_20231010/` and then `mkdocs gh-deploy`.\n\nFinally, we only need to set up the GitHub `Project` repo settings.\n\n4. Publishing your GitHub Page\n\nGo to your GitHub repo settings and configure the Page section. Since you are using the `mkdocs gh-deploy` command to publish your site in the `gh-pages` branch (as explained the the mkdocs documentation), we need to change where GitHub is fetching the website from:\n\n![GitHub Pages setup](./images/git_pages.png)\n\n- Branch should be `gh-pages`\n- Folder should be `root`\n\nAfter a couple of minutes, your webpage should be ready!",
    "crumbs": [
      "Course material",
      "Key practices",
      "Data Analysis with Version Control"
    ]
  },
  {
    "objectID": "develop/05_VC.html#wrap-up",
    "href": "develop/05_VC.html#wrap-up",
    "title": "5. Version Control with Git and GitHub",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we explored version control and utilized Git and GitHub to establish data analysis repositories from our Project folders. Additionally, we delved into creating a GitHub organization and leveraging GitHub Pages to showcase data analysis scripts and notebooks publicly. Remember to complete the corresponding exercise from the practical workshop to reinforce your knowledge.\n\nSources\n\nVersion Control and Code Repository Link\nGit cheat sheet",
    "crumbs": [
      "Course material",
      "Key practices",
      "5. Version Control with Git and GitHub"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#resources-and-databases-folder",
    "href": "develop/03_DOD.html#resources-and-databases-folder",
    "title": "3. Data organization and storage",
    "section": "3. Resources and databases folder",
    "text": "3. Resources and databases folder\nHealth databases are utilized for storing, organizing, and providing access to diverse health-related data, including genomic data, clinical records, imaging data, and more. These resources are regularly updated and released under different versions from various sources. To ensure data reproducibility, it‚Äôs crucial to manage and specify the versions and sources of data within these databases.\n\n\n\n\n\n\nExample NGS: genomic resources\n\n\n\n\n\nFor example, preprocessing NGS data involves utilizing various genomic resources for tasks like aligning and annotating fastq files. Essential resources include reference genomes in FASTA format (e.g., human and mouse), indexed fasta files for alignment tools like STAR and Bowtie, and GTF or GFF files for quantifying reads into genomic regions. One of the latest human reference genome is GRCh38, however many past studies are based on GRCh37.\nHow can you keep track of your resources? Name the folder using the version, or use a reference genome manager such as refgenie.\n\nRefgenie\nIt manages the storage, access, and transfer of reference genome resources. It provides command-line and Python interfaces to download pre-built reference genome ‚Äúassets‚Äù, like indexes used by bioinformatics tools. It can also build assets for custom genome assemblies. Refgenie provides programmatic access to a standard genome folder structure, so software can swap from one genome to another. Check this tutorial to get started.\n\n\n\n\n\nManual Download\nBest practices for downloading data from the source while ensuring the preservation of information about the version and other metadata include:\n\nOrganizing data structure: Create a data structure that allows storing all versions in the same parent directory, and ensure that all lab members follow these practices.\nDocumentation and metadata preservation: Before downloading, carefully review the documentation provided by the database. Download files containing the data version and any associated metadata.\nREADME.md: Record the version of the data in the README.md file.\nChecksums: Check for and use checksums (MD5, SHA1, SHA256, ‚Ä¶) provided by the database to verify the integrity of the downloaded data, ensuring that it hasn‚Äôt been corrupted during transfer. Do the exercise below to get more familiar with these files.\nVerify File size: Check the file size provided by the source. It is not as secure as checksum verification but discrepancies could indicate corruption.\nAutomated Processes: whenever possible, automate the download process to reduce the likelihood of errors and ensure consistency (e.g.¬†use bash script or pipeline).\n\n\n\n\n\n\n\nOptional: Exercise on CHECKSUMS\n\n\n\n\n\nWe recommend the use of md5sum to verify data integrity, especially if you are downloading large datasets, as it is commonly used. In this example, we use data from the HLA FTP Directory.\n\nInstall md5sum (from coreutils package)\n\n#!/bin/bash\n# On Ubuntu/Debian\napt-get install coreutils\n# On macOS\nbrew install coreutils\n\nCreate a bash script to download the target files (named ‚Äúdw_resources.sh‚Äù in the data structure).\n\n#!/bin/bash\n# Important: go through the README before downloading! Check if a checksums file is included. \n\n# 1. Create or change the directory to the resources dir. \n\n# Check for checksums (e.g.: md5checksum.txt), download, and modify it so that it only contains the checksums of the target files. The file will look like this:\n7348fbef5ab204f3aca67e91f6c59ed2  hla_prot.fasta\n# Finally, save it: \nmd5file=\"md5checksum.txt\"\n\n# Define the URL of the files to download\nurl=\"ftp://ftp.ebi.ac.uk/pub/databases/ipd/imgt/hla/hla_prot.fasta\"\n\n# (Optional 1) Save the original file name: filename=$(basename \"$url\")\n# (Optional 2) Define a different filename to save the downloaded file (`wget -O $out_filename`)\n# out_filename = \"imgt_hla_prot.fasta\"\n\n# Download the file\nwget $url && \\\nmd5sum --status --check $md5file\n\nWe recommend using the argument `--status` **only** when you incorporate this sanity check as part of your pipeline so that it only prints the errors (it doesn't print output when success).\n\nFolder structure\n\ngenomic_resources/\n‚îú‚îÄ‚îÄ specie1/\n‚îÇ  ‚îî‚îÄ‚îÄ version/\n‚îÇ     ‚îú‚îÄ‚îÄ files.txt\n‚îÇ     ‚îî‚îÄ‚îÄ indexes/\n‚îî‚îÄ‚îÄ dw_resources.sh\n\nCreate a md5sum file and share it with collaborators before sharing the data. This allows others to check the integrity of the files.\n\nmd5sum &lt;data&gt;\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nDownload a file using md5sums. Choose a file from your favorite dataset or select one from the HLA database (for quick testing, consider using a text file such as Nomenclature_2009.txt).",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#metadata-matters",
    "href": "develop/04_metadata.html#metadata-matters",
    "title": "4. Metadata for NGS data",
    "section": "Metadata matters",
    "text": "Metadata matters\nMetadata provides essential context and structure to (primary) data, enabling researchers to understand its significance and facilitate efficient data management. Documentation and metadata are essential components in ensuring your data adheres to FAIR principles.Some common elements found in metadata for bioinformatics data include:\n\nSample information and collection details\nExperimental conditions\nData processing steps applied to the raw data\nAnnotation and Ontology terms\nFile metadata (file type, file format, etc.)\nEthical and Legal compliance\n\n From ontotext.com\nMetadata serves as a crucial guide in navigating the complex landscape of data, akin to a cheat sheet for piecing together the puzzle of information. Much like identifying puzzle pieces, metadata provides essential details about data origin, structure, and context, such as sample collection details, experimental procedures, and equipment used. Metadata enables data exploration, interpretation, and future accessibility, promoting effective management and facilitating data usability and reuse.\n\n\n\n\n\n\nBenefits of collecting proper metadata\n\n\n\n\nData Context and Interpretation: Aiding in understanding experimental conditions, sample origins, and processing methods, crucial for accurate results interpretation.\nData Discovery and Access: Metadata enables easy locating and accessing of specific datasets by quickly identifying relevant data through sample identifiers, experimental parameters, and timestamps.\nReproducibility and Collaboration: Metadata facilitates experiment replication and validation by enabling colleagues to reproduce analyses, compare results, and collaborate effectively, enhancing the integrity of scientific findings.\nQuality Control and Validation: Metadata supports data quality assessment by tracking the origin and handling of NGS data, allowing identification of errors or biases to validate analysis accuracy and reliability.\nLong-Term Data Preservation: metadata ensures preservation over time, facilitating future understanding and utilization of archived datasets for continued scientific impact as research progresses.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Metadata for NGS data"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#streamlining-metadata-collection",
    "href": "develop/04_metadata.html#streamlining-metadata-collection",
    "title": "4. Metadata for NGS data",
    "section": "Streamlining Metadata Collection",
    "text": "Streamlining Metadata Collection\nData and project directories should both include a metadata and a README file.\n\n\n\n\n\n\nPractical tips\n\n\n\n\nImplement a logical structure with clear and descriptive file names.\nUse of controlled vocabularies and ontologies to ensure consistency and efficient data management and interpretation.\nUse a repository and a versioning system\nMake it Machine-readable, -actionable and/or -interpretable\nDevelop standards further within your research environment FAIRsharing standards\nInclude all information for others to comprehend and effectively utilize the data.\n\n\n\n\nREADME.md\nThe README.md file, written in markdown format, provides a detailed description of the folder‚Äôs content. It includes information such as the purpose of the data, collection methods, and relevant details. The content might differ based on the purpose of the data.\n\n\n\n\n\n\nIdentify README.md key components.\n\n\n\n\n\n\n\nSelect one of the examples below and reflect on how effectively the README communicates important information about the project. Please note that some of the links lead to README files describing databases, while others pertain to software and tools.\n\n1000 Genomes Project. You will find several readme files here.\n\nHomo Sapiens, fasta GRCh38\nIPD-IMGT/HLA Database\nDocker\nPython pandas\n\n\n\n\n\n\nStructure for bioinformatics projects.\n\nDescription of the project\nObjectives and aims\nDatasets and software requirements\nInstruction for data interpretation\nSummary of results\nContributions\nAdditional comments or notes\n\n\n\nmetadata.yml\nMetadata can be written in many file formats (commonly used: YAML, TXT, JSON and CSV). We recommend YAML format, which is a text document that contains data formatted using a human-readable data format for data serialization. The content will be specific to the type of project.\nmetadata:\n  project: \"Title\"\n  author: \"Name\"\n  date: \"YYYYMMDD\"\n  description: \"Project short description\"\n  version: \"1.0\"\n  analysis:\n    tool: \"software\"\n    version: \"1.1.1\"\nSome general metadata fields used across different disciplines: - Project Title:A concise and informative name for the dataset. - Author(s): The individual(s) or organization responsible for creating the dataset. Include ORCID for identification. - Date Created: The date when the dataset was originally generated or compiled, in YYYY-MM-DD format. - Date Modified: The date when the dataset was last updated or modified (YYYY-MM-DD). - Object ID: The project or assay ID for tracking and reference purposes. - Description: A short narrative explaining the content, purpose, and context of the project. - Keywords: Descriptive terms or phrases capturing the main topics and attributes. - Ethical and Legal Considerations: Information about ethical approvals, consent, and any legal restrictions. - Version: The version number or identifier, useful for tracking changes. - Related Publications: Links or references to scientific publications associated with the folder. Always add the DOI. - Funding Source: Details about the funding agency or source that supported the research or data generation. - License: The type of license or terms of use associated with the dataset/project. - Contact Information: Contact details for individuals who can provide further information about the dataset/project.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Metadata for NGS data"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#sources",
    "href": "develop/03_DOD.html#sources",
    "title": "3. Best Practices for Data Storage",
    "section": "Sources",
    "text": "Sources\n\nELIXIR (2021) Research Data Management Kit. A deliverable from the EU-funded ELIXIR-CONVERGE project (grant agreement 871075): https://rdmkit.elixir-europe.org.\nUK Data Service: https://ukdataservice.ac.uk/learning-hub/research-data-management/format-your-data/organising/\nOakland University: https://library.oakland.edu/services/research-data/file-org.html\nCessda guidelines: https://dmeg.cessda.eu/Data-Management-Expert-Guide/2.-Organise-Document/File-naming-and-folder-structure.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Best Practices for Data Storage"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#documentation-and-metadata",
    "href": "develop/04_metadata.html#documentation-and-metadata",
    "title": "4. Documentation for biodata",
    "section": "Documentation and metadata",
    "text": "Documentation and metadata\nEssential documentation comes in different forms and flavors, serving various purposes in research. Examples include protocols outlining experimental procedures, detailed lab journals recording experimental conditions and observations, codebooks explaining concepts, variables, and abbreviations used in the analysis, information about the structure and content of a dataset, software installation, and usage manual, code explanation within files or methodological information outlining data processing steps.\n From ontotext.com\nMetadata provides essential context and structure to (primary) data, enabling researchers to understand its significance and facilitate efficient data management. Some common elements found in metadata for bioinformatics data include:\n\nSample information and collection details\nExperimental conditions\nData processing steps applied to the raw data\nAnnotation and Ontology terms\nFile metadata (file type, file format, etc.)\nEthical and Legal Compliance\n\nMetadata serves as a crucial guide in navigating the complex landscape of data, akin to a cheat sheet for piecing together the puzzle of information. Much like identifying puzzle pieces, metadata provides essential details about data origin, structure, and context, such as sample collection details, experimental procedures, and equipment used. Metadata enables data exploration, interpretation, and future accessibility, promoting effective management and facilitating data usability and reuse.\n\n\n\n\n\n\nBenefits of collecting proper metadata\n\n\n\n\nData Context and Interpretation: Aiding in understanding experimental conditions, sample origins, and processing methods, is crucial for accurate results interpretation.\nData Discovery and Access: Metadata enables easy locating and accessing of specific datasets by quickly identifying relevant data through sample identifiers, experimental parameters, and timestamps.\nReproducibility and Collaboration: Metadata facilitates experiment replication and validation by enabling colleagues to reproduce analyses, compare results, and collaborate effectively, enhancing the integrity of scientific findings.\nQuality Control and Validation: Metadata supports data quality assessment by tracking the origin and handling of NGS data, allowing the identification of errors or biases to validate analysis accuracy and reliability.\nLong-Term Data Preservation: metadata ensures preservation over time, facilitating future understanding and utilization of archived datasets for continued scientific impact as research progresses.\n\n\n\n\nStreamlining Metadata Collection\nData and project directories should both include metadata and a README file.\n\n\n\n\n\n\nPractical tips\n\n\n\n\nImplement a logical structure with clear and descriptive file names.\nUse of controlled vocabularies and ontologies to ensure consistency and efficient data management and interpretation.\nUse a repository and a versioning system\nMake it Machine-readable, -actionable, and -interpretable.\nDevelop standards further within your research environment FAIRsharing standards.\nInclude all information for others to comprehend and effectively utilize the data.\n\n\n\n\n\nREADME.md\nThe README.md file, written in markdown format, provides a detailed description of the folder‚Äôs content. It includes information such as the purpose of the data, collection methods, and relevant details. The content might differ based on the purpose of the data.\n\n\n\n\n\n\nExercise 1: Identify README.md key components.\n\n\n\n\n\n\n\nSelect one of the examples below and reflect on how effectively the README communicates important information about the project. Please note that some of the links lead to README files describing databases, while others pertain to software and tools.\n\n1000 Genomes Project. You will find several readme files here.\n\nHomo Sapiens, fasta GRCh38\nIPD-IMGT/HLA Database\nDocker\nPython pandas\n\n\n\n\n\n\nStructure for bioinformatics projects.\n\nDescription and relevance the project\nObjectives and aims\nDatasets and software requirements\nInstruction for data interpretation\nSummary of results\nContributions\nAdditional comments or notes\n\n\n\nmetadata.yml\nMetadata can be written in many file formats (commonly used: YAML, TXT, JSON, and CSV). We recommend YAML format, which is a text document that contains data formatted using a human-readable data format for data serialization. The content will be specific to the type of project.\nmetadata:\n  project: \"Title\"\n  author: \"Name\"\n  date: \"YYYYMMDD\"\n  description: \"Project short description\"\n  version: \"1.0\"\n  analysis:\n    tool: \"software\"\n    version: \"1.1.1\"\nSome general metadata fields used across different disciplines:\n\nProject Title: A concise and informative name for the dataset.\nAuthor(s): The individual(s) or organization responsible for creating the dataset. Include ORCID for identification.\nDate Created: The date when the dataset was originally generated or compiled, in YYYY-MM-DD format.\nDate Modified: The date when the dataset was last updated or modified (YYYY-MM-DD).\nObject ID: The project or assay ID for tracking and reference purposes.\nDescription: A short narrative explaining the content, purpose, and context of the project.\nKeywords: Descriptive terms or phrases capturing the main topics and attributes.\nEthical and Legal Considerations: Information about ethical approvals, consent, and any legal restrictions.\nVersion: The version number or identifier, useful for tracking changes.\nRelated Publications: Links or references to scientific publications associated with the folder. Always add the DOI.\nFunding Source: Details about the funding agency or source that supported the research or data generation.\nLicense: The type of license or terms of use associated with the dataset/project.\nContact Information: Contact details for individuals who can provide further information about the dataset/project.\n\n\n\n\n\n\n\nTip\n\n\n\nThere is an exercise in the practical material to streamline the creation of metadata files using Cookiecutter, a template-based scaffolding tool.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nCreate a metadata file with the following description fields: name, date, description, version, authors, keywords, license. Fill it up at the start of the project, when you generate the file structure.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Documentation for biodata"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#controlled-vocabularies-and-ontologies",
    "href": "develop/04_metadata.html#controlled-vocabularies-and-ontologies",
    "title": "4. Documentation for biodata",
    "section": "Controlled vocabularies and ontologies",
    "text": "Controlled vocabularies and ontologies\nResearchers encountering inconsistent and non-standardized terms (e.g., gene names, disease names, cell types, protein domains, etc.) across datasets may face challenges in data integration. Thus, requiring additional curation time to enable meaningful comparisons. Standardized vocabularies streamline integration, improving consistency and comparability in analysis. Leveraging widely accepted ontologies in the documentation ensures consistent capture of experiment details in metadata fields, aiding data interpretation.\n\n\n\n\n\n\nExamples of ontology services\n\n\n\n\nBiological ontologies for data scientists - Bionty\nAnatomy - Uberon\nTissue - Uberon\nChemical compoundsChemical Entities of Biological Interest\nExperimentalFactor - Experimental Factor Ontology\nSpecies - NCBI Taxonomy, Ensembl Species\nDisease - Mondo, Human Disease\nGene - Ensembl, NCBI Gene, Gene ontology,Microarray Gene Expression Society Ontology (MGED)\nProtein - Uniprot\nCellLine - Cell Line Ontology\nCellType - Cell Ontology\nCellMarker - CellMarker\nPhenotype - Human Phenotype, Phecodes, PATO, Mammalian Phenotype, Zebrafish Phenotype\nPathway - Gene Ontology, Pathway Ontology\nDevelopmentalStage - Human Developmental Stages, Mouse Developmental Stages\nDrug - Drug Ontology\nEthnicity - Human Ancestry Ontology\nBFXPipeline - largely based on nf-core\nBioSample - NCBI BioSample attributes\nArticles Indexing Medical Subject Headings (MeSH)\n\n\n\n\n\n\n\n\n\nOntology definition\n\n\n\n\n\n\n\nAn ontology is a structured framework representing concepts, attributes, and relationships within a specific domain, aiding knowledge organization and integration. Employing standardized vocabularies, it facilitates effective communication and reasoning between humans and computers. Ontologies are crucial for knowledge representation, data integration, and semantic interoperability, enhancing understanding and collaboration across complex domains.\n\n\n\n\n\nStandardization improves data discoverability and interoperability, enabling robust analysis, accelerating knowledge sharing, and facilitating cross-study comparisons. Ontologies act as universal translators, fostering harmonious data interpretation and collaboration across scientific disciplines.\nYou can find three examples of metadata tailored for different purposes NGS data examples: sample metadata, project metadata, and experimental metadata. We suggest exploring controlled vocabularies and metadata standards within your field and seeking additional specialized sources. You will find a few sources at the end of the page.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Documentation for biodata"
    ]
  },
  {
    "objectID": "develop/examples/NGS_metadata.html",
    "href": "develop/examples/NGS_metadata.html",
    "title": "NGS Assay and Project metadata",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nDevelop your metadata\n\n\n\nYou should consider revisiting these examples after completing lesson 4 in the course material. Please review these three tables containing pre-filled data fields for metadata, each serving distinct purposes: sample metadata, project metadata, and experimental metadata.\n\nProject metadata fields\nHere you will find a table with possible metadata fields that you can use to annotate and track your Project folders:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nproject\nProject ID\n&lt;surname\\&gt;_et_al_2023\nNA\nproks_et_al_2023\n\n\nauthor\nOwner of the project\n&lt;First name\\&gt; &lt;Surname\\&gt;\nNA\nMartin Proks\n\n\ndate\nDate of creation\nYYYYMMDD\nNA\n20230101\n\n\ndescription\nShort description of the project\nPlain text\nNA\nThis is a project describing the effect of Oct4 perturbation after pERK activation\n\n\n\n\n\n\n\n\n\n\nSample metadata fields\nSome details might be specific to your samples. For example, which samples are treated, which are controlled, which tissue they come from, which cell type, the age, etc. Here is a list of possible metadata fields that you can use:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nsample\nName of the sample\nNA\nNA\ncontrol_rep1, treat_rep1\n\n\nfastq_1\nPath to fastq file 1\nNA\nNA\nAEG588A1_S1_L002_R1_001.fastq.gz\n\n\nfastq_2\nPath to paired fastq file, if it is a paired experiment\nNA\nNA\nAEG588A1_S1_L002_R2_001.fastq.gz\n\n\nstrandedness\nThe strandedness of the cDNA library\n&lt;unstranded OR forward OR reverse \\&gt;\nNA\nunstranded\n\n\ncondition\nVariable of interest of the experiment, such as \"control\", \"treatment\", etc\nwordWord\ncamelCase\ncontrol, treat1, treat2\n\n\ncell_type\nThe cell type(s) known or selected to be present in the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\ntissue\nThe tissue from which the sample was taken\nNA\nUberon\nNA\n\n\nsex\nThe biological/genetic sex of the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\ncell_line\nCell line of the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\norganism\nOrganism origin of the sample\n&lt;Genus species&gt;\nTaxonomy\nMus musculus\n\n\nreplicate\nReplicate number\n&lt;integer\\&gt;\nNA\n1\n\n\nbatch\nBatch information\nwordWord\ncamelCase\n1\n\n\ndisease\nAny diseases that may affect the sample\nNA\nDisease Ontology or MONDO\nNA\n\n\ndevelopmental_stage\nThe developmental stage of the sample\nNA\nNA\nNA\n\n\nsample_type\nThe type of the collected specimen, eg tissue biopsy, blood draw or throat swab\nNA\nNA\nNA\n\n\nstrain\nStrain of the species from which the sample was collected, if applicable\nNA\nontology field - e.g. NCBITaxonomy\nNA\n\n\ngenetic variation\nAny relevant genetic differences from the specimen or sample to the expected genomic information for this species, eg abnormal chromosome counts, major translocations or indels\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n\nAssay metadata fields\nHere you will find a table with possible metadata fields that you can use to annotate and track your Assay folders:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nassay_ID\nIdentifier for the assay that is at least unique within the project\n&lt;Assay-ID\\&gt;_&lt;keyword\\&gt;_YYYYMMDD\nNA\nCHIP_Oct4_20200101\n\n\nassay_type\nThe type of experiment performed, eg ATAC-seq or seqFISH\nNA\nontology field- e.g. EFO or OBI\nChIPseq\n\n\nassay_subtype\nMore specific type or assay like bulk nascent RNAseq or single cell ATACseq\nNA\nontology field- e.g. EFO or OBI\nbulk ChIPseq\n\n\nowner\nOwner of the assay (who made the experiment?).\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nJose Romero\n\n\nplatform\nThe type of instrument used to perform the assay, eg Illumina HiSeq 4000 or Fluidigm C1 microfluidics platform\nNA\nontology field- e.g. EFO or OBI\nIllumina\n\n\nextraction_method\nTechnique used to extract the nucleic acid from the cell\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nlibrary_method\nTechnique used to amplify a cDNA library\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nexternal_accessions\nAccession numbers from external resources to which assay or protocol information was submitted\nNA\neg protocols.io, AE, GEO accession number, etc\nGSEXXXXX\n\n\nkeyword\nKeyword for easy identification\nwordWord\ncamelCase\nOct4ChIP\n\n\ndate\nDate of assay creation\nYYYYMMDD\nNA\n20200101\n\n\nnsamples\nNumber of samples analyzed in this assay\n&lt;integer\\&gt;\nNA\n9\n\n\nis_paired\nPaired fastq files or not\n&lt;single OR paired\\&gt;\nNA\nsingle\n\n\npipeline\nPipeline used to process data and version\nNA\nNA\nnf-core/chipseq -r 1.0\n\n\nstrandedness\nThe strandedness of the cDNA library\n&lt;+ OR - OR *\\&gt;\nNA\n*\n\n\nprocessed_by\nWho processed the data\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nSarah Lundregan\n\n\norganism\nOrganism origin\n&lt;Genus species\\&gt;\nTaxonomy name\nMus musculus\n\n\norigin\nIs internal or external (from a public resources) data\n&lt;internal OR external\\&gt;\nNA\ninternal\n\n\npath\nPath to files\n&lt;/path/to/file\\&gt;\nNA\nNA\n\n\nshort_desc\nShort description of the assay\nplain text\nNA\nOct4 ChIP after pERK activation\n\n\nELN_ID\nID of the experiment/assay in your Electronic Lab Notebook software, like labguru or benchling\nplain text\nNA\nNA\n\n\n\n\n\n\n\n\nThe metadata must include key details such as the project‚Äôs short description, author information, creation date, experimental protocol, assay ID, assay type, platform utilized, library details, keywords, sample count, paired-end status, processor information, organism studied, sample origin, and file path.\nIf you would create a database from the metadata files, your table should look like this (each row corresponding to one project):\n\n\n\n\n\n\n\n\n\nassay_ID\nassay_type\nassay_subtype\nowner\nplatform\nextraction_method\nlibrary_method\nexternal_accessions\nkeyword\ndate\nnsamples\nis_paired\npipeline\nstrandedness\nprocessed_by\norganism\norigin\npath\nshort_desc\nELN_ID\n\n\n\n\nRNA_oct4_20200101\nRNAseq\nbulk RNAseq\nSarah Lundregan\nNextSeq 2000\nNA\nNA\nNA\noct4\n20200101\n9\npaired\nnf-core/chipseq 2.3.1\n*\nSL\nMus musculus\ninternal\nNA\nBulk RNAseq of Oct4 knockout\n234\n\n\nCHIP_oct4_20200101\nChIPseq\nbulk ChIPseq\nJose Romero\nNextSeq 2000\nNA\nNA\nNA\noct4\n20200101\n9\nsingle\nnf-core/rnaseq 3.12.0\n*\nJARH\nMus musculus\ninternal\nNA\nBulk ChIPseq of Oct4 overexpression\n123\n\n\nCHIP_med1_20190204\nChIPseq\nbulk ChIPseq\nMartin Proks\nNextSeq 2000\nNA\nNA\nNA\nmed1\n20190204\n12\nsingle\nnf-core/rnaseq 3.12.0\n*\nMP\nMus musculus\ninternal\nNA\nBulk ChIPseq of Med1 overexpression\n345\n\n\nSCR_humanSkin_20210302\nRNAseq\nsingle cell RNAseq\nJose Romero\nNextSeq 2000\nNA\nNA\nNA\nhumanSkin\n20210302\n23123\npaired\nnf-core/scrnaseq 1.8.2\n*\nJARH\nHomo sapiens\nexternal\nNA\nscRNAseq analysis of human skin development\nNA\n\n\nSCR_humanBrain_20220610\nRNAseq\nsingle cell RNAseq\nMartin Proks\nNextSeq 2000\nNA\nNA\nNA\nhumanBrain\n20220610\n1234\npaired\ncustom\n*\nMP\nHomo sapiens\nexternal\nNA\nscRNAseq analysis of human brain development\nNA\n\n\n\n\n\n\n\n\n\n\nSources\n\nTranscriptomics metadata standards and fields\nBiological ontologies for data scientists,Bionty\n\n\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "Use cases",
      "NGS data",
      "NGS Assay and Project metadata"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#database-and-data-catalogs",
    "href": "develop/04_metadata.html#database-and-data-catalogs",
    "title": "4. Documentation for biodata",
    "section": "Database and data catalogs",
    "text": "Database and data catalogs\nMetadata can be used to create data catalogs, particularly beneficial for the efficient organization of experimental or sequencing data generated by researchers. While databases can range from simple tabular formats like Excel to sophisticated DataBase Management Systems (DBMS) like SQLite, the choice depends on factors such as complexity and volume of data. Leveraging a DBMS offers advantages like efficient data storage, enhanced security, and rapid data querying capabilities.\n\nTables as databases\nA browsable table can be created by recursively navigating through a project‚Äôs folder hierarchy using a script and generating a TSV file (tab-separated values) named, for example, database_YYYYMMDD.tsv. This table acts as a centralized repository for all project data, simplifying access and organization. Consistency in metadata structure across projects is vital for efficient data management and integration, as it aids in tracking all conducted assays. Adhering to a uniform metadata format enables the seamless inclusion of essential information from YAML files into the browsable table.\n\n\n\n\n\n\nExercise 2: Generate database tables from metadata\n\n\n\n\n\n\n\nWrite a script (R or Python) that recursively fetches metadata.yml files in a given path. It is important that each subdirectory contains its corresponding metadata.yml.\nRequirements:\n\nData folder structure: containing all project folders\nYAML metadata files associated with each project\n\nClick on the hint to reveal the solution and a code example for the exercise, which may serve as inspiration.\nYou can find a thorough guided exercise in the practical material - Exercise 4.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n# Load required packages\npackages &lt;- c(\"yaml\", \"ggplot2\", \"lubridate\")\n\n# Function to recursively fetch YAML files files, read and convert them to a data frame\n\ndf = lapply(file_list, yaml::yaml.load_file)\n\n# Save the data frame as a TSV file\n\n\n\n\n\n\n\n\n\n\n\n\nSQLite database\nAn alternative to the tabular format is SQLite, a lightweight and self-contained relational database management system known for its simplicity and efficiency. SQLite operates without the need for a separate server, making it ideal for scenarios requiring minimal resource usage. It excels in tasks involving structured data storage and retrieval, making it suitable for managing experiment metadata. Similar to the previous example, you can use a script that records all the information from the YAML file in a SQLite database.\n\n\n\n\n\n\nAdvantages of using SQLite database\n\n\n\n\nEfficient Querying: SQLite databases optimize querying and data retrieval, enabling fast and efficient extraction of specific information.\nStructured Organization: Databases provide structured and organized data storage, ensuring easy access and maintenance.\nData Integrity: SQLite databases enforce data integrity through constraints and validations, minimizing errors and inconsistencies.\nConcurrency and Multi-User Support: SQLite supports concurrent read access from multiple users, ensuring accessibility without compromising data integrity.\nScalability: It can handle growing volumes of data without significant performance degradation.\nModularity and Portability: Databases are self-contained and modular, simplifying data distribution and portability.\nSecurity and Access Control: SQLite offers security features like password protection and encryption, with granular control over user access.\nIndexing: Support for indexing accelerates data retrieval based on specific columns, particularly beneficial for large datasets.\nData Relationships: Databases allow for the establishment of relationships between tables, facilitating storage of interconnected data, such as project, assay, and sample information.\n\n\n\n\n\n\n\n\n\nExercise 3: Generate a SQLite database from metadata\n\n\n\n\n\n\n\nClick on the hint to reveal the necessary libraries and some functions, which may serve as inspiration.\nYou can find a thorough guided exercise, complete with code example, in the practical material - Exercise 4, option B.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n# Load required packages\npackages &lt;- c(\"yaml\", \"ggplot2\", \"lubridate\", \"DBI\")\n\n# Function to recursively fetch YAML files files, read and convert them to a data frame\n\ndf = lapply(file_list, yaml::yaml.load_file)\n\n# Create an SQLite database from a dataframe and insert data\ndbConnect(SQLite(), \"filenameXXX.sqlite\")\ndbWriteTable()\n\n\n\n\n\n\n\n\n\n\n\n\nCatalog browser\nTo further optimize the use of your metadata and improve the integration of all your lab metadata, you can design a user-friendly catalog browser for your database using tools like Rshiny or Panel. These frameworks provide interfaces for dynamic search, filtering, and visualization, facilitating efficient exploration of database contents.\nCreating such a tool with RShiny is straightforward and does not require extensive development knowledge, whether using a TSV file or a SQLite database. In the practical materials, we demonstrate both scenarios and showcase various functionalities for inspiration. SQLite files are particularly advantageous for data fetching and other operations due to their efficient querying and indexing capabilities.\nHere‚Äôs an example of an SQLite database catalog created by the Brickman Lab at the Center for Stem Cell Medicine. It‚Äôs simple yet effective! Clicking on a data row opens the metadata.yml file, allowing access to detailed metadata for that assay.\n\n\nVideo\ntype:video\n\n\n\n\n\n\n\n\nExercise 4: Create your first catalog browser using Rshiny\n\n\n\n\n\n\n\nGo to the practical material for complete exercise instructions and solutions. The code provided can serve as inspiration for you to adapt as needed.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nThese are some of the libraries required: install.packages(c(\"shiny\", \"DT\", \"DBI\"))\nYou need to define both a user interface (UI) and a server function. The UI (fluidPage()) outlines the app‚Äôs layout using for example, the sidebarLayout() and mainPanel() functions for input controls and output displays.\nThe server function manages data manipulation and user interactions. Use shinyApp() to launch the app once the UI and server are set up.\nHere is a simple example of a server function settup including the main parts (additional components provide advanced functionalities):\n  server &lt;- function(input, output, session) {\n    # Define a reactive expression for data based on user inputs\n    data &lt;- reactive({\n        req(input$dataInput)  # Ensure data input is available\n        # Load or manipulate data here\n    })\n\n    # Define an output table based on data\n    output$dataTable &lt;- renderTable({\n        data()  # Render the data as a table\n    })\n\n    # Observe a button click event and perform an action\n    observeEvent(input$actionButton, {\n        # Perform an action when the button is clicked\n    })\n\n    # Define cleanup tasks when the app stops\n    onStop(function() {\n        # Close connections or save state if necessary\n    })\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5: Add complex features to your catalog browser\n\n\n\n\n\n\n\nOnce you‚Äôve finished the previous exercise, consider implementing these additional ideas to maximize the utility of your catalog browser.\n\nAdd a functionality to only select certain columns uiOutput(\"column_select\")\nAdd buttons to order numeric columns ascending or descending using radioButtons()\nUse SQL aggregation functions (e.g., SUM, COUNT, AVG) to perform custom data summaries and calculations.\nAdd a tab tabPanel() to create a project directory interactively (and fill up the metadata fields), tips: dir.create(), data.frame(), write.table()\nModify existing entries\nVisualize results using Cirrocumulus, an interactive visualization tool for large-scale single-cell genomics data.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nExplore this example with advanced features such as a two-tab layout, filtering by numeric values and matching strings, and a color-customized dashboard here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nFor R Enthusiasts Explore demosfrom the R Shiny community to kickstart your projects or for inspiration.\nFor python Enthusiasts If you want to dive deeper into Shiny apps and their various uses (such as dynamic plots or other interactive widgets), Shiny for Python provides live, interactive code throughout its entire tutorial. Additionally, it offers a great tool called Playground, where you can code and test your own app to explore how different features render.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Documentation for biodata"
    ]
  },
  {
    "objectID": "develop/05_VC.html#from-project-folders-to-git-repositories",
    "href": "develop/05_VC.html#from-project-folders-to-git-repositories",
    "title": "Data Analysis with Version Control",
    "section": "",
    "text": "Moving from Git to GitHub involves transitioning from a local version control setup to a remote hosting platform. You will need a GitHub account for exercise in this section.\n\n\n\n\n\n\nCreate a Github account\n\n\n\nIf you don‚Äôt have a Github account yet, click here.\n\n\nYou have two options when it comes to creating a repository for your project. First, you can start from scratch by creating a new repository and adding files to it as your project progresses. Alternatively, if you already have an existing folder structure for your project, you can initialize a repository directly from that folder. It is crucial to initiate version control in the early stages of a project to facilitate easy tracking of changes and effective management of the project‚Äôs version history from the beginning.\n\n\nIf you successfully completed all the exercises in lesson 3, you should have a project data structure prepared. Otherwise, consider using one of your existing projects or creating a small toy example for practice using cookiecutter (see practical_workshop).\n\n\n\n\n\n\nGithub documentation link\n\n\n\n\nAdding locally hosted code to Github\n\n\n\n\n\n\n\n\n\nExercise 1: initialize a repository from an existing folder:\n\n\n\n\n\n\n\n\nFirst, initialize the repository using the command git init. This command is run only once, even in collaborative projects (git init).\nOnce the repository is initialized, create a remote repository on GitHub.\nAdd the remote URL to your local git repository using git remote add origin &lt;URL&gt;`. This associates the remote URL with the name ‚Äúorigin‚Äù.\nEnsure you have at least one commit in your history by staging existing files with git add and then creating a snapshot, known as committing, with git commit.\nFinally, push your local commits to the remote repository and establish a tracking relationship using git push -u origin master.\n\n\n\n\n\n\n\n\n\nAlternatively to converting folders to repositories, you can create a new repository remotely, and then clone (git clone) it locally. Here, git init is not needed. You can move the files into the repository locally (git add, git commit, and git push).\nIf the repository already exists on a remote, you would choose to git clone and not git init. On the other hand, if you create a remote repository first with the intent of moving your project to it later, you may have a few other steps to follow. If there are no commits in the remote repository, you can follow the steps above for git init. If there are commits and files in the remote repository but you would still like it to contain your project files, git clone that repository. Then, move the project‚Äôs files into that cloned repository. git add, git commit, and git push to create a history that makes sense for the beginning of your project. Then, your team can interact with the repository without git init again.\n!!! tip ‚ÄúTips to write good commit messages‚Äù\nIf you would like to know more about Git commits and the best way to make clear git messages, check out [this post](https://www.conventionalcommits.org/en/v1.0.0/)!",
    "crumbs": [
      "Course material",
      "Key practices",
      "Data Analysis with Version Control"
    ]
  },
  {
    "objectID": "develop/07_repos.html#data-repositories-and-archives",
    "href": "develop/07_repos.html#data-repositories-and-archives",
    "title": "7. Storing and sharing biodata",
    "section": "Data Repositories and Archives",
    "text": "Data Repositories and Archives\nSpecialized repositories and archives securely store, curate, and disseminate scientific data, ensuring long-term preservation, transparency, and citability of research findings through standardized formats and rigorous curation processes.\nImportance of archiving scientific data\n\nLong-term accessibility and preservation: Ensures data remains accessible for future researchers.\nEnhanced visibility and attribution: Unique identifiers like DOIs enable citation of datasets, enhancing visibility and proper attribution.\nImproved dataset discoverability and interpretability: Comprehensive metadata, including methodology and experimental details, facilitates understanding and usability by other researchers.\nPromotion of Transparency, Reproducibility, and Research Integrity: Mandatory data deposition fosters transparency and upholds research integrity.\nAmplification of Research Impact and Contribution: Archiving data elevates research quality and extends its impact within the scientific community.\nFulfilling Scholarly Obligations: Compliance with requirements set by scientific journals and funding agencies ensures adherence to scholarly standards.\n\n\n\n\n\n\n\nre3data.org\n\n\n\nCheck the registry of research data repositories‚Äìre3data.org for a full overview. You can browse by subject if you are looking within a specific field.\n\n\nThere are two types of repositories:\n\nGeneral repositories: relevant to a wide range of disciplines (e.g.¬†Zenodo).\nDomain-specific: repositories are customized for specific fields, providing specialized curation and context-specific features (e.g.¬†ENA, GEO, Annotare, etc.)\n\n\n\n\n\n\n\nList of repositories for biological data\n\n\n\n\nEuropean Nucleotide Archive (ENA)\nNCBI Gene Expression Omnibus (GEO)\nSequence Read Archive (SRA)\nProtein Data Bank (PDB)\nProteomics Identifications Database (PRIDE)\nUniversal Protein Resource (UniProt) SPIN\nArrayExpress\nEBI Metagenomics (MGnify)\nPhysioNet\nFunctional Annotation of Animal Genomes (FAANG) Data Repository\n\nFor more data repositories, please refer to the links provided below to find the appropriate repository for your data:\n\nEMBL-EBI data resources here\nNHI data resources here\nELIXIR Core Data Resources here\n\nYour institution might as well have its repositories such as ERDA (Electronic research data archive at the University of Copenhagen).",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Storing and sharing biodata"
    ]
  },
  {
    "objectID": "develop/07_repos.html#domain-specific-repositories",
    "href": "develop/07_repos.html#domain-specific-repositories",
    "title": "7. Storing and sharing biodata",
    "section": "Domain-specific repositories",
    "text": "Domain-specific repositories\nThis tailored approach ensures alignment with standards and maximizes the utility and impact of research findings. By catering to particular research areas, these repositories offer researchers a more focused audience, deeper domain expertise, and increased visibility within their specific research community.\nExplore some examples of NGS data repositories below:\n\n\n\n\n\n\nENA (European Nucleotide Archive)\n\n\n\n\n\n\n\nENA: hosted by the European Bioinformatics Institute (EBI), provides researchers with a platform to deposit and access nucleotide sequences along with associated metadata, ensuring data preservation and contextualization. ENA adheres to community standards and guidelines for data submission, including those established by the International Nucleotide Sequence Database Collaboration (INSDC).\n\n\n\n\n\n\n\n\n\n\n\nGEO (Gene Expression Omnibus)\n\n\n\n\n\n\n\nGEO (Gene Expression Omnibus): curated by the National Center for Biotechnology Information (NCBI), serves as a specialized repository for high-throughput functional genomic data sets, particularly gene expression data across diverse biological conditions and experimental designs. Researchers can easily deposit and access a variety of genomic data, fostering data transparency and reproducibility within the scientific community. GEO assigns unique accession numbers to each dataset, ensuring traceability and proper citation in research publications.\n\n\n\n\n\n\n\n\n\n\n\nAnnotare\n\n\n\n\n\n\n\nArrayExpress/Annotare: hosted by the European Bioinformatics Institute (EBI), is a specialized repository tailored for storing and submitting functional genomics experiments for high-throughput sequencing data. It offers researchers a platform to upload experimental data along with comprehensive metadata ensuring preservation and contextualization. Annotare provides a curated environment aligned with the standards and practices of the field. This specialization enhances data discoverability, promotes collaboration, and facilitates deeper insights into the functional aspects of the genome.\n\n\n\n\n\nThe repositories mentioned earlier adhere to established community standards for data submission and sharing in genomics research such as:\n\nMIAME (Minimum Information About a Microarray Experiment): These guidelines ensure comprehensive and standardized reporting of microarray experiments.\nMIxS (Minimum Information about a high-throughput SeQuencing Experiment): MIxS standards, developed by the Genomic Standards Consortium, ensure consistent reporting of metadata for high-throughput sequencing experiments.\nSequence Read Archive (SRA) Submission Guidelines: They include requirements for data formatting, metadata inclusion, and quality control.\nCommunity-Specific Standards designed to ensure that submitted data meets the specific requirements and expectations of the field.\n\nBy adhering to standards, repositories ensure that submitted data is high quality, well-documented, and compliant with community best practices, promoting data discovery, reproducibility, and interoperability within the scientific community.\nFollowing all the recommendations in this course makes it straightforward to provide the necessary documentation and information for these repositories. For instance, repositories specific to NGS data will require the raw FASTQ files, sample metadata, and protocols as well as final pre-processing results (for instance, read count matrices in BED files).\n\n\n\n\n\n\nWarning\n\n\n\nKeep in mind that these repositories are not intended for downstream analysis data and associated code. However, you should already have those versions controlled by GitHub, which eliminates any concerns. You can then archive such repositories in a general repository like Zenodo.\nArchives for software source code are essential for long-term accessibility and reproducibility and are becoming very popular. Check Software Heritage if you are developing software.",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Storing and sharing biodata"
    ]
  },
  {
    "objectID": "develop/07_repos.html#general-repositories",
    "href": "develop/07_repos.html#general-repositories",
    "title": "7. Storing and sharing biodata",
    "section": "General repositories",
    "text": "General repositories\nThere are plenty of data archiving repositories. We recommend to check the Longwood Research Data management website at Harvard for a quick overview. Some of the most well-known are:\n\nDataverse\nDryad\nfigshare\nOpen Science Framework (OSF)\nZenodo\n\nWe will be using Zenodo for our practical workshop. However, please review the table provided by the Longwood, Harvard Biomedical Data Management team, which outlines the differences between various repositories.\n\n\n\nClick to enlarge\n\n\n\nZenodo\nZenodo is one of the widely used repositories for a variety of research outputs. It is an open-access digital platform supported by the European Organization for Nuclear Research (CERN) and the European Commission. It caters to various research outputs, including datasets, papers, software, and multimedia files, making it a valuable resource for researchers worldwide. With its user-friendly platform, researchers can easily upload, share, and preserve their research data. Each deposited item receives a unique Digital Object Identifier (DOI), ensuring citability and long-term accessibility. Additionally, Zenodo offers robust metadata capabilities for enriching submissions with contextual information. Moreover, researchers can link their GitHub accounts to Zenodo, simplifying the process of archiving the GitHub repository releases for long-term accessibility and citation.\nOnce your accounts are linked, creating a Zenodo archive becomes as straightforward as tagging a release in your GitHub repository. Zenodo automatically detects the release and generates a corresponding archive, complete with a unique Digital Object Identifier (DOI) for citable reference. Therefore, before submitting your work to a journal, link your data analysis repository to Zenodo, obtain a DOI, and cite it in your manuscript which enhances reproducibility in research.\n\nStep-by-Step Setup Guide\nCheck the practical material where we demonstrate how to link Zenodo and Github (see Exercise 6 in the practical material).",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Storing and sharing biodata"
    ]
  },
  {
    "objectID": "develop/06_pipelines.html#wrap-up",
    "href": "develop/06_pipelines.html#wrap-up",
    "title": "6. Processing and analyzing biodata",
    "section": "Wrap up",
    "text": "Wrap up\nThis lesson emphasized the importance of reproducibility in computational research and provided practical techniques for achieving it. Using annotated notebooks, pipeline frameworks, and community-curated pipelines, such as those developed by the nf-core community, enhances reproducibility and readability.\n\nSources\n\nThe turing way - reproducible research\nRDMkit, Elixir Data Management - Data Analysis\nCode documentation by Johns Hopkins Sheridan libraries. This link includes best practices for code documentation, style guides, R markdown, Jupyter Notebook, version control, and code repository.\nGuide to reproducible code in ecology and evolution\nBest practices for Scientific computing\nElixir Software Best Practices\nfaircookbook workflows\nAtlassian software development tutorial",
    "crumbs": [
      "Course material",
      "Key practices",
      "6. Processing and analyzing biodata"
    ]
  },
  {
    "objectID": "index.html#research-data-management-for-biological-data",
    "href": "index.html#research-data-management-for-biological-data",
    "title": "Welcome to RDM for biodata",
    "section": "Research Data Management for biological data",
    "text": "Research Data Management for biological data\nThe course ‚ÄúResearch Data Management (RDM) for biological data‚Äù is designed to provide participants with foundational knowledge and practical skills in handling the extensive data generated by modern studies, with a focus on Next Generation Sequencing (NGS) data. It emphasizes the importance of Open Science and FAIR principles in managing data effectively. This course covers essential principles and best practices guidelines in data organization, metadata annotation, version control, and data preservation. These principles are explored from a computational perspective, ensuring participants gain hands-on experience in applying them to real-world scenarios in their research labs. Additionally, the course delves into FAIR principles and Open Science, promoting collaboration and reproducibility in research endeavors. By the course‚Äôs conclusion, attendees will possess essential tools and techniques to address the data challenges prevalent in today‚Äôs NGS research landscape, as well as in other related fields to health and bioinformatics.\n\n\n\n\n\n\nCourse Overview\n\n\n\n\nüìñ Syllabus:\n\n\nData Lifecycle Management\nData Management Plans (DMPs)\nData Organization and storage\nDocumentation standards for biodata\nVersion Control and Collaboration\nProcessing and analyzing biodata\nStoring and sharing biodata\n\n\n‚è∞ Total Time Estimation: X hours\n\nüìÅ Supporting Materials:\n\nüë®‚Äçüíª Target Audience: Ph.D., MSc, anyone interested in RDM for NGS data or other related fields within bioinformatics.\nüë©‚Äçüéì Level: Beginner.\nüîí License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\nüí∞ Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).\n\n\n\n\n\n\n\n\n\n\nCourse Requirements\n\n\n\n\nBasic understanding Next Generation Sequencing data and formats.\nCommand Line experience\nBasic programming experience\nQuarto or Mkdocs tools\n\n\n\nThis course offers participants with an in-depth introduction to effectively managing the vast amounts of data generated in modern studies. Throughout the program, emphasis is placed on practical understanding of RDM principles and the importance of efficient handling of large datasets. In this context, participants will learn the necessity of adopting Open Science and FAIR principles for enhancing data accessibility and reusability.\nParticipants will acquire practical skills for organizing data, including the creation of folder and file structures, and the implementation of metadata to facilitate data discoverability and interpretation. Special attention is given to the development of Data Management Plans (DMPs) with examples tailored to omics data, ensuring compliance with institutional and funding agency requirements while maintaining data integrity. Attendees will also gain insights into the establishment of simple databases and the use of version control systems to track changes in data analysis, thereby promoting collaboration and reproducibility.\nThe course concludes with a focus on archiving and data repositories, enabling participants to learn strategies for preserving and sharing data for long-term scientific usage. By the end of the course, attendees will be equipped with essential tools and techniques to effectively navigate the challenges prevalent in today‚Äôs research landscape. This will not only foster successful data management practices but also enhance collaboration within the scientific community.\n\n\n\n\n\n\nCourse Goals\n\n\n\nBy the end of this workshop, you should be able to apply the following concepts in the context of Next Generation Sequencing data:\n\nUnderstand the Importance of Research Data Management (RDM)\nFamiliarize Yourself with FAIR and Open Science Principles\nDraft a Data Management Plan for your own Data\nEstablish File and Folder Naming Conventions\nEnhance Data with Descriptive Metadata\nImplement Version Control for Data Analysis\nSelect an Appropriate Repository for Data Archiving\nMake your data analysis and workflows reproducible and FAIR\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is a computational workshop that focuses primarily on the digital aspect of our data. While wet lab Research Data Management (RDM) involving protocols, instruments, reagents, ELM or LIMS systems is integral to the entire RDM process, it won‚Äôt be covered in this course.\nAs part of effective data management, it‚Äôs crucial to prioritize strategies that ensure security and privacy. While these aspects are important, please note that they won‚Äôt be covered in our course. However, we highly recommend enrolling in the GDPR course offered by Center for Health Data Science, specially if you‚Äôre working with sensitive data. This course specifically focuses on GDPR compliance and will provide you with valuable insights and skills in managing data privacy and security.\n\n\n\nDanish institutional RDM links\n\nUniversity of Copenhagen\nUniversity Library of Southern Denmark\nTechnical University of Denmark\nAalborg University\nAarhus University\n\n\n\nAcknowledgements\n\nRDMkit, ELIXIR (2021) Research Data Management Kit. A deliverable from the EU-funded ELIXIR-CONVERGE project (grant agreement 871075).\nUniversity of Copenhagen Research Data Management Team.\nMartin Proks and Sarah Lundregan, Brickman Lab, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nRichard Dennis, Data Steward, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nNBISweden."
  },
  {
    "objectID": "practical_workflows.html#snakemake",
    "href": "practical_workflows.html#snakemake",
    "title": "FAIR computational pipelines and environments",
    "section": "Snakemake",
    "text": "Snakemake\nIt is a text-based tool using python-based language plus domain specific syntax. The workflow is decompose into rules that are define to obtain output files from input files. It infers dependencies and the execution order.\n\nBasics\n\nSemantics: define rules\nGeneralise the rule: creating wildcards You can refer by index or by name\nDependencies are determined top-down\n\nFor a given target, a rule that can be applied to create it, is determined (a job) For the input files of the rule, go on recursively, If no target is specified, snakemake, tries to apply the first rule\n\nRule all: target rule that collects results\n\n\n\nJob execution\nA job is executed if and only if: - otuput file is target and does not exist - output file needed by another executed job and does not exist - input file newer than output file - input file will be updated by other job (eg. changes in rules) - execution is force (‚Äò‚Äìforce-all‚Äô)\nYou can plot the DAG (directed acyclic graph) of the jobs\n\n\nUseful command line interface\n# dry-run (-n), print shell commands (-p)\nsnakemake -n -p\n# Snakefile named different in another location \nsnakemake --snakefile path/to/file.smoker\n# dry-run (-n), print execution reason for each job\nsnakemake -n -r\n# Visualise DAG of jobs using Graphviz dot command\nsnakemake --dag | dot -Tsvg &gt; dag.svg\n\n\nDefining resources\nrule myrule:\n  resources: mem_mb= 100 #(100MB memory allocation)\n  threads: X\n  shell:\n    \"command {threads}\"\nLet‚Äôs say you defined our rule myrule needs 4 works, if we execute the workflow with 8 cores as follows:\nsnakemake --cores 8\nThis means that 2 ‚Äòmyrule‚Äô jobs, will be executed in parallel.\nThe jobs are schedules to maximize parallelization, high priority jobs will be scheduled first, all while satisfying resource constrains. This means:\nIf we allocate 100MB for the execution of ‚Äòmyrule‚Äô and we call snakemake as follows:\nsnakemake --resources mem_mb=100 --cores 8\nOnly one ‚Äòmyrule‚Äô job can be executed in parallel (you do not provide enough memory resources for 2). The memory resources is useful for jobs that are heavy memory demanding to avoid running out of memory. You will need to benchmark your pipeline to estimate how much memory and time your full workflow will take. We highly recommend doing so, get a subset of your dataset and give it a go! Log files will come very handy for the resource estimation. Of course, the execution of jobs is dependant on the free resources availability (eg. CPU cores).\nrule myrule:\n  log: \"logs/myrule.log\"\n  threads: X\n  shell:\n    \"command {threads}\"\nLog files need to define the same wildcards as the output files, otherwise, you will get an error.\n\n\nConfig files\nYou can also define values for wildcards or parameters in the config file. This is recommended when the pipeline might be used several times at different time points, to avoid unwanted modifications to the workflow. parameterization is key for such cases.\n\n\nCluster execution\nWhen working from cluster systems you can execute the workflow using -qsub submission command\nsnakemake --cluster qsub \n\n\nAdditional advanced features\n\nmodularization\nhandling temporary and protected files: very important for intermediate files that filled up our memory and are not used in the long run and can be deleted once the final output is generated. This is automatically done by snakemake if you defined them in your pipeline HTML5 reports\nrule parameters\ntracking tool versions and code changes: will force rerunning older jobs when code and software are modified/updated.\ndata provenance information per file\npython API for embedding snakemake in other tools\n\n\n\nCreate an isolated environment to install dependencies\nBasic file structure\n| - config.yml\n| - requirements.txt (commonly also named environment.txt)\n| - rules/\n|   | - myrules.smk\n| - scripts/\n|   | - script1.py\n| - Snakefile\nCreate conda environment, one per project!\n# create env\nconda create -n myworklow --file requirements.txt\n# activate environment\nsource activate myworkflow\n# then execute snakemake\nUse git repositories to save your projects and pipelines!"
  },
  {
    "objectID": "practical_workflows.html#sources",
    "href": "practical_workflows.html#sources",
    "title": "FAIR computational pipelines and environments",
    "section": "Sources",
    "text": "Sources\n\nSnakemake tutorial\nSnakemake turorial slides by Johannes Koster\nhttps://bioconda.github.io\nK√∂ster, Johannes and Rahmann, Sven. ‚ÄúSnakemake - A scalable bioinformatics workflow engine‚Äù. Bioinformatics 2012.\nK√∂ster, Johannes. ‚ÄúParallelization, Scalability, and Reproducibility in Next-Generation Sequencing Analysis‚Äù, PhD thesis, TU Dortmund 2014.\nfaircookbook worflows\n\nRegister and publish your scientific computational workflow on WorkflowHub"
  },
  {
    "objectID": "develop/practical_workshop.html#create-a-catalog-of-your-assay-folder",
    "href": "develop/practical_workshop.html#create-a-catalog-of-your-assay-folder",
    "title": "Practical material",
    "section": "4. Create a catalog of your assay folder",
    "text": "4. Create a catalog of your assay folder\nThe next step is to collect all the NGS datasets that you have created in the manner explained above. Since your folders all should contain the metadata.yml file in the same place with the same metadata, it should be very easy to iteratively go through all the folders and merge all the metadata.yml files into a one single table. This table can be then browsed easily with Microsoft Excel, for example. If you are interested in making a Shiny app or Python Panel tool to interactively browse the catalog, check out this lesson.\n\n\n\n\n\n\nExercise 4: create a metadata.tsv catalog\n\n\n\n\n\n\n\nWe will make a small script in R (or you can make one with Python) that recursively goes through all the folders inside an input path (like your Assays folder), fetches all the metadata.yml files, and merges them. Finally, it will write a TSV file as an output.\n\nCreate a folder called Assays\nUnder that folder, make three new Assay folders from your cookiecutter template\nRun the script below with R (or create your own with Python). Modify the folder_path variable so it matches the path to the folder Assays. The table will be written under the same folder_path.\nVisualize your Assays table with Excel\n\n\nlibrary(yaml)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Function to recursively fetch metadata.yml files\nget_metadata &lt;- function(folder_path) {\n    file_list &lt;- list.files(path = folder_path, pattern = \"metadata\\\\.yml$\", recursive = TRUE, full.names = TRUE)\n    metadata_list &lt;- lapply(file_list, yaml::yaml.load_file)\n    return(metadata_list)\n    }\n\n# Specify the folder path\n    folder_path &lt;- \"/path/to/your/folder\"\n\n    # Fetch metadata from the specified folder\n    metadata &lt;- get_metadata(folder_path)\n\n    # Convert metadata to a data frame\n    metadata_df &lt;- data.frame(matrix(unlist(metadata), ncol = length(metadata), byrow = TRUE))\n    colnames(metadata_df) &lt;- names(metadata[[1]])\n\n    # Save the data frame as a TSV file\n    output_file &lt;- paste0(\"database_\", format(Sys.Date(), \"%Y%m%d\"), \".tsv\")\n    write.table(metadata_df, file = output_file, sep = \"\\t\", quote = FALSE, row.names = FALSE)\n\n    # Print confirmation message\n    cat(\"Database saved as\", output_file, \"\\n\")"
  },
  {
    "objectID": "develop/06_pipelines.html#code-and-pipelines-for-data-analysis",
    "href": "develop/06_pipelines.html#code-and-pipelines-for-data-analysis",
    "title": "6. Processing and analyzing biodata",
    "section": "",
    "text": "In this section, we explore essential elements of reproducibility and efficiency in computational research, highlighting techniques and tools for creating robust and transparent coding and workflows. By prioritizing reproducibility and replicability, researchers can enhance the credibility and impact of their findings while fostering collaboration and knowledge dissemination within the scientific community.\n\n\n\n\n\n\nBefore you start‚Ä¶\n\n\n\n\nChoose a folder structure (e.g., using cookiecutter)\nChoose a file naming system\nAdd a README describing the project (and the naming conventions)\nInstall and set up version control (e.g., Git and Github)\nChoose a coding style!\n\n\nPython: Python‚Äôs PEP or Google‚Äôs style guide\nR: Google‚Äôs style guide or Tidyverse‚Äôs style guide\n\n\n\n\n\nThrough techniques such as scripting, containerization (e.g., Docker), and virtual environments, researchers can create reproducible analyses that enable others to validate and build upon their work. Emphasizing the documentation of data processing steps, parameters, and results ensures transparency and accountability in research outputs. To write clear and reproducible code, take the following approach: write functions, code defensively (such as input validation, error handling, etc.), add comments, conduct testing, and maintain proper documentation.\nTools for reproducibility:\n\nCode notebooks: Utilize tools like Jupyter Notebook and R Markdown to combine code with descriptive text and visualizations, enhancing data documentation.\n\nIntegrated development environments: Consider using platforms such as (knitr or MLflow) to streamline code development and documentation processes.\nPipeline frameworks or workflow management systems: Implement systems like Nextflow and Snakemake to automate data analysis steps (including data extraction, transformation, validation, visualization, and more). Additionally, they contribute to ensuring interoperability by facilitating seamless integration and interaction between different components or stages.\n\n\n\nComputational notebooks (e.g., Jupyter, R Markdown) provide researchers with a versatile platform for exploratory and interactive data analysis. These notebooks facilitate sharing insights with collaborators and documentation of analysis procedures.\n\n\n\nTools such as Nextflow and Snakemake streamline and automate various data analysis steps, enabling parallel processing and seamless integration with existing tools. Remember to create portable code and use relative paths to ensure transferability between users.\n\nNextflow: offers scalable and portable NGS data analysis pipelines, facilitating data processing across diverse computing environments.\nSnakemake: Utilizing Python-based scripting, Snakemake allows for flexible and automated NGS data analysis pipelines, supporting parallel processing and integration with other tools.\n\nOnce your scientific computational workflow is ready to be shared, publish your scientific computational workflow on WorkflowHub.\n\n\n\nEach computer or HPC (High-Performance Computing) platform has a unique computational environment that includes its operating system, installed software, versions of software packages, and other features. If a research project is moved to a different computer or platform, the analysis might not run or produce consistent results if it depends on any of these factors.\nFor research to be reproducible, the original computational environment must be recorded so others can replicate it. There are several methods to achieve this:\n\nContainerization platforms (e.g., Docker, Singularity): allow the researcher to package their software and dependencies into a standardized container image.\nVirtual Machines (e.g., VirtualBox): can share an entire virtualized computing environment (OS, software and dependencies)\nEnvironment managers: provide an isolated environment with specific packages and dependencies that can be installed without affecting the system-wide configuration. These environments are particularly useful for managing conflicting dependencies and ensuring reproducibility. Configuration files can automate the setup of the computational environment:\n\nconda: allows users to export environment specifications (software and dependencies) to YAML files enabling easy recreation of the environment on another system\nPython virtualenv: is a tool for creating isolated environments to manage dependencies specific to a project\nrequirements.txt: may contain commands for installing packages (such as pip for Python packages or apt-get for system-level dependencies), configuring system settings, and setting environment variables. Package managers can be used to install, upgrade and manage packages.\nR‚Äôs renv: The ‚Äòrenv‚Äô package creates isolated environments in R.\n\nEnvironment descriptors\n\nsessionInfo() or devtools::session_info(): In R, these functions provide detailed information about the current session\nsessionInfo(), similarly, in Python. Libraries like NumPy and Pandas have show_versions() methods to display package versions.\n\n\nWhile environment managers are very easy to use and share across different systems, and are lightweight and efficient, offering fast start-up times, Docker containers provide a full env isolation (including the operating system) which ensures consistent behavior across different systems.\n\n\n\n\nTo maintain clarity and organization in the data analysis process, adopt best practices such as:\n\nData documentation: create a README.md file to provide an overview of the project and its structure, and metadata for understanding the context of your analysis.\nAnnotate your pipelines and comment your code (look for tutorials and templates such as this one from freeCodeCamp).\nUse coding style guides (code lay-out, whitespace in expressions, comments, naming conventions, annotations‚Ä¶) to maintain consistency.\nLabel files numerically to organize the entire data analysis process (scripts, notebooks, pipelines, etc.).\n\n00.preprocessing., 01.data_analysis_step1., etc.\n\nProvide environment files for reproducing the computational environment (such as ‚Äòrequirements.txt‚Äô for Python or ‚Äòenvironment.yml‚Äô for Conda). The simplest way is to document the dependencies by reporting the packages and their versions used to run your analysis.\nData versioning: use version control systems (e.g., Git) and upload your code to a code repository Lesson 5.\nIntegrated development environments (e.g., RStudio, PyCharm) offer tools and features for writing, testing, and debugging code\nUse git submodule for code and software that is reused in several projects\nLeverage curated pipelines such as the ones developed by the nf-core community, further ensuring adherence to community standards and guidelines.\nUse Software Heritage an archive for software source code are essential for long-term accessibility and reproducibility\nAdd a LICENSE file and perform regular updates: clarifying usage permissions and facilitating collaboration.\n\n\n\n\n\n\n\nPractical HPC pipes\n\n\n\nWe provide a hand-on workshop on computational environments and pipelines. Keep an eye on the upcoming events on the Sandbox website. If you‚Äôre interested in delving deeper, check out the HPC best practices module we‚Äôve developed here.",
    "crumbs": [
      "Course material",
      "Key practices",
      "6. Processing and analyzing biodata"
    ]
  },
  {
    "objectID": "practical_workflows.html#nextflow",
    "href": "practical_workflows.html#nextflow",
    "title": "FAIR computational pipelines and environments",
    "section": "Nextflow",
    "text": "Nextflow"
  },
  {
    "objectID": "develop/examples/proteomics_metadata.html",
    "href": "develop/examples/proteomics_metadata.html",
    "title": "Protemics metadata (In development)",
    "section": "",
    "text": "Protemics metadata (In development)\nCheck this link for more details on different metadata file formats and standarised practices.\n\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "Use cases",
      "Proteomics",
      "Protemics metadata (*In development*)"
    ]
  },
  {
    "objectID": "develop/practical_workshop.html#organize-and-structure-your-datasets-and-data-analysis",
    "href": "develop/practical_workshop.html#organize-and-structure-your-datasets-and-data-analysis",
    "title": "Practical material",
    "section": "1. Organize and structure your datasets and data analysis",
    "text": "1. Organize and structure your datasets and data analysis\nEstablishing a consistent file structure and naming conventions will help you efficiently manage your data. We will classify your data and data analyses into two distinct types of folders to ensure the data can be used and shared by many lab members while preventing modifications by any individual:\n\nData folders (assay or external databases and resources): They house the raw and processed datasets, alongside the pipeline/workflow used to generate the processed data, the provenance of the raw data, and quality control reports of the data. The data should be locked and set to read-only to prevent unintended modifications. This applies to experimental data generated in your lab as well as external resources. Provide an MD5 checksum file when you download them yourself to verify their integrity.\nProject folders: They contain all the essential files for a specific research project. Projects may use data from various resources or experiments, or build upon previous results from other projects. The data should not be copied or duplicated, instead, it should be linked directly from the source.\n\nData and data analysis are kept separate because a project may utilize one or more datasets to address a scientific question. Data can be reused in multiple projects over time, combined with other datasets for comparison, or used to build larger datasets. Additionally, data may be utilized by different researchers to answer various research questions.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nWhen organizing your data folders, separate assays from external resources and maintain a consistent structure. For example, organize genome references by species and further categorize them by versions. Make sure to include all relevant information, and refer to this lesson for additional tips on data organization.\nThis will help you to keep your data tidied up, especially if you are working in a big lab where assays may be used for different purposes and by different people!\n\n\n\n\n\n\nData folders\nWhether your lab generates its own experimental data, receives it from collaborators, or works with previously published datasets, the data folder should follow a similar structure to the one presented here. Create a separate folder for each dataset, including raw files and processed files alongside the corresponding documentation and pipeline that generated the processed data. Raw files should remain untouched, and you should consider locking modifications to the final results once data preprocessing is complete. This precaution helps prevent unwanted changes to the data. Each subfolder should be named in a way that is distinct, easily readable and clear at a glance. Check this lesson for tips on naming conventions.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nUse an acronym (1) that describes the type of NGS assay (RNAseq, ChIPseq, ATACseq) a keyword (2) that represents a unique element to that assay, and the date (3).\n&lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD\nFor example CHIP_Oct4_20230101 is a ChIPseq assay made on 1st January 2023 with the keyword Oct4, so it is easily identifiable by the eye.\n\n\n\n\n\nLet‚Äôs explore a potential folder structure and the types of files you might encounter within it.\n&lt;data_type&gt;_&lt;keyword&gt;_YYYYMMDD/\n‚îú‚îÄ‚îÄ README.md \n‚îú‚îÄ‚îÄ CHECKSUMS\n‚îú‚îÄ‚îÄ pipeline\n    ‚îú‚îÄ‚îÄ pipeline.md\n    ‚îú‚îÄ‚îÄ scripts/\n‚îú‚îÄ‚îÄ processed\n    ‚îú‚îÄ‚îÄ fastqc/\n    ‚îú‚îÄ‚îÄ multiqc/\n    ‚îú‚îÄ‚îÄ final_fastq/\n‚îî‚îÄ‚îÄ raw\n    ‚îú‚îÄ‚îÄ .fastq.gz \n    ‚îî‚îÄ‚îÄ samplesheet.csv\n\nREADME.md: This file contains a detailed description of the dataset commonly in markdown format. It should include the provenance of the raw data (such as samples, laboratory protocols used, the aim of the project, folder structure, naming conventions, etc.).\nmetadata.yml: This metadata file outlines different keys and essential information, usually presented in YAML format. For more details, refer to this lesson.\npipeline.md: This file provides an overview of the pipeline used to process raw data, as well as the commands to run the pipeline. The pipeline itself and all the required scripts should be collected in the same directory.\nprocessed: This folder contains the results from the preprocessing pipeline. The content vary depending on the specific pipeline used (create additional subdirectories as needed).\nraw: This folder holds the raw data.\n\n.fastq.gz: For example, in NGS assays, there should be ‚Äòfastq‚Äô files.\nsamplesheet.csv: This file holds essential metadata for the samples, including sample identification, experimental variables, batch information, and other metrics crucial for downstream analysis. It is important that this file is complete and current, as it is key to interpreting results. If you are considering running nf-core pipelines, this file will be required.\n\n\n\n\nProject folders\nOn the other hand, we have another type of folder called Projects which refers to data analyses that are specific to particular tasks, such as those involved in preparing a potential article. In this folder, you will create a subfolder for each project that you or your lab is working on. Each Project subfolder should include project-specific information, data analysis pipelines, notebooks, and scripts used for that particular project. Additionally, you should include an environment file with all the required software and dependencies needed for the project, including their versions. This helps ensure that the analyses can be easily replicated and shared with others.\nThe Project folder should be named in a way that is unique, easy to read, distinguishable, and clear at a glance. For example, you might name it based on the main author‚Äôs initials, the dataset being analyzed, the project name, a unique descriptive element related to the project, or the part of the project you are responsible for, along with the date:\n&lt;project&gt;_&lt;keyword&gt;_YYYYMMDD\n\n\n\n\n\n\nNaming examples\n\n\n\n\n\n\n\n\nRNASeq_Mouse_Brain_20230512: a project RNA sequencing data from a mouse brain experiment, created on May 12, 2023\nEHR_COVID19_Study_20230115: a project around electronic health records data for a COVID-19 study, created on January 15, 2023.\n\n\n\n\n\n\nNow, let‚Äôs explore an example of a folder structure and the types of files you might encounter within it.\n&lt;project&gt;_&lt;keyword&gt;_YYYYMMDD\n‚îú‚îÄ‚îÄ data\n‚îÇ  ‚îî‚îÄ‚îÄ &lt;ID&gt;_&lt;keyword&gt;_YYYYMMDD &lt;- symbolic link\n‚îú‚îÄ‚îÄ documents\n‚îÇ  ‚îî‚îÄ‚îÄ research_project_template.docx\n‚îú‚îÄ‚îÄ metadata.yml\n‚îú‚îÄ‚îÄ notebooks\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_processing.rmd\n‚îÇ  ‚îî‚îÄ‚îÄ 02_data_analysis.rmd\n‚îÇ  ‚îî‚îÄ‚îÄ 03_data_visualization.rmd\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ reports\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_processing.html\n‚îÇ  ‚îî‚îÄ‚îÄ 02_data_analysis.html\n‚îÇ  ‚îú‚îÄ‚îÄ 03_data_visualization.html\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ figures\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ tables\n‚îú‚îÄ‚îÄ requirements.txt // env.yaml\n‚îú‚îÄ‚îÄ results\n‚îÇ  ‚îú‚îÄ‚îÄ figures\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ 02_data_analysis/\n‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ heatmap_sampleCor_20230102.png\n‚îÇ  ‚îú‚îÄ‚îÄ tables\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ 02_data_analysis/\n‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ DEA_treat-control_LFC1_p01.tsv\n‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ SumStats_sampleCor_20230102.tsv\n‚îú‚îÄ‚îÄ pipeline\n‚îÇ  ‚îú‚îÄ‚îÄ rules // processes \n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ step1_data_processing.smk\n‚îÇ  ‚îî‚îÄ‚îÄ pipeline.md\n‚îú‚îÄ‚îÄ scratch\n‚îî‚îÄ‚îÄ scripts\n\ndata: This folder contains symlinks or shortcuts to the actual data files, ensuring that the original files remain unaltered.\ndocuments: This folder houses Word documents, slides, or PDFs associated with the project, including data and project explanations, research papers, and more. It also includes the Data Management Plan.\n\nresearch_project_template.docx. If you download our template you will find a is a pre-filled Data Management Plan based on the Horizon Europe guidelines named ‚ÄòNon-sensitive_NGS_research_project_template.docx‚Äô.\n\nmetadata.yml: metadata file describing various keys of the project or experiment (see this lesson).\nnotebooks: This folder stores Jupyter, R Markdown, or Quarto notebooks containing the data analysis. Figures and tables used for the reports are organized under subfolders named after the notebook that created them for provenance purposes.\nREADME.md: A detailed project description in markdown or plain-text format.\nreports: Notebooks rendered as HTML, docx, or PDF files for sharing with colleagues or as formal data analysis reports.\n\nfigures: figures produced upon rendering notebooks. The figures will be saved under a subfolder named after the notebook that created them. This is for provenance purposes so we know which notebook created which figures.\n\nrequirements.txt: This file lists the necessary software, libraries, and their versions required to reproduce the code. If you‚Äôre using conda environments, you will also find the env.yaml file here, which outlines the specific environment configuration.\nresults: This folder contains analysis results, such as figures and tables. Organizing results by the pipeline, script, or notebook that generated them will make it easier to locate and interpret the data.\npipeline: A folder containing pipeline scripts or workflows for processing and analyzing data.\nscratch: A folder designated for temporary files or workspace for experiments and development.\nscripts: Folder for helper scripts needed to run data analysis or reproduce the work.\n\n\n\nTemplate engine\nCreating a folder template is straightforward with cookiecutter a command-line tool that generates projects from templates (called cookiecutters). For example, it can help you set up a Python package project based on a Python package project template.\n\n\n\n\n\n\nCookiecutter templates\n\n\n\nHere are some template that you can use to get started, adapt and modify them to your own needs:\n\nPython package project\nSandbox bioinformatics project\nSandbox data project\nData science\nNGS data\n\nCreate your own template from scratch.\n\n\n\nQuick tutorial on cookiecutter\nBuilding a Cookiecutter template from scratch requires defining a folder structure, crafting a cookiecutter.json file, and outlining placeholders (keywords) that will be substituted when generating a new project. Here‚Äôs a step-by-step guide on how to proceed:\n\nStep 1: Create a Folder Template\nFirst, begin by creating a folder structure that aligns with your desired template design. For instance, let‚Äôs set up a simple Python project template:\nmy_template/\n|-- {{cookiecutter.project_name}}\n|   |-- main.py\n|-- tests\n|   |-- test_{{cookiecutter.project_name}}.py\n|-- README.md\nIn this example, {cookiecutter.project_name} is a placeholder that will be replaced with the actual project name when the template is used. This directory contains a python script (‚Äòmain.py‚Äô), a subdirectory (‚Äòtests‚Äô) with a second python script named after the project (‚Äòtest_{{cookiecutter.project_name}}.py‚Äô) and a ‚ÄòREADME.md‚Äô file.\n\n\nStep 2: Create cookiecutter.json\nIn the root of your template folder, create a file named cookiecutter.json. This file will define the variables (keywords) that users will be prompted to fill in. For our Python project template, it might look like this:\n{\n  \"project_name\": \"MyProject\",\n  \"author_name\": \"Your Name\",\n  \"description\": \"A short description of your project\"\n}\nWhen users generate a project based on your template, they will be prompted with these questions. The provided values (‚Äúresponses‚Äù) will be used to substitute the placeholders in your template files.\nBeyond substituting placeholders in file and directory names, Cookiecutter can automatically populate text file contents with information. This feature is useful for offering default configurations or code file templates. Let‚Äôs enhance our earlier example by incorporating a placeholder within a text file:\nFirst, modify the my_template/main.py file to include a placeholder inside its contents:\n\n\nmain.py\n\n# main.py\ndef hello():\n    print(\"Hello, {{cookiecutter.project_name}}!\")\n\nThe ‚Äò{{cookiecutter.project_name}}‚Äô placeholder is now included within the main.py file. When you execute Cookiecutter, it will automatically replace the placeholders in both file and directory names and within text file contents.\nAfter running Cookiecutter, your generated ‚Äòmain.py‚Äô file could appear as follows:\n# main.py, assuming \"MyProject\" was entered as the project_name\ndef hello():\n    print(\"Hello, MyProject!\") \n\n\nStep 3: Use Cookiecutter\nOnce your template is prepared, you can utilize Cookiecutter to create a project from it. Open a terminal and execute:\ncookiecutter path/to/your/template\nCookiecutter will prompt you to provide values for project_name, author_name, and description. Once you input these values, Cookiecutter will replace the placeholders in your template files with the entered values.\n\n\nStep 4: Review the Generated Project\nAfter the generation process is complete, navigate to the directory where Cookiecutter created the new project. You will find a project structure with the placeholders replaced by the values you provided.\n\n\n\n\n\n\nExercise 1: Create your own template.\n\n\n\n\n\n\n\nUse Cookiecutter to create custom templates for your folders. You can do it from scratch (see Exercise 1, part B) or opt for one of our pre-made templates available as a Github repository (recommended for this workshop). Feel free to tailor the template to your specific requirements‚Äîyou don‚Äôt have to follow our examples exactly.\nRequirements\nWe assume you have already gone through the requirements at the beginning of the practical lesson. This includes installing the necessary tools and setting up accounts as needed.\nProject\n\nGo to our Cookicutter template and click on the Fork button at the top-right corner of the repository page to create a copy of the repository on your own GitHub account or organization. \nOpen a terminal on your computer, copy the URL of your fork and clone the repository to your local machine (the URL should look something like https://github.com/your_username/cookiecutter-template):\ngit clone &lt;your URL to the template&gt;\nIf you have a GitHub Desktop, click Add and select ‚ÄúClone repository‚Äù from the options.\nOpen the repository and navigate through the different directories.\nModify the contents of the repository as needed to fit your project‚Äôs requirements. You can change files, add new ones, remove existing one or adjust the folder structure. For inspiration, review the data structure above under ‚ÄòProject folder‚Äô. Our Cookiecutter template is missing the ‚Äòreports‚Äô directory or the ‚Äòrequirements.txt‚Äô file. Consider creating them, along with a subdirectory named ‚Äòreports/figures‚Äô.\n‚îú‚îÄ‚îÄ results/\n‚îÇ   ‚îú‚îÄ‚îÄ figures/\n‚îú‚îÄ‚îÄ requirements.txt\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nHere‚Äôs an example of how to do it:\n# Open your terminal and navigate to your template directory. Then: \ncd \\{\\{\\ cookiecutter.project_name\\ \\}\\}/  \nmkdir reports \ntouch requirements.txt\n\n\n\n\n\n\nCommit and push changes when you are done with your modifications.\n\n\nStage the changes with git add.\nCommit the changes with a meaningful commit message git commit -m \"update cookicutter template\".\nPush the changes to your forked repository on Github git push origin main (or the appropriate branch name).\n\n\nTest your template by using cookiecutter &lt;URL to your GitHub repository \"cookicutter-template\"&gt;.\nFill up the variables and verify that the new structure (and folders) looks like you would expect. Have any new folders been added, or have some been removed?\n\n\n\n\n\n\n\n\n\n\n\n\nOptional Exercise 1, part B\n\n\n\n\n\n\n\nCreate a template from scratch using this tutorial scratch, it can be as basic as this one below or ‚ÄòData folder‚Äô:\nmy_template/\n|-- {{cookiecutter.project_name}}\n|   |-- main.py\n|-- tests\n|   |-- test_{{cookiecutter.project_name}}.py\n|-- README.md\n\nStep 1: Create a directory for the template.\nStep 2: Write a cookiecutter.json file with variables such as project_name and author.\nStep 3: Set up the folder structure by creating subdirectories and files as needed.\nStep 4: Incorporate cookiecutter variables in the names of files.\nStep 5: Use cookiecutter variables within scripts, such as printing a message that includes the project name."
  },
  {
    "objectID": "develop/practical_workshop.html#data-documentation",
    "href": "develop/practical_workshop.html#data-documentation",
    "title": "Practical material",
    "section": "2. Data documentation",
    "text": "2. Data documentation\nData documentation involves organizing, describing, and providing context for datasets and projects. While metadata concentrates on the data itself, README files provide a broader perspective on the overall project or resource.\n\nMetadata\n\n\n\n\n\n\nmetadata.yml\n\n\n\nChoose the format that best suits the project‚Äôs needs. In this workshop, we will focus on YAMl as it is highly used for configuration files (e.g., in conda or pipelines).\n\n\n\n\n\n\nFile formats\n\n\n\n\n\n\n\n\nXML (eXtensible Markup Language): uses custom tags to describe data and allows for a hierarchical structure.\nJSON (JavaScript Object Notation): lightweight and human-readable format that is easy to parse and generate.\nCSV (Comma-Separated Values) or TSV (tabulate-separate values): simple and widely supported for representing tabular formats. Easy to manipulate using software or programming languages. It is often use for sample metadata.\nYAML (YAML Ain‚Äôt Markup Language): human-readable data serialization format, commonly used as project configuration files.\n\nOthers such as RDF or HDF5.\n\n\n\n\n\nLink to the file format database.\n\n\nMetadata in biological datasets refers to the information that describes the data and provides context for how the data was collected, processed, and analyzed. Metadata is crucial for understanding, interpreting, and using biological datasets effectively. It also ensures that datasets are reusable, reproducible and understandable by other researchers. Some of the components may differ depending on the type of project, but there are general concepts that will always be shared across different projects:\n\nSample information and collection details\nBiological context (such experimental conditions if applicable)\nData description\nData processing steps applied to the raw data\nAnnotation and Ontology terms\nFile metadata (file type, file format, etc.)\nEthical and Legal Compliance (ownership, access, provenance)\n\n\n\n\n\n\n\nMetadata and controlled vocabularies\n\n\n\nTo maximize the usefulness of metadata, aim to use controlled vocabularies across all fields. Read more about data documentation and find ontology services examples in lesson 4. We encourage you to begin implementing them systematically on your own (under the ‚Äúsources‚Äù section, you will find some helpful links to guide you putting them in practice).\nIf you work with NGS data, check out this recommendations and examples of metadata for samples, projects and datasets.\n\n\n\n\nREADME file\n\n\n\n\n\n\nREADME.md\n\n\n\nChoose the format that best suits the project‚Äôs needs. In this workshop, we will focused on Markdown as it is the most used format due to its balance of simplicity and expressive formatting options.\n\n\n\n\n\n\nFile formats\n\n\n\n\n\n\n\n\nMarkdown (.md): commonly used because is easy to read and write and is compatible across platforms (e.g., GitHub, GitLab). Supports formatting like headings, lists, links, images, and code blocks.\nPlain Text (.txt): Simple and straightforward format without any rich formatting and great for basic instructions. Lack the ability of structure content effectively.\nReStructuredText (.rst): commonly used for python projects. Supports advanced formatting (takes, links, images and code blocks) .\n\nOthers such as HTML, YAML and Notebooks.\n\n\n\n\n\nLink to the file format database\n\n\nThe README.md file is a markdown file that provides a comprehensive description of the data within a folder. Its rich text format (including bold, italic, links, etc.) allows you to explain the contents of the folder, as well as the reasons and methods behind its creation or collection. The content will vary depending on what it described (data or assays, project, software‚Ä¶).\nHere is an example of a README file for a bioinformatics project:\n\n\n\n\n\n\nREADME\n\n\n\n\n\n# TITLE\nClear and descriptive.\n# OVERVIEW\nIntroduction to the project including its aims, and its significance. Describe the main purpose and the biological questions being addressed.\n\n\n\n\n\n\nExample text\n\n\n\n\n\n\n\nThis project aims to investigate gene expression patterns across various human tissues using Next Generation Sequencing (NGS) data. By analyzing the transcriptomes of different tissues, we seek to uncover tissue-specific gene expression profiles and identify potential markers associated with specific biological functions or diseases.\nUnderstanding tissue-specific gene expression is crucial for deciphering the molecular basis of health and disease. Identifying genes that are uniquely expressed in certain tissues can provide insights into tissue function, development, and potential therapeutic targets. This project contributes to our broader understanding of human biology and has implications for personalized medicine and disease research.\n\n\n\n\n\n# TABLE OF CONTENTS (optional but helpful for others to navigate to different sections)\n# INSTALLATION AND SETUP\nList all prerequisites, software, dependencies, and system requirements needed for others to reproduce the project. If available, you may link to a Docker image, Conda YAML file, or requirements.txt file.\n# USAGE\nInclude command-line examples for various functionalities or steps and path for running a pipeline, if applicable.\n# DATASETS\nDescribe the data,, including its sources, format, and how to access it. If the data has undergone preprocessing, provide a description of the processes applied or the pipeline used.\n\n\n\n\n\n\nExample text\n\n\n\n\n\n\n\nWe have used internal datasets with IDs: RNA_humanSkin_20201030, RNA_humanBrain_20210102, RNA_humanLung_20220304.\nIn addition, we utilized publicly available NGS datasets from the GTEx (Genotype-Tissue Expression) project, which provides comprehensive RNA-seq data across multiple human tissues. These datasets offer a wealth of information on gene expression levels and isoform variations across diverse tissues, making them ideal for our analysis.\n\n\n\n\n\n# RESULTS\nSummarize the results and key findings or outputs.\n\n\n\n\n\n\nExample text\n\n\n\n\n\n\n\nOur analysis revealed distinct gene expression patterns among different human tissues. We identified tissue-specific genes enriched in brain tissues, highlighting their potential roles in neurodevelopment and function. Additionally, we found a set of genes that exhibit consistent expression across a range of tissues, suggesting their fundamental importance in basic cellular processes.\nFurthermore, our differential expression analysis unveiled significant changes in gene expression between healthy and diseased tissues, shedding light on potential molecular factors underlying various diseases. Overall, this project underscores the power of NGS data in unraveling intricate gene expression networks and their implications for human health.\n\n\n\n\n\n# CONTRIBUTIONS AND CONTACT INFO\n# LICENSE\n\n\n\n\n\n\n\n\n\n\nExercise 2: modify the metadata.yml file in your Cookiecutter template\n\n\n\n\n\n\n\nIt is time now to customize your Cookiecutter templates and modify the metadata.yml files so that they fit your needs!\n\nConsider changing variables (add/remove) in the metadata.yml file from the cookicutter template.\nModify the cookiecutter.json file. You could add new variables or change the default key and/or values:\n\n\ncookiecutter.json\n\n{\n\"project_name\": \"myProject\",\n\"project_slug\": \"{{ cookiecutter.project_name.lower().replace(' ', '_').replace('-', '_') }}\",\n\"authors\": \"myName\",\n\"start_date\": \"{% now 'utc', '%Y%m%d' %}\",\n\"short_desc\": \"\",\n\"version\": \"0.1.0\"\n}\n\nThe metadata file will be filled accordingly.\nOptional: You can customize or remove this prompt message entirely, allowing you to tailor the text to your preferences for a unique experience each time you use the template.\n\n\ncookiecutter.json\n\n\"__prompts__\": {\n    \"project_name\": \"Project directory name [Example: project_short_description_202X]\",\n    \"author\": \"Author of the project\",\n    \"date\": \"Date of project creation, default is today's date\",\n    \"short_description\": \"Provide a detailed description of the project (context/content)\"\n},\n\nModify the metadata.yml file so that it includes the metadata recorded by the cookiecutter.json file. Hint below:\n\n\nmetadata.yml\n\nproject: {{ cookiecutter.project_name }}\nauthor: {{ cookiecutter.author }}\ndate: {{ cookiecutter.date }}\ndescription: {{ cookiecutter.short_description }}\n\nModify the README.md file so that it includes the short description recorded by the cookiecutter.json file and the metadata at the top of the markdown file (top between lines of dashed).\n\n\nREADME.md\n\n---\ntitle: {{ cookiecutter.project_name }}\ndate: \"{{ cookiecutter.date }}\"\nauthor: {{ cookiecutter.author }}\nversion: {{ cookiecutter.version }}\n---\n\nProject description\n----\n\n{{ cookiecutter.short_description }}\n\nCommit and push changes when you are done with your modifications\n\n\nStage the changes with git add\nCommit the changes with a meaningful commit message git commit -m \"update cookicutter template\"\nPush the changes to your forked repository on Github git push origin main (or the appropriate branch name)\n\n\nTest your template by using cookiecutter &lt;URL to your GitHub repository \"cookicutter-template\"&gt;\nFill up the variables and verify that the modified information looks like you would expect."
  },
  {
    "objectID": "develop/practical_workshop.html#overview",
    "href": "develop/practical_workshop.html#overview",
    "title": "Practical material",
    "section": "Overview",
    "text": "Overview\nIntroduction to the project including its aims, and its significance. Describe the main purpose and the biological questions being addressed\n\n\n\n\n\n\nExample text\n\n\n\n\n\n\n\nThis project aims to investigate gene expression patterns across various human tissues using Next Generation Sequencing (NGS) data. By analyzing the transcriptomes of different tissues, we seek to uncover tissue-specific gene expression profiles and identify potential markers associated with specific biological functions or diseases.\nUnderstanding tissue-specific gene expression is crucial for deciphering the molecular basis of health and disease. Identifying genes that are uniquely expressed in certain tissues can provide insights into tissue function, development, and potential therapeutic targets. This project contributes to our broader understanding of human biology and has implications for personalized medicine and disease research."
  },
  {
    "objectID": "develop/practical_workshop.html#table-of-contents-optional-but-helpful-for-others-to-navigate-to-different-sections",
    "href": "develop/practical_workshop.html#table-of-contents-optional-but-helpful-for-others-to-navigate-to-different-sections",
    "title": "Practical material",
    "section": "Table of Contents (optional but helpful for others to navigate to different sections)",
    "text": "Table of Contents (optional but helpful for others to navigate to different sections)"
  },
  {
    "objectID": "develop/practical_workshop.html#installation-and-setup",
    "href": "develop/practical_workshop.html#installation-and-setup",
    "title": "Practical material",
    "section": "Installation and setup",
    "text": "Installation and setup\nList all prerequisites, software, dependencies, and system requirements needed for others to reproduce the project. If available, you may link to a Docker image, Conda YAML file, or requirements.txt file."
  },
  {
    "objectID": "develop/practical_workshop.html#usage",
    "href": "develop/practical_workshop.html#usage",
    "title": "Practical material",
    "section": "Usage",
    "text": "Usage\nInclude command-line examples for various functionalities or steps and path for running a pipeline, if applicable."
  },
  {
    "objectID": "develop/practical_workshop.html#results",
    "href": "develop/practical_workshop.html#results",
    "title": "Practical material",
    "section": "Results",
    "text": "Results\nOur analysis revealed distinct gene expression patterns among different human tissues. We identified tissue-specific genes enriched in brain tissues, highlighting their potential roles in neurodevelopment and function. Additionally, we found a set of genes that exhibit consistent expression across a range of tissues, suggesting their fundamental importance in basic cellular processes.\nFurthermore, our differential expression analysis unveiled significant changes in gene expression between healthy and diseased tissues, shedding light on potential molecular factors underlying various diseases. Overall, this project underscores the power of NGS data in unraveling intricate gene expression networks and their implications for human health.\n\nFor more details, refer to our Jupyter Notebook for the complete analysis pipeline and code."
  },
  {
    "objectID": "develop/04_metadata.html#data-documentation",
    "href": "develop/04_metadata.html#data-documentation",
    "title": "4. Documentation for biodata",
    "section": "Data documentation",
    "text": "Data documentation\nEssential documentation comes in different forms and flavors, serving various purposes in research. Examples include protocols outlining experimental procedures, detailed lab journals recording experimental conditions and observations, codebooks explaining concepts, variables, and abbreviations used in the analysis, information about the structure and content of a dataset, software installation, and usage manual, code explanation within files or methodological information outlining data processing steps.\n From ontotext.com\nData documentation provides essential context and structure to (primary) data, enabling researchers to understand its significance and facilitate efficient data management. Some common elements found in metadata for bioinformatics data include:\n\nData collection information: source (e.g., organism, tissue or location), date (YYYY-MM-DD format) and time, collection methods employed or experimental conditions.\nData processing information: data content, data format, data cleaning and transformation such as filtering and normalizations techniques, and software and tools used.\nData description: variables and attributes, and data types (e.g., categorical, numerical, or textual).\nBiological context: experimental design, biological purpose and relevance and implications in the broader context.\nData ownership and access: authorship, licensing of the data and details on accessing and sharing.\nProvenance and tracking: version control information over time and citations, such as links to publications or studies that reference the data.\n\nData documentation also serves as a crucial guide in navigating the complex landscape of data, akin to a cheat sheet for piecing together the puzzle of information. Much like identifying puzzle pieces, metadata provides essential details about data origin, structure, and context, such as sample collection details, experimental procedures, and equipment used. Metadata enables data exploration, interpretation, and future accessibility, promoting effective management and facilitating data usability and reuse.\n\n\n\n\n\n\nBenefits of collecting proper documentation\n\n\n\n\nData Context and Interpretation: Aiding in understanding experimental conditions, sample origins, and processing methods, is crucial for accurate results interpretation.\nData Discovery and Access: Documentation enables easy locating and accessing of specific datasets by quickly identifying relevant data through sample identifiers, experimental parameters, and timestamps.\nReproducibility and Collaboration: Documentation facilitates experiment replication and validation by enabling colleagues to reproduce analyses, compare results, and collaborate effectively, enhancing the integrity of scientific findings.\nQuality Control and Validation: Documentation supports data quality assessment by tracking the origin and handling of NGS data, allowing the identification of errors or biases to validate analysis accuracy and reliability.\nLong-Term Data Preservation: Documentation ensures preservation over time, facilitating future understanding and utilization of archived datasets for continued scientific impact as research progresses.\n\n\n\n\nStreamlining Metadata Collection\nData and project directories should both include metadata and a README file. Metadata delivers descriptive information about a dataset or project, offering insights for interpreting, using, and sharing the data effectively. README files offer an overview and purpose of the project or dataset, providing instructions and guidance for setting up, running, and using the data or tools. While metadata concentrates on the data itself, README files provide a broader perspective on the overall project or resource.\n\n\n\n\n\n\nPractical tips\n\n\n\n\nImplement a logical structure with clear and descriptive file names.\nUse of controlled vocabularies and ontologies to ensure consistency and efficient data management and interpretation.\nUse a repository and a versioning system\nMake it Machine-readable, -actionable, and -interpretable.\nDevelop standards further within your research environment FAIRsharing standards.\nInclude all information for others to comprehend and effectively utilize the data.\n\n\n\n\n\nREADME.md\n\n\n\n\n\n\nFile formats\n\n\n\nLink to the file format database\n\nMarkdown (.md): commonly used because is easy to read and write and is compatible across platforms (e.g., GitHub, GitLab). Supports formatting like headings, lists, links, images, and code blocks.\nPlain Text (.txt): Simple and straightforward format without any rich formatting and great for basic instructions. Lack the ability of structure content effectively.\nReStructuredText (.rst): commonly used for python projects. Supports advanced formatting (takes, links, images and code blocks) .\n\nOthers such as HTML, YAML and Notebooks.\n\n\nThe README.md file, written in markdown format, provides a detailed description of the folder‚Äôs content. It includes information such as the purpose of the data, collection methods, and relevant details. The content might differ based on the purpose of the data.\n\n\n\n\n\n\nExercise 1: Identify README.md key components.\n\n\n\n\n\n\n\nSelect one of the examples below and reflect on how effectively the README communicates important information about the project. Please note that some of the links lead to README files describing databases, while others pertain to software and tools.\n\n1000 Genomes Project. You will find several readme files here.\n\nHomo Sapiens, fasta GRCh38\nIPD-IMGT/HLA Database\nDocker\nPython pandas\n\n\n\n\n\n\nStructure for bioinformatics projects.\n\nDescription and relevance the project\nObjectives and aims\nDatasets and software requirements\nInstruction for data interpretation\nSummary of results\nContributions\nAdditional comments or notes\n\n\n\nmetadata.yml\n\n\n\n\n\n\nFile formats\n\n\n\n\nXML (eXtensible Markup Language): uses custom tags to describe data and allows for a hierarchical structure.\nJSON (JavaScript Object Notation): lightweight and human-readable format that is easy to parse and generate.\nCSV (Comma-Separated Values) or TSV (tabulate-separate values): simple and widely supported for representing tabular formats. Easy to manipulate using software or programming languages. It is often use for sample metadata.\nYAML (YAML Ain‚Äôt Markup Language): human-readable data serialization format, commonly used as project configuration files.\n\nOthers such as RDF or HDF5.\n\n\nLink to the file format database.\nMetadata can be written in many file formats (commonly used: YAML, TXT, JSON, and CSV). We recommend YAML format, which is a text document that contains data formatted using a human-readable data format for data serialization. However, choose the format that best suits the project‚Äôs needs. The content will be specific to the type of project.\nmetadata:\n  project: \"Title\"\n  author: \"Name\"\n  date: \"YYYYMMDD\"\n  description: \"Project short description\"\n  version: \"1.0\"\n  analysis:\n    tool: \"software\"\n    version: \"1.1.1\"\nSome general metadata fields used across different disciplines:\n\nProject Title: A concise and informative name for the dataset.\nAuthor(s): The individual(s) or organization responsible for creating the dataset. Include ORCID for identification.\nDate Created: The date when the dataset was originally generated or compiled, in YYYY-MM-DD format.\nDate Modified: The date when the dataset was last updated or modified (YYYY-MM-DD).\nObject ID: The project or assay ID for tracking and reference purposes.\nDescription: A short narrative explaining the content, purpose, and context of the project.\nKeywords: Descriptive terms or phrases capturing the main topics and attributes.\nEthical and Legal Considerations: Information about ethical approvals, consent, and any legal restrictions.\nVersion: The version number or identifier, useful for tracking changes.\nRelated Publications: Links or references to scientific publications associated with the folder. Always add the DOI.\nFunding Source: Details about the funding agency or source that supported the research or data generation.\nLicense: The type of license or terms of use associated with the dataset/project.\nContact Information: Contact details for individuals who can provide further information about the dataset/project.\n\n\n\n\n\n\n\nTip\n\n\n\nThere is an exercise in the practical material to streamline the creation of metadata files using Cookiecutter, a template-based scaffolding tool.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nCreate a metadata file with the following description fields: name, date, description, version, authors, keywords, license. Fill it up at the start of the project, when you generate the file structure.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Documentation for biodata"
    ]
  },
  {
    "objectID": "develop/practical_workshop.html#create-a-catalog-of-your-data-folder",
    "href": "develop/practical_workshop.html#create-a-catalog-of-your-data-folder",
    "title": "Practical material",
    "section": "4. Create a catalog of your data folder",
    "text": "4. Create a catalog of your data folder\nThe next step is to collect all the datasets that you have created in the manner explained above. Since your folders all should contain the metadata.yml file in the same place with the same metadata, it should be very easy to iteratively go through all the folders and merge all the metadata.yml files into a one single table. he table can be easily viewed in your terminal or even with Microsoft Excel.\n\n\n\n\n\n\nExercise 4: create a metadata.tsv catalog\n\n\n\n\n\n\n\nWe will make a small script in R (or you can make one with Python) that recursively goes through all the folders inside an input path (like your Assays folder), fetches all the metadata.yml files, merges them and writes a TSV file as an output.\n\nCreate a folder called dataset and change directory cd dataset\nFork this repository: a Cookiecutter template designed for NGS datasets.While you are welcome to create your own template from scratch, we recommend using this one to save time.\nRun the cookiecutter cc-data-template command at least twice to create multiple datasets or projects. Use different values each time to simulate various scenarios (do this in the dataset directory that you have previously created).\nExecute the script below using R (or create your own script in Python). Adjust the folder_path variable so that it matches the path to the Assays folder. The resulting table will be saved in the same folder_path.\nOpen your database_YYYYMMDD.tsv table in a text editor from the command-line, or view it in Excel for better visualization.\n\n\nSolution A. From a TSV\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n# R version 4.3.2\n# RScript to read all yaml files in directory and save the metadata into a dataframe\nquiet &lt;- function(package_name) {\n  # Suppress warnings and messages while checking and installing the package\n  suppressMessages(suppressWarnings({\n    # Check if the package is available and load it\n    if (!requireNamespace(package_name, quietly = TRUE)) {\n      install.packages(package_name)\n    }\n    # Load the package\n    library(package_name, character.only = TRUE)\n  }))\n}\n\n# Check and install necessary libraries\nquiet(\"yaml\")\nquiet(\"dplyr\")\nquiet(\"lubridate\")\n\n\nread_yaml &lt;- function(file_path) {\n  # Read the YAML file and convert it to a data frame\n  df &lt;- yaml::yaml.load_file(file_path) %&gt;% as.data.frame(stringsAsFactors = FALSE)\n  \n  # Return the data frame\n  return(df)\n}\n\n# Function to recursively fetch metadata.yml files\nget_metadata &lt;- function(folder_path) {\n  file_list &lt;- list.files(path = folder_path, pattern = \"metadata\\\\.yml$\", recursive = TRUE, full.names = TRUE)\n\n  metadata_list &lt;- lapply(file_list, read_yaml)\n  \n  # Combine the list of data frames into a single data frame using dplyr::bind_rows()\n  combined_metadata &lt;- bind_rows(metadata_list)\n\n  return(combined_metadata)\n}\n\n# Specify the folder path\nfolder_path &lt;- \"./\" #/path/to/your/folder\n\n# Fetch metadata from the specified folder\ndf &lt;- get_metadata(folder_path)\n\n# Save the data frame as a TSV file\noutput_file &lt;- paste0(\"database_\", format(Sys.Date(), \"%Y%m%d\"), \".tsv\")\nwrite.table(df, file = output_file, sep = \"\\t\", quote = FALSE, row.names = FALSE)\n\n# Print confirmation message\ncat(\"Database saved as\", output_file, \"\\n\")\n\n\n\n\n\nExercise 4, option B: create a SQLite database \nAlternatively, create a SQLite database from a metadata. If you opt for this option in the exercise, you must still complete the first three steps outlined above. Read more from the RSQLite documentation.\n\nSolution B. SQLite database\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nprint(\"Assuming the libraries from Exercise 4 are already loaded and a dataframe has been generated from the YAML files...\")\n\n# check_and_install() form Exercise 4, and load the other packages. \nquiet(\"DBI\")\nquiet(\"RSQLite\")\n\n# Initialize a temporary in memory database and copy the data.frame into it\n\ndb_file_path &lt;- paste0(\"database_\", format(Sys.Date(), \"%Y%m%d\"), \".sqlite\")\ncon &lt;- dbConnect(RSQLite::SQLite(), db_file_path)\n\ndbWriteTable(con, \"metadata\", df,  overwrite=TRUE) #row.names = FALSE,append =\n\n# Print confirmation message\ncat(\"Database saved as\", db_file_path, \"\\n\")\n\n# Close the database connection\ndbDisconnect(con)\n\n\n\n\n\n\n\n\n\n\n\nShiny apps\nTo get the most out of your metadata file and the ones from other colleagues, you can combine them and explore them by creating an interactive catalog browser. You can create interactive web apps straight from R or Python. Whether you have generated a tabulated-file or a sqlite database, browse through the metadata using Shiny. Shiny apps are perfect for researchers because they enable you to create interactive visualizations and dashboards with dynamic data inputs and outputs without needing extensive web development knowledge. Shiny provides a variety of user interface components such as forms, tables, graphs, and maps to help you organize and present your data effectively. It also allows you to filter, sort, and segment data for deeper insights.\n\n\n\n\n\n\nTip\n\n\n\n\nFor R Enthusiasts\n\nExplore demos from the R Shiny community to kickstart your projects or for inspiration.\n\nFor python Enthusiasts\n\nShiny for Python provides live, interactive code throughout its entire tutorial. Additionally, it offers a great tool called Playground, where you can code and test your own app to explore how different features render.\n\n\n\n\n\n\n\n\nExercise 5: Skill Booster, build an interactive catalog browser\n\n\n\n\n\n\n\nBuild an interactive web app straight from R or Python. Below, you will find an example of an R shiny app. In either case, you will need to define a user interface (UI) and a server function. The UI specifies the layout and appearance of the app, including input controls and output displays. The server function contains the app‚Äôs logic, handling data manipulation, and responding to user interactions. Once you set up the UI and server, you can launch the app!\nHere‚Äôs the UI and server function structure for an R Shiny app:\n# Don't forget to load shiny and DT libraries!\n\n# Specify the layout\nui &lt;- fluidPage(\n    titlePanel(...)\n    # Define the appearance of the app\n    sidebarLayout(\n        sidebarPanel(...)\n        mainPanel(...)\n    )\n)\n\nserver &lt;- function(input, output, session) {\n    # Define a reactive expression for data based on user inputs\n    data &lt;- reactive({\n        req(input$dataInput)  # Ensure data input is available\n        # Load or manipulate data here\n    })\n\n    # Define an output table based on data\n    output$dataTable &lt;- renderTable({\n        data()  # Render the data as a table\n    })\n\n    # Observe a button click event and perform an action\n    observeEvent(input$actionButton, {\n        # Perform an action when the button is clicked\n    })\n\n    # Define cleanup tasks when the app stops\n    onStop(function() {\n        # Close connections or save state if necessary\n    })\n}\n# Run the app\nshinyApp(ui, server)\nIf you need more assistance, take a look at the code below (Hint).\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n# R version 4.3.2\nprint(\"Assuming the libraries from Exercise 4 are already loaded and a dataframe has been generated from the YAML files...\")\n\n# check_and_install() form Exercise 4. \nquiet(\"shiny\")\nquiet(\"DT\")\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"TSV File Viewer\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Choose a TSV file\", accept = c(\".tsv\")),\n      selectInput(\"filter_column\", \"Filter by Column:\", choices = c(\"n_samples\", \"technology\"), selected = \"technology\"),\n      textInput(\"filter_value\", \"Filter Value:\", value = \"\"),\n      # if only numbers, numericInput()\n      radioButtons(\"sort_order\", \"Sort Order:\", choices = c(\"Ascending\", \"Descending\"), selected = \"Ascending\")\n    ),\n    \n    mainPanel(\n      DTOutput(\"table\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output) {\n  \n  data &lt;- reactive({\n    req(input$file)\n    df &lt;- read.delim(input$file$datapath, sep = \"\\t\")\n    print(str(df))\n\n    # Filter the DataFrame based on user input\n    if (input$filter_column != \"\" && input$filter_value != \"\") {\n      # Check if the column is numeric, and filter for value\n      if (is.numeric(df[[input$filter_column]])) {\n        df &lt;- df[df[[input$filter_column]] &gt;= as.numeric(input$filter_value), ]\n      }\n      # Check if the column is a string\n      else if (is.character(df[[input$filter_column]])) {\n        df &lt;- df[df[[input$filter_column]] == input$filter_value, ]\n      }\n    }\n    \n    # Sort the DataFrame based on user input\n    sort_order &lt;- if (input$sort_order == \"Ascending\") TRUE else FALSE\n    df &lt;- df[order(df[[input$filter_column]], decreasing = !sort_order), ]\n    df\n  })\n  \n  output$table &lt;- renderDT({\n    datatable(data())\n  })\n}\n\n# Run the app\nshinyApp(ui, server)\n\n\n\n\n\nIn the optional exercise below, you‚Äôll find a code example for using an SQLite database as input instead of a tabulated file.\n\n\n\n\n\n\n\n\n\n\n\nExercise (optional)\n\n\n\n\n\n\n\nOnce you‚Äôve finished the previous exercise, consider implementing these additional ideas to maximize the utility of your catalog browser.\n\nUse SQLite databases as input\nAdd a functionality to only select certain columns uiOutput(\"column_select\")\nFilter columns by value using column_filter_select()\nAdd multiple tabs using tabsetPanel()\nAdd buttons to order numeric columns ascending or descending using radioButtons()\nUse SQL aggregation functions (e.g., SUM, COUNT, AVG) to perform custom data summaries and calculations.\nAdd a tab tabPanel() to create a project directory interactively (and fill up the metadata fields), tips: dir.create(), data.frame(), write.table()\nModify existing entries\nVisualize results using Cirrocumulus, an interactive visualization tool for large-scale single-cell genomics data.\n\nIf you need some assistance, take a look at the code below (Hint).\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nExplore an example with advanced features such as a two-tab layout, filtering by numeric values and matching strings, and a color-customized dashboard here."
  },
  {
    "objectID": "develop/practical_workshop.html#version-control-using-git-and-github",
    "href": "develop/practical_workshop.html#version-control-using-git-and-github",
    "title": "Practical material",
    "section": "5. Version control using Git and GitHub",
    "text": "5. Version control using Git and GitHub\nVersion control involves systematically tracking changes to a project over time, offering a structured way to document revisions and understand the progression of your work. In research data management and data analytics, it plays a critical role and provides numerous benefits.\nGit is a distributed version control system that helps developers and researchers efficiently manage project history, collaborate seamlessly, and maintain data integrity. On the other hand, GitHub is a web-based platform that builds on Git‚Äôs functionality by providing a centralized, collaborative hub for hosting Git repositories. It offers several key functionalities, such as tracking issues, security features to safeguard your repos, and GitHub Pages that allow you to create websites to showcase your projects.\n\n\n\n\n\n\nCreate a GitHub organization for your lab or department\n\n\n\nGitHub users can create organizations, allowing groups to collaborate or create repositories under the same organization umbrella. You can create an educational organization on Github for free, by setting up a Github account for your lab.\nFollow these instructions to create a GitHub organization.\nOnce you‚Äôve established your GitHub organization, be sure to create your repositories within the organization‚Äôs space rather than under your personal user account. This keeps your projects centralized and accessible to the entire group. Best practices for managing an organization on GitHub include setting clear access permissions, regularly reviewing roles and memberships, and organizing repositories effectively to keep your projects structured and easy to navigate.\n\n\n\nSetting up a GitHub repository for your project folder\nVersion controlling your data analysis folders becomes straightforward once you‚Äôve established your Cookiecutter templates. After you‚Äôve created several folder structures and metadata using your Cookiecutter template, you can manage version control by either converting those folders into Git repositories or copying a folder into an existing Git repository. Both approaches are explained in Lesson 5.\n\n\n\n\n\n\nExercise 6: initialize a repository from an existing folder:\n\n\n\n\n\n\n\n\nInitialize the repository: Begin by running the command git init in your project directory. This command sets up a new Git repository in the current directory and is executed only once, even for collaborative projects. See (git init) for more details.\nCreate a remote repository: Once the local repository is initialized, create an empty new repository on GitHub (website or Github Desktop).\nConnect the remote repository: Add the GitHub repository URL to your local repository using the command git remote add origin &lt;URL&gt;. This associates the remote repository with the name ‚Äúorigin.‚Äù\nCommit changes: If you have files you want to add to your repository, stage them using git add ., then create a commit to save a snapshot of your changes with git commit -m \"add local folder\".\nPush to GitHub: To synchronize your local repository with the remote repository and establish a tracking relationship, push your commits to the GitHub repository using git push -u origin main.\n\n\n\n\n\n\n\n\n\n\n\n\nTips to write good commit messages\n\n\n\nIf you would like to know more about Git commits and the best way to make clear Git messages, check out this post!\n\n\n\n\nGitHub Pages\nAfter creating your repository and hosting it on GitHub, you can now add your data analysis reports‚Äîsuch as Jupyter Notebooks, R Markdown files, or HTML reports‚Äîto a GitHub Page website. Setting up a GitHub Page is straightforward, and we recommend following GitHub‚Äôs helpful tutorial. However, we will go through the key steps in the exercise below. There are several ways to create your web pages, but we suggest using Quarto as a framework to build a sleek, professional-looking website with ease. The folder templates from the previous exercise already contain the necessary elements to launch a webpage. Familiarizing yourself with the basics of Quarto will help you design a webpage that suits your preferences. Other common options include MkDocs. If you want to use MkDocs instead, click here and follow the instructions.\n\n\n\n\n\n\nTip\n\n\n\nHere are some useful links to get started with Github Pages:\n\nGithub Pages\nQuarto Github Pages\n\n\n\n\n\n\n\n\n\nExercise 7: Create a Github Page using Quarto\n\n\n\n\n\n\n\n\nHead over to GitHub and create a new public repository named username.github.io, where username is your username (or organization name) on GitHub. If the first part of the repository doesn‚Äôt exactly match your username, it won‚Äôt work, so make sure to get it right.\nGo to the folder where you want to store your project, and clone the new repository: git clone https://github.com/username/username.github.io (or use Github Desktop)\nCreate a new file named _quarto.yml\n\n\n_quarto.yml\n\nproject:\n    type: website\n\nOpen the terminal ```{.bash filename=‚ÄúTerminal‚Äù} # Add a .nojekyll file to the root of the repository not to do additional processing of your published site touch .nojekyll #copy NUL .nojekyll for windows\n# Render and push it to Github quarto render git commit -m ‚ÄúPublish site to docs/‚Äù git push ```\nIf you do not have a gh-pages, you can create one as follows\n\n\nTerminal\n\ngit checkout --orphan gh-pages\ngit reset --hard # make sure all changes are committed before running this!\ngit commit --allow-empty -m \"Initialising gh-pages branch\"\ngit push origin gh-pages\n\nBefore attempting to publish you should ensure that the Source branch for your repository is gh-pages and that the site directory is set to the repository root (/)\n\nIt is important to not check your _site directory into version control, add the output directory of your project to .gitignore\n\n\n.gitignore\n\n/.quarto/\n/_site/\n\nNow is time to publish your website\n\n\n.Terminal\n\nquarto publish gh-pages\n\nOnce you‚Äôve completed a local publish, add a publish.yml GitHub Action to your project by creating this YAML file and saving it to .github/workflows/publish.yml. Read how to do it here"
  },
  {
    "objectID": "develop/examples/mkdocs_pages.html",
    "href": "develop/examples/mkdocs_pages.html",
    "title": "Build your GitHub Page using Mkdocs",
    "section": "",
    "text": "Build your GitHub Page using Mkdocs\n\n\n\n\n\n\nExercise 5: make a project folder and publish a data analysis webpage\n\n\n\n\n\n\n\n\nConfigure your main GitHub Page and its repo\nThe first step is to set up the main GitHub Page site and the repository that will host it. This is very simple, as you will only need to follow these steps. In a Markdown document, outline the primary objectives of the organization and provide an overview of ongoing research projects. After you have created the organization/usernamegithub.io, it is time to configure your Project repository webpage using MkDocs!\nStart a new project from Cookiecutter or use one from the previous exercise.\nIf you use a Project repo from the first exercise, go to the next paragraph. Using Cookiecutter, create a new data analysis project. Remember to fill up your metadata and description files! After you have created the folder, it would be best to initialize a Git repo following the instructions from the previous section.\nNext, link your data of interest (or create a small fake dataset) and make an example of a data analysis notebook/report (this could be just a scatter plot of a random matrix of values). Depending on your setup, you might be using Jupyter Notebooks or R Markdown files. The extensions that we have installed using pip allow you to directly add a Jupyter Notebook file to the mkdocs.yml navigation section. On the other hand, if you are using R Markdown files, you will have to knit your document into either an HTML page or a GitHub document.\nFor the purposes of this exercise, we have already included a basic index.md markdown file that can serve as the intro page of your repo, and a jupyter_example.ipynb with some code in it. You are welcome to modify them further to test them out!\nUse MkDocs to create your webpage\nWhen you are happy with your files and are ready to publish them, make sure to add, commit, and push the changes to the remote. Then, build up your webpage using MkDocs and the mkdocs gh-deploy command from the same directory where the mkdocs.yml file is. For example, if your mkdocs.yml for your Project folder is in /Users/JARH/Projects/project1_JARH_20231010/mkdocs.yml, do cd /Users/JARH/Projects/project1_JARH_20231010/ and then mkdocs gh-deploy. This requires a couple of changes in your GitHub organization settings.\nRemember to make sure that your markdowns, images, reports, etc., are included in the docs folder and properly set up in the navigation section of your mkdocs.yml file.\nFinally, we only need to set up the GitHub Project repo settings.\nPublishing your GitHub Page\nGo to your GitHub repo settings and configure the Page section. Since you are using the mkdocs gh-deploy command to publish your site in the gh-pages branch (as explained the the mkdocs documentation), we need to change where GitHub is fetching the website. You will need to configure the settings of this repository in GitHub so that the Page is taken from the gh-pages branch and the root folder.\n\n\n\nGitHub Pages setup\n\n\n\nBranch should be gh-pages\nFolder should be root\n\nAfter a couple of minutes, your webpage should be ready! You should be able to see your webpage through the link provided in the Page section!\n\nNow it is also possible to include this repository webpage in your main webpage &lt;organization&gt;.github.io by including the link of the repo website (https://&lt;organization&gt;.github.io/repo-name) in the navigation section of the mkdocs.yml file in the main organizationgithub.io repo.\n\n\n\n\n\n\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "Use cases",
      "General",
      "Build your GitHub Page using Mkdocs"
    ]
  },
  {
    "objectID": "practical_workflows.html#conda",
    "href": "practical_workflows.html#conda",
    "title": "FAIR computational pipelines and environments",
    "section": "Conda",
    "text": "Conda"
  },
  {
    "objectID": "practical_workflows.html#docker",
    "href": "practical_workflows.html#docker",
    "title": "FAIR computational pipelines and environments",
    "section": "Docker",
    "text": "Docker"
  },
  {
    "objectID": "develop/05_VC.html#best-practices-in-data-analysis",
    "href": "develop/05_VC.html#best-practices-in-data-analysis",
    "title": "5. Version Control with Git and GitHub",
    "section": "Best Practices in Data Analysis",
    "text": "Best Practices in Data Analysis\nThis lesson introduces version control with Git and Github and its significance in research. You will gain the ability to create Git repositories, and skills to build GitHub pages for showcasing data analysis.\nVersion control systematically tracks project changes, documenting alterations for understanding project evolution. It holds significant importance in research data management, software development, and data analysis, offering numerous advantages.\n\n\n\n\n\n\nAdvantages of using version control\n\n\n\n\nDocument Progress: Detailed change history aids understanding of project development and modifications.\nEnsure Data Integrity: Prevents accidental data loss or corruption, with each change tracked for easy recovery.\nFacilitate Collaboration: Enables seamless collaboration among team members, allowing multiple individuals to work concurrently without conflicts.\nReproducibility: Preserves project state for accurate validation and analysis.\nBranching and Experimentation: Allows the creation of alternative project versions for experimentation, without altering the main branch.\nGlobal Accessibility: Platforms like GitHub provide visibility for sharing, feedback, and contribution to open science.\n\n\n\n\n\n\n\n\n\nTake our course on Git & Github\n\n\n\nif you‚Äôre interested in delving deeper, explore our course on Git and GitHub.\nAlternatively, here are some examples and online resources to expand your understanding:\n\nGit and GitHub online resources\nGitHub documentation\nGit documentation\n\n\n\n\nVersion control using Git\nGit is a widely adopted version control system that empowers developers and researchers to efficiently manage their project‚Äôs history, collaborate seamlessly, track changes, and ensure data integrity. Git operates on core principles and mechanisms:\n\nLocal Repository: Each user maintains a local repository on their computer, storing the complete project history for independent work.\nSnapshots, Not Files: Git captures snapshots of the entire project at different points instead of tracking individual file changes, ensuring data consistency.\nCommits: Users create ‚Äòcommits‚Äô as snapshots of the project at specific moments, recording changes made to files along with explanatory commit messages.\nBranching: Git supports branching, enabling users to create separate lines of development for new features or bug fixes without affecting the main branch.\nMerging: Changes from one branch can be merged into another, facilitating the incorporation of new features or bug fixes back into the main project with a smooth merging process.\nDistributed Architecture: Git‚Äôs distributed nature means each user‚Äôs local repository is a complete copy of the project, enabling offline work and ensuring data redundancy.\nRemote Repositories: Users can connect and synchronize their local repositories with remote repositories hosted on platforms like GitHub, facilitating collaboration and project sharing.\nPush and Pull: Users ‚Äòpush‚Äô their local changes to a remote repository to share with others and ‚Äòpull‚Äô changes made by others into their local repository to stay updated.\nConflict Resolution: Git provides tools to resolve conflicts manually in cases of conflicting changes, ensuring data integrity during collaboration.\nVersioning and Tagging: Git offers versioning and tagging capabilities, allowing users to mark specific points in history such as major releases or significant milestones.\n\n\n\nGitHub Hosting for Git\nIn addition to exploring Git, we will also explore GitHub, a collaborative platform for hosting Git repositories. GitHub enhances Git‚Äôs capabilities by offering features like issue tracking, security measures to protect repositories, and GitHub Pages for creating project websites. Additionally, GitHub provides the option to set repositories as private until you are ready to share your work publicly.\n\n\n\n\n\n\nAlternatives flows for collaborative projects\n\n\n\n\nGitLab\nBitBucket\n\nWe will focus on GitHub for the remainder of this lesson due to its widespread usage and compatibility.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe will discuss repositories for archiving experimental or large datasets in lesson 7. However, if you are interested in version control large files, we recommend the use of git annex. It is important to store files with a checksum (MD5, SHA1, SHA256) to verify that files are not altered or corrupted buy recomputing their signature.\n\n\n\nFrom Project folders to Git repositories\nMoving from Git to GitHub involves transitioning from a local version control setup to a remote hosting platform. You will need a GitHub account for the exercise in this section.\n\n\n\n\n\n\nCreate a GitHub account\n\n\n\n\nIf you don‚Äôt have a GitHub account yet, click here\nInstall Git from Git webpage\n\n\n\nYou have two options when it comes to creating a repository for your project. First, you can start from scratch by creating a new repository and adding files to it as your project progresses. Alternatively, if you already have an existing folder structure for your project, you can initialize a repository directly from that folder. It is crucial to initiate version control in the early stages of a project to facilitate easy tracking of changes and effective management of the project‚Äôs version history from the beginning.\n\nConverting Folders to Git Repositories\nIf you completed all the exercises in lesson 3, you should have a project data structure prepared. Otherwise, consider using one of your existing projects or creating a small toy example for practice using cookiecutter (see practical_workshop).\n\n\n\n\n\n\nGithub documentation link\n\n\n\n\nAdding locally hosted code to Github\n\n\n\n\n\n\n\n\n\nExercise 1: initialize a repository from an existing folder:\n\n\n\n\n\n\n\n\nInitialize the repository: Begin by running the command git init in your project directory. This command sets up a new Git repository in the current directory and is executed only once, even for collaborative projects. See (git init) for more details.\nCreate a remote repository: Once the local repository is initialized, create am empty new repository on GitHub.\nConnect the remote repository: Add the GitHub repository URL to your local repository using the command git remote add origin &lt;URL&gt;. This associates the remote repository with the name ‚Äúorigin.‚Äù\nCommit changes: If you have files you want to add to your repository, stage them using git add ., then create a commit to save a snapshot of your changes with git commit -m \"add local folder\".\nPush to GitHub: To synchronize your local repository with the remote repository and establish a tracking relationship, push your commits to the GitHub repository using git push -u origin main.\n\n\n\n\n\n\n\n\nSetting Up a Git Repository and copying an existing folder\nAlternatively to converting folders to repositories, you can create a new repository remotely, and then clone (git clone) it locally. Here, git init is not needed. You can move the files into the repository locally (git add, git commit, and git push). If you are creating a collaborative repository, you can now share it with your colleagues.\n\n\n\n\n\n\nTips to write good commit messages\n\n\n\nWrite useful and clear Git commits. Check out this post for tips.\n\n\n\n\n\n\nGithub pages\nAfter setting up your repository on GitHub, take advantage of the opportunity to enhance it by adding your data analysis reports. Whether they are in Jupyter Notebooks, R Markdown files, or HTML reports, you can showcase them on a GitHub Page.\nOnce you have created your repository (and put it in GitHub), you have now the opportunity to add your data analysis reports that you created, in either Jupyter Notebooks, R Markdown files, or HTML reports, in a GitHub Page website. Creating a GitHub page is very simple, and we recommend that you follow the nice tutorial that GitHub has put for you.\nFor simplicity, we recommend using Quarto or MkDocs. Visit their websites and follow the instructions to get started.\n\n\n\n\n\n\nTutorial links\n\n\n\n\nGet started in quarto: https://quarto.org/docs/get-started/. We recommend using the VS code tool, if you do, follow this tutorial.\nMkDocs materials to further customize MkDocs websites.\n\n\n\n\n\nStep-by-Step Setup Guide\nWe provide an example of setting up Git, Quarto, and a GitHub account, enabling you to replicate the process independently! (see Exercise 5 in the practical material)",
    "crumbs": [
      "Course material",
      "Key practices",
      "5. Version Control with Git and GitHub"
    ]
  },
  {
    "objectID": "practical_workflows.html#fair-workflows",
    "href": "practical_workflows.html#fair-workflows",
    "title": "FAIR computational pipelines and environments",
    "section": "FAIR Workflows",
    "text": "FAIR Workflows\nData analysis typically involves the use of different tools, algorithms, and scripts. It often requires multiple steps to transform, filter, aggregate, and visualize data. The process can be time-consuming because each tool may demand specific inputs and parameter settings. As analyses become more complex, the importance of reproducible and scalable automated workflow management increases. Workflow management encompasses tasks such as parallelization, resumption, logging, and data provenance.\nIf you develop your own software make sure you follow FAIR principles. We highly endorse following these FAIR recommendations and to register your computational workflow here.\nUsing workflow managers, you ensure:\n\nautomation\nconvenience\nportability\nreproducibility\nscalability\nreadable\n\nPopular workflow management systems such as Snakemake, Nextflow, and Galaxy can be scaled effortlessly across server, cluster, and cloud environments without altering the workflow definition. They also allow for specifying the necessary software, ensuring the workflows can be deployed in any setting.\nDuring this lesson, you will learn about: - Syntax: understand the syntax of two workflow languages. - Defining steps: how to define a step in each of the language (rule in Snakemake, process in Nextflow), including specifying input, outputs and execution statements. - Generalizing steps: explore how to generalise steps and create a chain of dependency across multiple steps using wildcards (Snakemake) or parameters and channel operators (Nextflow). - Advanced Customisation: gain knowledge of advanced pipeline customisation using configuration files and custom-made functions - Scaling workflows: understand how to scale workflows to compute servers and clusters while adapting to hardware-specific constraints\n\nSnakemake\nIt is a text-based tool using python-based language plus domain specific syntax. The workflow is decompose into rules that are define to obtain output files from input files. It infers dependencies and the execution order.\n\nBasics\n\nSemantics: define rules\nGeneralise the rule: creating wildcards You can refer by index or by name\nDependencies are determined top-down\n\nFor a given target, a rule that can be applied to create it, is determined (a job) For the input files of the rule, go on recursively, If no target is specified, snakemake, tries to apply the first rule\n\nRule all: target rule that collects results\n\n\n\nJob execution\nA job is executed if and only if: - otuput file is target and does not exist - output file needed by another executed job and does not exist - input file newer than output file - input file will be updated by other job (eg. changes in rules) - execution is force (‚Äò‚Äìforce-all‚Äô)\nYou can plot the DAG (directed acyclic graph) of the jobs\n\n\nUseful command line interface\n# dry-run (-n), print shell commands (-p)\nsnakemake -n -p\n# Snakefile named different in another location \nsnakemake --snakefile path/to/file.smoker\n# dry-run (-n), print execution reason for each job\nsnakemake -n -r\n# Visualise DAG of jobs using Graphviz dot command\nsnakemake --dag | dot -Tsvg &gt; dag.svg\n\n\nDefining resources\nrule myrule:\n  resources: mem_mb= 100 #(100MB memory allocation)\n  threads: X\n  shell:\n    \"command {threads}\"\nLet‚Äôs say you defined our rule myrule needs 4 works, if we execute the workflow with 8 cores as follows:\nsnakemake --cores 8\nThis means that 2 ‚Äòmyrule‚Äô jobs, will be executed in parallel.\nThe jobs are schedules to maximize parallelization, high priority jobs will be scheduled first, all while satisfying resource constrains. This means:\nIf we allocate 100MB for the execution of ‚Äòmyrule‚Äô and we call snakemake as follows:\nsnakemake --resources mem_mb=100 --cores 8\nOnly one ‚Äòmyrule‚Äô job can be executed in parallel (you do not provide enough memory resources for 2). The memory resources is useful for jobs that are heavy memory demanding to avoid running out of memory. You will need to benchmark your pipeline to estimate how much memory and time your full workflow will take. We highly recommend doing so, get a subset of your dataset and give it a go! Log files will come very handy for the resource estimation. Of course, the execution of jobs is dependant on the free resources availability (eg. CPU cores).\nrule myrule:\n  log: \"logs/myrule.log\"\n  threads: X\n  shell:\n    \"command {threads}\"\nLog files need to define the same wildcards as the output files, otherwise, you will get an error.\n\n\nConfig files\nYou can also define values for wildcards or parameters in the config file. This is recommended when the pipeline might be used several times at different time points, to avoid unwanted modifications to the workflow. parameterization is key for such cases.\n\n\nCluster execution\nWhen working from cluster systems you can execute the workflow using -qsub submission command\nsnakemake --cluster qsub \n\n\nAdditional advanced features\n\nmodularization\nhandling temporary and protected files: very important for intermediate files that filled up our memory and are not used in the long run and can be deleted once the final output is generated. This is automatically done by snakemake if you defined them in your pipeline HTML5 reports\nrule parameters\ntracking tool versions and code changes: will force rerunning older jobs when code and software are modified/updated.\ndata provenance information per file\npython API for embedding snakemake in other tools\n\n\n\nCreate an isolated environment to install dependencies\nBasic file structure\n| - config.yml\n| - requirements.txt (commonly also named environment.txt)\n| - rules/\n|   | - myrules.smk\n| - scripts/\n|   | - script1.py\n| - Snakefile\nCreate conda environment, one per project!\n# create env\nconda create -n myworklow --file requirements.txt\n# activate environment\nsource activate myworkflow\n# then execute snakemake\nUse git repositories to save your projects and pipelines!\n\n\n\nNextflow"
  },
  {
    "objectID": "practical_workflows.html#fair-environments",
    "href": "practical_workflows.html#fair-environments",
    "title": "FAIR computational pipelines and environments",
    "section": "FAIR environments",
    "text": "FAIR environments\nRecording and sharing the computational environment is essential for reproducibility and transparency. There are several methods to achieve this, but we are going to focus on two of them, conda (an environment manager) and Docker (a container). Environment managers are user-friendly, easy to share across different systems, and offer lightweight, efficient, and fast start-up times. However, Docker containers provide complete environment isolation (including the operating system), which ensures consistent behavior across various systems.\n\nConda\n\n\nDocker"
  },
  {
    "objectID": "develop/cheatSheet.html",
    "href": "develop/cheatSheet.html",
    "title": "Cheat sheet",
    "section": "",
    "text": "Click on the image to enlarge it or use the download button to save it.\n\n\n\n\n Download me \n\n\n\nCopyrightCC-BY-SA 4.0 license"
  }
]