---
title: Practical material
format: html
date-modified: last-modified
date-format: long
date: 2023-11-30
summary: This is a workshop on RDM for biodata focused on practical aspects
hide: 
    - navigation
---

:::{.callout-note title="Course Overview"}

⏰ **Time Estimation:** X minutes  
💬 **Learning Objectives:**    

1. Organize and structure your data and data analysis with Cookiecutter templates
2. Define metadata fields and collect metadata when creating a Cookiecutter folder
3. Establish naming conventions for your data
4. Create a catalog of your data
5. Use GitHub repositories of your data analysis and display them as GitHub Pages
6. Archive GitHub repositories on Zenodo
:::

This practical version covers practical aspects of RDM applied to biodata. The exercises provided here aim to help you organize and structure your datasets and data analyses. You'll learn how to manage your experimental metadata effectively and safely version control and archive your data analyses using GitHub repositories and Zenodo. Through these guided exercises and step-by-step instructions, we hope you will acquire essential skills for managing and sharing your research data efficiently, thereby enhancing the reproducibility and impact of your work.

:::{.callout-warning title="Requirements"}
Ensure all necessary tools and software are installed before beginning the practical exercises:

- A [GitHub account](https://github.com/) for hosting and collaborating on projects
- [Git](https://git-scm.com/downloads) for version control of your projects
- A [Zenodo account](https://zenodo.org/) for archiving and sharing your research outputs
- [Python](https://www.python.org/)
- [pip](https://pip.pypa.io/en/stable/installation/) for managing Python packages
- [Cookicutter](https://cookiecutter.readthedocs.io/en/stable/) for creating folder structure templates (`pip install cookiecutter`)
- [cruft](https://cruft.github.io/cruft/) to version control your templates (`pip install cruft`)

Two more tools will be required, choose the one you are familiar with or the first option:

- Option a. Install [Quarto](https://quarto.org/docs/get-started/). We recommend Quarto as is easy to use and provides native support for notebooks (both R Markdown and Jupyter Notebooks). It requires no additional extensions or dependencies. 
- Option b. Install MkDocs and MkDocs extensions using the command line. Additional extensions are optional but can be useful if you choose this approach. 

```{.bash}
pip install mkdocs # create webpages
pip install mkdocs-material # customize webpages
pip install mkdocs-video # add videos or embed videos from other sources
pip install mkdocs-minify-plugin # Minimize html code
pip install mkdocs-git-revision-date-localized-plugin # display last updated date 
pip install mkdocs-jupyter # include Jupyter notebooks
pip install mkdocs-bibtex # add references in your text (`.bib`)
pip install neoteroi-mkdocs # create author cards
pip install mkdocs-table-reader-plugin # embed tabular format files (`.tsv`)
```
:::


## 1. Organize and structure your datasets and data analysis

Establishing a consistent file structure and naming conventions will help you efficiently manage your data. We will classify your data and data analyses into two distinct types of folders to ensure the data can be used and shared by many lab members while preventing modifications by any individual:

1. **Data folders** (assay or external databases and resources): They house the **raw and processed datasets**, alongside the pipeline/workflow used to generate the processed data, the provenance of the raw data, and quality control reports of the data. The data should be **locked and set to read-only** to prevent unintended modifications. This applies to experimental data generated in your lab as well as external resources. Provide an MD5 checksum file when you download them yourself to verify their integrity.
2. **Project folders**: They contain **all the essential files for a specific research project**. Projects may use data from various resources or experiments, or build upon previous results from other projects. The data should not be copied or duplicated, instead, it should be linked directly from the source.

Data and data analysis are kept separate because a project may utilize one or more datasets to address a scientific question. Data can be reused in multiple projects over time, combined with other datasets for comparison, or used to build larger datasets. Additionally, data may be utilized by different researchers to answer various research questions.

:::{.callout-hint}
When organizing your data folders, separate assays from external resources and maintain a consistent structure. For example, organize genome references by species and further categorize them by versions. Make sure to include all relevant information, and refer to [this lesson](./03_DOD.qmd#folder-organization) for additional tips on data organization.

This will help you to keep your data tidied up, especially if you are working in a big lab where assays may be used for different purposes and by different people!
:::

#### Data folders
Whether your lab generates its own experimental data, receives it from collaborators, or works with previously published datasets, the data folder should follow a similar structure to the one presented here. Create a separate folder for each dataset, including raw files and processed files alongside the corresponding documentation and pipeline that generated the processed data. Raw files should remain untouched, and you should consider locking modifications to the final results once data preprocessing is complete. This precaution helps prevent unwanted changes to the data. Each subfolder should be named in a way that is distinct, easily readable and clear at a glance. Check [this lesson](./03_DOD.qmd#naming-conventions) for tips on naming conventions. 

:::{.callout-hint}
Use an acronym (1) that describes the type of NGS assay (RNAseq, ChIPseq, ATACseq) a keyword (2) that represents a unique element to that assay, and the date (3). 
```{.bash}
<Assay-ID>_<keyword>_YYYYMMDD
```
For example `CHIP_Oct4_20230101` is a ChIPseq assay made on 1st January 2023 with the keyword Oct4, so it is easily identifiable by the eye.
:::

Let's explore a potential folder structure and the types of files you might encounter within it.

```{.bash}
<data_type>_<keyword>_YYYYMMDD/
├── README.md 
├── CHECKSUMS
├── pipeline
    ├── pipeline.md
    ├── scripts/
├── processed
    ├── fastqc/
    ├── multiqc/
    ├── final_fastq/
└── raw
    ├── .fastq.gz 
    └── samplesheet.csv
```

- **README.md**: This file contains a detailed description of the dataset commonly in markdown format. It should include the provenance of the raw data (such as samples, laboratory protocols used, the aim of the project, folder structure, naming conventions, etc.).
- **metadata.yml**: This metadata file outlines different keys and essential information, usually presented in YAML format. For more details, refer to [this lesson](./04_metadata.qmd).
- **pipeline.md**: This file provides an overview of the pipeline used to process raw data, as well as the commands to run the pipeline. The pipeline itself and all the required scripts should be collected in the same directory. 
- **processed**: This folder contains the results from the preprocessing pipeline. The content vary depending on the specific pipeline used (create additional subdirectories as needed).
- **raw**: This folder holds the raw data.
    - *.fastq.gz*: For example, in NGS assays, there should be 'fastq' files.
    - *samplesheet.csv*: This file holds essential metadata for the samples, including sample identification, experimental variables, batch information, and other metrics crucial for downstream analysis. It is important that this file is complete and current, as it is key to interpreting results. If you are considering running nf-core pipelines, this file will be required.

#### Project folders

On the other hand, we have another type of folder called Projects which refers to data analyses that are specific to particular tasks, such as those involved in preparing a potential article. In this folder, you will create a subfolder for each project that you or your lab is working on. Each Project subfolder should include project-specific information, data analysis pipelines, notebooks, and scripts used for that particular project. Additionally, you should include an environment file with all the required software and dependencies needed for the project, including their versions. This helps ensure that the analyses can be easily replicated and shared with others.

The Project folder should be named in a way that is unique, easy to read, distinguishable, and clear at a glance. For example, you might name it based on the main author's initials, the dataset being analyzed, the project name, a unique descriptive element related to the project, or the part of the project you are responsible for, along with the date:

```{.bash}
<project>_<keyword>_YYYYMMDD
```

:::{.callout-definition}
# Naming examples 

- `RNASeq_Mouse_Brain_20230512`: a project RNA sequencing data from a mouse brain experiment, created on May 12, 2023
- `EHR_COVID19_Study_20230115`: a project around electronic health records data for a COVID-19 study, created on January 15, 2023.
:::

Now, let's explore an example of a folder structure and the types of files you might encounter within it.

```{.bash}
<project>_<keyword>_YYYYMMDD
├── data
│  └── <ID>_<keyword>_YYYYMMDD <- symbolic link
├── documents
│  └── research_project_template.docx
├── metadata.yml
├── notebooks
│  └── 01_data_processing.rmd
│  └── 02_data_analysis.rmd
│  └── 03_data_visualization.rmd
├── README.md
├── reports
│  └── 01_data_processing.html
│  └── 02_data_analysis.html
│  ├── 03_data_visualization.html
│  │  └── figures
│  │  └── tables
├── requirements.txt // env.yaml
├── results
│  ├── figures
│  │  └── 02_data_analysis/
│  │    └── heatmap_sampleCor_20230102.png
│  ├── tables
│  │  └── 02_data_analysis/
│  │    └── DEA_treat-control_LFC1_p01.tsv
│  │    └── SumStats_sampleCor_20230102.tsv
├── pipeline
│  ├── rules // processes 
│  │  └── step1_data_processing.smk
│  └── pipeline.md
├── scratch
└── scripts
```

- **data**: This folder contains symlinks or shortcuts to the actual data files, ensuring that the original files remain unaltered.
- **documents**: This folder houses Word documents, slides, or PDFs associated with the project, including data and project explanations, research papers, and more. It also includes the [Data Management Plan](./02_DMP.qmd).
    - *research_project_template.docx*. If you download our template you will find a is a pre-filled Data Management Plan based on the Horizon Europe guidelines named 'Non-sensitive_NGS_research_project_template.docx'.
- **metadata.yml**: metadata file describing various keys of the project or experiment ([see this lesson](./04_metadata.qmd)).
- **notebooks**: This folder stores Jupyter, R Markdown, or Quarto notebooks containing the data analysis. Figures and tables used for the reports are organized under subfolders named after the notebook that created them for provenance purposes.
- **README.md**: A detailed project description in markdown or plain-text format.
- **reports**:  Notebooks rendered as HTML, docx, or PDF files for sharing with colleagues or as formal data analysis reports.
    - *figures*: figures produced upon rendering notebooks. The figures will be saved under a subfolder named after the notebook that created them. This is for provenance purposes so we know which notebook created which figures.
- **requirements.txt**: This file lists the necessary software, libraries, and their versions required to reproduce the code. If you're using conda environments, you will also find the env.yaml file here, which outlines the specific environment configuration.
- **results**: This folder contains analysis results, such as figures and tables. Organizing results by the pipeline, script, or notebook that generated them will make it easier to locate and interpret the data.
- **pipeline**: A folder containing pipeline scripts or workflows for processing and analyzing data.
- **scratch**: A folder designated for temporary files or workspace for experiments and development.
- **scripts**: Folder for helper scripts needed to run data analysis or reproduce the work.

### Template engine

Creating a folder template is straightforward with [cookiecutter](https://github.com/cookiecutter/cookiecutter) a command-line tool that generates projects from templates (called cookiecutters). For example, it can help you set up a Python package project based on a Python package project template. 

:::{.callout-note title="Cookiecutter templates"}
Here are some template that you can use to get started, adapt and modify them to your own needs:

- [Python package project](https://cookiecutter.readthedocs.io/en/stable/tutorials/tutorial1.html#step-1-generate-a-python-package-project)
- [Sandbox test](https://github.com/hds-sandbox/project-template/)
- [Data science](https://github.com/drivendata/cookiecutter-data-science/)
- [NGS data](https://github.com/brickmanlab/ngs-template)

Create your own template from [scratch](https://cookiecutter.readthedocs.io/en/stable/tutorials/tutorial2.html). 
:::

#### Quick tutorial on cookiecutter

Building a Cookiecutter template from scratch requires defining a folder structure, crafting a cookiecutter.json file, and outlining placeholders (keywords) that will be substituted when generating a new project. Here's a step-by-step guide on how to proceed:

##### Step 1: Create a Folder Template

First, begin by creating a folder structure that aligns with your desired template design. For instance, let's set up a simple Python project template:

```plaintext
my_template/
|-- {{cookiecutter.project_name}}
|   |-- main.py
|-- tests
|   |-- test_{{cookiecutter.project_name}}.py
|-- README.md
```

In this example, `{{cookiecutter.project_name}}` is a placeholder that will be replaced with the actual project name when the template is used. This directory contains a python script ('main.py'), a subdirectory ('tests') with a second python script named after the project ('test_{{cookiecutter.project_name}}.py') and a 'README.md' file. 

##### Step 2: Create `cookiecutter.json`

In the root of your template folder, create a file named `cookiecutter.json`. This file will define the variables (keywords) that users will be prompted to fill in. For our Python project template, it might look like this:

```{.json .code-overflow-wrap}
{
  "project_name": "MyProject",
  "author_name": "Your Name",
  "description": "A short description of your project"
}
```

When users generate a project based on your template, they will be prompted with these questions. The provided values ("responses") will be used to substitute the placeholders in your template files.

Beyond substituting placeholders in file and directory names, Cookiecutter can automatically populate text file contents with information. This feature is useful for offering default configurations or code file templates. Let's enhance our earlier example by incorporating a placeholder within a text file:

First, modify the `my_template/main.py` file to include a placeholder inside its contents:

```{.python .code-overflow-wrap}
# main.py

def hello():
    print("Hello, {{cookiecutter.project_name}}!")
```

The '{{cookiecutter.project_name}}' placeholder is now included within the main.py file. When you execute Cookiecutter, it will automatically replace the placeholders in both file and directory names and within text file contents. 

After running Cookiecutter, your generated 'main.py' file could appear as follows:

```{.python .code-overflow-wrap}
# main.py

def hello():
    print("Hello, MyProject!")  # Assuming "MyProject" was entered as the project_name
```

##### Step 3: Use Cookiecutter

Once your template is prepared, you can utilize Cookiecutter to create a project from it. Open a terminal and execute:

```{.bash .code-overflow-wrap}
cookiecutter path/to/your/template
```

Cookiecutter will prompt you to provide values for `project_name`, `author_name`, and `description`. Once you input these values, Cookiecutter will replace the placeholders in your template files with the entered values.

##### Step 4: Review the Generated Project

After the generation process is complete, navigate to the directory where Cookiecutter created the new project. You will find a project structure with the placeholders replaced by the values you provided. 

:::{.callout-exercise}

# Exercise 1: Create your own template

Use Cookiecutter to create custom templates for your folders. You can do it from scratch (see Exercise 1, part B) or opt for one of our pre-made templates available as a Github repository (recommended for this workshop). Feel free to tailor the template to your specific requirements—you don't have to follow our examples exactly.

**Requirements**

We assume you have already gone through the requirements at the beginning of the practical lesson. This includes installing the necessary tools and setting up accounts as needed.
    
**Project**
    
1. Go to our [Cookicutter template](https://github.com/hds-sandbox/cookiecutter-template) and click on the **Fork** button at the top-right corner of the repository page to create a copy of the repository on your own GitHub account or organization.
    ![fork_repo_example](./images/fork_repo_project.png)
2. Open a terminal on your computer, copy the URL of your fork and **clone** the repository to your local machine (the URL should look something like https://github.com/your_username/cookiecutter-template):

    ```{.bash}
    git clone <your URL to the template>
    ```
If you have a GitHub Desktop, click **Add** and select "Clone repository" from the options

3. Open the repository and navigate through the different directories

4. Modify the contents of the repository as needed to fit your project's requirements. You can change files, add new ones. remove existing one or adjust the folder structure. For inspiration, review the data structure above under 'Project folder'. For instance, this template is missing the 'reports' directory and add the 'requirements.txt' file. Consider creating it, along with a subdirectory named 'reports/figures'. 

    ```plaintext
    ├── results/
    │   ├── figures/
    ├── requirements.txt
    ```
    Here’s an example of how to do it: 

    ```{.bash}
    # Open your terminal and navigate to your template directory. Then: 
    cd \{\{\ cookiecutter.project_name\ \}\}/  
    mkdir reports 
    touch requirements.txt
    ```
5. Commit and push changes when you are done with your modifications
- Stage the changes with `git add`
- Commit the changes with a meaningful commit message `git commit -m "update cookicutter template" `
- Push the changes to your forked repository on Github `git push origin main` (or the appropriate branch name)
6. Test your template by using `cookiecutter <URL to your GitHub repository "cookicutter-template">`

    Fill up the variables and verify that the new structure (and folders) looks like you would expect. Have any new folders been added, or have some been removed?
:::

:::{.callout-exercise}
# Optional Exercise 1, part B

Create a template from scratch using this tutorial [scratch](https://cookiecutter.readthedocs.io/en/stable/tutorials/tutorial2.html), it can be as basic as this one below or 'Data folder': 

```plaintext
my_template/
|-- {{cookiecutter.project_name}}
|   |-- main.py
|-- tests
|   |-- test_{{cookiecutter.project_name}}.py
|-- README.md
```
- Step 1: Create a directory for the template.
- Step 2: Write a cookiecutter.json file with variables such as project_name and author.
- Step 3: Set up the folder structure by creating subdirectories and files as needed.
- Step 4: Incorporate cookiecutter variables in the names of files.
- Step 5: Use cookiecutter variables within scripts, such as printing a message that includes the project name.
:::

## 2. Data documentation  

Data documentation involves organizing, describing, and providing context for datasets and projects. While metadata concentrates on the data itself, README files provide a broader perspective on the overall project or resource.

### Metadata
:::{.callout-warning title="metadata.yml"}
Choose the format that best suits the project's needs. In this workshop, we will focus on YAMl as it is highly used for configuration files (e.g., in conda or pipelines).

:::{.callout-definition}
# File formats 
- XML (eXtensible Markup Language): uses custom tags to describe data and allows for a hierarchical structure.
- JSON (JavaScript Object Notation): lightweight and human-readable format that is easy to parse and generate.
- CSV (Comma-Separated Values) or TSV (tabulate-separate values): simple and widely supported for representing tabular formats. Easy to manipulate using software or programming languages. It is often use for sample metadata. 
- YAML (YAML Ain't Markup Language): human-readable data serialization format, commonly used as project configuration files. 

Others such as RDF or HDF5. 
:::
 Link to the [file format database](https://fileinfo.com/). 
   
:::

Metadata in biological datasets refers to the information that describes the data and provides context for how the data was collected, processed, and analyzed. Metadata is crucial for understanding, interpreting, and using biological datasets effectively. It also ensures that datasets are reusable, reproducible and understandable by other researchers. Some of the components may differ depending on the type of project, but there are general concepts that will always be shared across different projects:

- Sample information and collection details
- Biological context (such experimental conditions if applicable)
- Data description 
- Data processing steps applied to the raw data
- Annotation and Ontology terms
- File metadata (file type, file format, etc.)
- Ethical and Legal Compliance (ownership, access, provenance)

:::{.callout-warning title="Metadata and controlled vocabularies"}
To maximize the usefulness of metadata, aim to use controlled vocabularies across all fields. Read more about data documentation and find ontology services examples in [lesson 4](https://hds-sandbox.github.io/RDM_NGS_course/develop/04_metadata.html#controlled-vocabularies-and-ontologies). We encourage you to begin implementing them systematically on your own (under the "sources" section, you will find some helpful links to guide you putting them in practice). 

If you work with NGS data, check out [this](https://hds-sandbox.github.io/RDM_NGS_course/develop/examples/NGS_metadata.html) recommendations and examples of metadata for samples, projects and datasets. 
:::


### README file

:::{.callout-warning title="README.md"}
Choose the format that best suits the project's needs. In this workshop, we will focused on Markdown as it is the most used format due to its balance of simplicity and expressive formatting options. 

:::{.callout-definition}
# File formats 
- Markdown (`.md`): commonly used because is easy to read and write and is compatible across platforms (e.g.,  GitHub, GitLab). Supports formatting like headings, lists, links, images, and code blocks.
- Plain Text (`.txt`): Simple and straightforward format without any rich formatting and great for basic instructions. Lack the ability of structure content effectively. 
- ReStructuredText (`.rst`): commonly used for python projects. Supports advanced formatting (takes, links, images and code blocks) .

Others such as HTML, YAML and Notebooks. 
:::
Link to the [file format database](https://fileinfo.com/)

:::

The README.md file is a [markdown file](https://www.markdownguide.org/) that provides a comprehensive description of the data within a folder. Its rich text format (including bold, italic, links, etc.) allows you to explain the contents of the folder, as well as the reasons and methods behind its creation or collection. The content will vary depending on what it described (data or assays, project, software...). 

Here is an example of a README file for a bioinformatics project:

:::{.callout-readme}

\#  TITLE

Clear and descriptive.

\#  OVERVIEW

Introduction to the project including its aims, and its significance. Describe the main purpose and the biological questions being addressed.

:::{.callout-definition}
# Example text 
This project aims to investigate gene expression patterns across various human tissues using Next Generation Sequencing (NGS) data. By analyzing the transcriptomes of different tissues, we seek to uncover tissue-specific gene expression profiles and identify potential markers associated with specific biological functions or diseases.

Understanding tissue-specific gene expression is crucial for deciphering the molecular basis of health and disease. Identifying genes that are uniquely expressed in certain tissues can provide insights into tissue function, development, and potential therapeutic targets. This project contributes to our broader understanding of human biology and has implications for personalized medicine and disease research.
:::

\# TABLE OF CONTENTS (optional but helpful for others to navigate to different sections)

\# INSTALLATION AND SETUP

List all prerequisites, software, dependencies, and system requirements needed for others to reproduce the project. If available, you may link to a Docker image, Conda YAML file, or requirements.txt file.

\# USAGE

Include command-line examples for various functionalities or steps and path for running a pipeline, if applicable. 

\# DATASETS

Describe the data,, including its sources, format, and how to access it. If the data has undergone preprocessing, provide a description of the processes applied or the pipeline used.

:::{.callout-definition}
# Example text 
We have used internal datasets with IDs: RNA_humanSkin_20201030, RNA_humanBrain_20210102, RNA_humanLung_20220304.

In addition, we utilized publicly available NGS datasets from the GTEx (Genotype-Tissue Expression) project, which provides comprehensive RNA-seq data across multiple human tissues. These datasets offer a wealth of information on gene expression levels and isoform variations across diverse tissues, making them ideal for our analysis.
:::

\# RESULTS

Summarize the results and key findings or outputs. 

:::{.callout-definition}
# Example text 
Our analysis revealed distinct gene expression patterns among different human tissues. We identified tissue-specific genes enriched in brain tissues, highlighting their potential roles in neurodevelopment and function. Additionally, we found a set of genes that exhibit consistent expression across a range of tissues, suggesting their fundamental importance in basic cellular processes.

Furthermore, our differential expression analysis unveiled significant changes in gene expression between healthy and diseased tissues, shedding light on potential molecular factors underlying various diseases. Overall, this project underscores the power of NGS data in unraveling intricate gene expression networks and their implications for human health.
:::

\# CONTRIBUTIONS AND CONTACT INFO

\# LICENSE
:::


:::{.callout-exercise} 
# Exercise 2: modify the metadata.yml file in your Cookiecutter template
It is time now to customize your Cookiecutter templates and modify the metadata.yml files so that they fit your needs! 

0. Consider changing variables (add/remove) in the metadata.yml file from the cookicutter template. 
1. Modify the `cookiecutter.json` file. You could add new variables or change the default key and/or values: 

    ```{.json .code-overflow-wrap}
    {
    "project_name": "myProject",
    "project_slug": "{{ cookiecutter.project_name.lower().replace(' ', '_').replace('-', '_') }}",
    "authors": "myName",
    "start_date": "{% now 'utc', '%Y%m%d' %}",
    "short_desc": "",
    "version": "0.1.0"
    }
    ```
The metadata file will be filled accordingly.

2. Optional: You can customize or remove this prompt message entirely, allowing you to tailor the text to your preferences for a unique experience each time you use the template.

    ```{.json .code-overflow-wrap}
    "__prompts__": {
        "project_name": "Project directory name [Example: project_short_description_202X]",
        "author": "Author of the project",
        "date": "Date of project creation, default is today's date",
        "short_description": "Provide a detailed description of the project (context/content)"
    },
    ```

3. Modify the `metadata.yml` file so that it includes the metadata recorded by the `cookiecutter.json` file. Hint below: 

    ```{.json .code-overflow-wrap}
    project: {{ cookiecutter.project_name }}
    author: {{ cookiecutter.author }}
    date: {{ cookiecutter.date }}
    description: {{ cookiecutter.short_description }}
    ```
4. Modify the `README.md` file so that it includes the short description recorded by the `cookiecutter.json` file and the metadata at the top of the markdown file (top between lines of dashed).

    ```{.md .code-overflow-wrap}
    ---
    title: {{ cookiecutter.project_name }}
    date: "{{ cookiecutter.date }}"
    author: {{ cookiecutter.author }}
    version: {{ cookiecutter.version }}
    ---

    Project description
    ----

    {{ cookiecutter.short_description }}
    ```

5. Commit and push changes when you are done with your modifications
- Stage the changes with `git add`
- Commit the changes with a meaningful commit message `git commit -m "update cookicutter template" `
- Push the changes to your forked repository on Github `git push origin main` (or the appropriate branch name)
6. Test your template by using `cookiecutter <URL to your GitHub repository "cookicutter-template">`

    Fill up the variables and verify that the modified information looks like you would expect.  

:::

## 3. Naming conventions

As discussed in [lesson 3](https://hds-sandbox.github.io/RDM_NGS_course/develop/03_DOD.html#naming-conventions), consistent naming conventions are key for interpreting, comparing, and reproducing findings in scientific research. Standardized naming helps organize and retrieve data or results, allowing researchers to locate and compare similar types of data within or across large datasets. 


:::{.callout-exercise}
#  Exercise 3: Define your file name conventions
Avoid long and complicated names and ensure your file names are both informative and easy to manage: 

1. For saving a new plot, a heatmap representing sample correlations 
2. When naming the file for the document containing the Research Data Management Course Objectives (Version 2, 2nd May 2024) from the University of Copenhagen
3. Consider the most common file types you work with, such as visualizations, figures, tables, etc., and create logical and clear file names

:::{.callout-hint}
1. `heatmap_sampleCor_20240101.png`
2. `KU_RDM-objectives_20240502_v02.doc` or `KU_RDMObj_20240502_v02.doc`
:::
:::

## 4. Create a catalog of your data folder

The next step is to collect all the NGS datasets that you have created in the manner explained above. Since your folders all should contain the `metadata.yml` file in the same place with the same metadata, it should be very easy to iteratively go through all the folders and merge all the metadata.yml files into a one single table. This table can be then browsed easily with Microsoft Excel, for example. If you are interested in making a Shiny app or Python Panel tool to interactively browse the catalog, check out this [lesson](./04_metadata.qmd).

:::{.callout-exercise}
# Exercise 4: create a metadata.tsv catalog

We will make a small script in R (or you can make one with Python) that recursively goes through all the folders inside an input path (like your `Assays` folder), fetches all the `metadata.yml` files, and merges them. Finally, it will write a TSV file as an output. 

1. Create a folder called `dataset` and change directory `cd dataset`
2. Fork [this repository](https://github.com/hds-sandbox/cc-data-template): a Cookiecutter template designed for NGS datasets.
While you are welcome to create your own template from scratch, we recommend using this one to save time.
3. Run the `cookiecutter cc-data-template` command at least twice to create multiple datasets or projects. Use different values each time to simulate various scenarios (do this in the dataset directory that you have previously created). 
Execute the script below using R (or create your own script in Python). Adjust the `folder_path` variable so that it matches the path to the Assays folder. The resulting table will be saved in the same `folder_path`.
4. Open your `database_YYYYMMDD.tsv` table in a text editor from the command-line, or view it in Excel for better visualization.

```{.r  .code-overflow-wrap}

library(yaml)
library(dplyr)
library(lubridate)

# Function to read a YAML file and transform it into a dataframe format.
read_yaml <- function(file_path) {
  # Read the YAML file and convert it to a data frame
  df <- yaml::yaml.load_file(file_path) %>% as.data.frame(stringsAsFactors = FALSE)
  
  # Return the data frame
  return(df)
}

# Function to recursively fetch metadata.yml files
get_metadata <- function(folder_path) {
  file_list <- list.files(path = folder_path, pattern = "metadata\\.yml$", recursive = TRUE, full.names = TRUE)

  metadata_list <- lapply(file_list, read_yaml)
  
  # Combine the list of data frames into a single data frame using dplyr::bind_rows()
  combined_metadata <- bind_rows(metadata_list)

  return(combined_metadata)
}

# Specify the folder path
folder_path <- "/path/to/your/folder"

# Fetch metadata from the specified folder
metadata <- get_metadata(folder_path)

# Save the data frame as a TSV file
output_file <- paste0("database_", format(Sys.Date(), "%Y%m%d"), ".tsv")
write.table(metadata, file = output_file, sep = "\t", quote = FALSE, row.names = FALSE)

# Print confirmation message
cat("Database saved as", output_file, "\n")
```
:::

## 5. Version control of your data analysis using Git and GitHub

Version control is a systematic approach to tracking changes made to a project over time. It provides a structured means of documenting alterations, allowing you to revisit and understand the evolution of your work. In research data management and data analytics, version control is very important and gives you a lot of advantages.

[Git](https://git-scm.com/about) is a distributed version control system that enables developers and researchers to efficiently manage their project's history, collaborate seamlessly, and ensure data integrity. At its core, Git operates through the following principles and mechanisms:
On the other hand, [GitHub](https://github.com/) is a web-based platform that enhances Git's capabilities by providing a collaborative and centralized hub for hosting Git repositories. It offers several key functionalities, such as tracking issues, security features to safeguard your repos, and GitHub Pages that allow you to create websites to showcase your projects.

:::{.callout-tip title="Create a GitHub organization for your lab or department"}
GitHub allows users to create organizations and teams that will collaborate or create repositories under the same umbrella organization. If you would like to create an educational organization in GitHub, you can do so for free! For example, you could create a GitHub account for your lab.

To create a GitHub organization, follow these [instructions](https://docs.github.com/en/organizations/collaborating-with-groups-in-organizations/creating-a-new-organization-from-scratch)

After you have created the GitHub organization, make sure that you create your repositories under the organization space and not your user!
:::

### Creating a git repo online and copying your project folder

Version controlling your data analysis folders, a.k.a. `Project` folder, is very easy once you have set up your Cookiecutter templates. The simplest way of doing this is to first create a remote GitHub repository from the webpage (or from the Desktop app, if you are using it) with a proper project name. Then `git clone` that repository you just made into your `Projects` main folder. Then, use cookiecutter to create a project folder template and copy-paste the contents of the folder template to your cloned repo. Remember to fill up your metadata and description files! If you wish, you could already git add, commit, and push the first changes to the folders and continue from there on.

Go back to the course material [lesson 5](./05_VC.qmd) and read the differences between converting folders to git repositories and cloning a folder to an existing git repository.

:::{.callout-tip title="Tips to write good commit messages"}
If you would like to know more about Git commits and the best way to make clear Git messages, check out [this post](https://www.conventionalcommits.org/en/v1.0.0/)!
:::

### GitHub Pages

Once you have created your repository (and put it in GitHub), you have now the opportunity to add your data analysis reports that you created, in either Jupyter Notebooks, Rmarkdowns, or HTML reports, in a [GitHub Page website](https://pages.github.com/). Creating a GitHub page is very simple, and we really recommend that you follow the nice tutorial that GitHub has put for you. Nonetheless, we will see the main steps in the exercise below.

There are many different ways to create your web pages. We recommend using Mkdocs and Mkdocs materials as a framework to create a nice webpage simply. The folder templates that we used as an example in the previous exercise already contain everything you need to start a webpage. Nonetheless, you will need to understand the basics of [MkDocs](https://www.mkdocs.org/) and [MkDocs materials](https://squidfunk.github.io/mkdocs-material/) to design a webpage to your liking. MkDocs is a static webpage generator that is very easy to use, while MkDocs materials is an extension of the tool that gives you many more options to customize your website. Check out their web pages to get started!

:::{.callout-exercise}
# Exercise 5: make a project folder and publish a data analysis webpage

1. Configure your main GitHub Page and its repo

    The first step is to set up the main GitHub Page site and the repository that will host it. This is very simple, as you will only need to follow [these steps](https://pages.github.com/). In a Markdown document, outline the primary objectives of the organization and provide an overview of ongoing research projects.
    After you have created the *organization/username*github.io, it is time to configure your `Project` repository webpage using MkDocs!

2. Start a new project from Cookiecutter or use one from the previous exercise.

    If you use a `Project` repo from the first exercise, go to the next paragraph. Using Cookiecutter, create a new data analysis project. Remember to fill up your metadata and description files! After you have created the folder, it would be best to initialize a Git repo following the instructions from the [previous section](#creating-a-git-repo-online-and-copying-your-project-folder).

    Next, link your data of interest (or create a small fake dataset) and make an example of a data analysis notebook/report (this could be just a scatter plot of a random matrix of values). Depending on your setup, you might be using Jupyter Notebooks or Rmarkdowns. The extensions that we have installed using `pip` allow you to directly add a Jupyter Notebook file to the `mkdocs.yml` navigation section. On the other hand, if you are using Rmarkdown, you will have to knit your document into either an HTML page or a GitHub document.
    
    For the purposes of this exercise, we have already included a basic `index.md` markdown file that can serve as the intro page of your repo, and a `jupyter_example.ipynb` with some code in it. You are welcome to modify them further to test them out!

3. Use MkDocs to create your webpage

    When you are happy with your files and are ready to publish them, make sure to add, commit, and push the changes to the remote. Then, build up your webpage using MkDocs and the [`mkdocs gh-deploy`](https://www.mkdocs.org/user-guide/deploying-your-docs/) command from the same directory where the `mkdocs.yml` file is. For example, if your `mkdocs.yml` for your `Project` folder is in `/Users/JARH/Projects/project1_JARH_20231010/mkdocs.yml`, do `cd /Users/JARH/Projects/project1_JARH_20231010/` and then `mkdocs gh-deploy`.
    This requires a couple of changes in your GitHub organization settings.

    Remember to make sure that your markdowns, images, reports, etc., are included in the `docs` folder and properly set up in the navigation section of your `mkdocs.yml` file.

    Finally, we only need to set up the GitHub `Project` repo settings.

4. Publishing your GitHub Page
    
    Go to your GitHub repo settings and configure the Page section. Since you are using the `mkdocs gh-deploy` command to publish your site in the `gh-pages` branch (as explained the the mkdocs documentation), we need to change where GitHub is fetching the website. You will need to configure the settings of this repository in GitHub so that the Page is taken from the `gh-pages` branch and the `root` folder. 

    ![GitHub Pages setup](./images/git_pages.png)

    - Branch should be `gh-pages`
    - Folder should be `root`

    After a couple of minutes, your webpage should be ready! You should be able to see your webpage through the link provided in the Page section!

Now it is also possible to include this repository webpage in your main webpage *organization*github.io by including the link of the repo website (https://*organization*github.io/*repo-name*) in the navigation section of the `mkdocs.yml` file in the main *organization*github.io repo.
:::

## 6. Archive GitHub repositories on Zenodo

Archives are dedicated digital platforms designed for the secure storage, curation, and dissemination of scientific data. These repositories hold great importance in the research community as they serve as reliable archives for preserving valuable datasets. Their standardized formats and robust curation processes ensure the long-term accessibility and citability of research findings. Researchers worldwide rely on these repositories to share, discover, and validate scientific information, thereby fostering transparency, collaboration, and the advancement of knowledge across various domains of study.

The next practical exercise will be to archive your `Project` folder that contains the data analyses performed on your NGS data in a repository like Zenodo. We can do this by linking your Zenodo account to your GitHub account.

:::{.callout-warning title="Archiving your NGS data"}
In this practical lesson, we will only archive our data analyses in the `Project` folders. Your actual NGS data should be deposited in a domain-specific archive such as [Gene Expression Omnibus (GEO)](https://www.ncbi.nlm.nih.gov/geo/) or [Annotare](https://www.ebi.ac.uk/fg/annotare/login/). If you want to know more about these archives, check out this [lesson](./07_repos.qmd)
:::

### Zenodo

Zenodo[https://zenodo.org/] is an open-access digital repository designed to facilitate the archiving of scientific research outputs. It operates under the umbrella of the European Organization for Nuclear Research (CERN) and is supported by the European Commission. Zenodo accommodates a broad spectrum of research outputs, including datasets, papers, software, and multimedia files. This versatility makes it an invaluable resource for researchers across a wide array of domains, promoting transparency, collaboration, and the advancement of knowledge on a global scale.

Operating on a user-friendly web platform, Zenodo allows researchers to easily upload, share, and preserve their research data and related materials. Upon deposit, each item is assigned a unique Digital Object Identifier (DOI), granting it a citable status and ensuring its long-term accessibility. Additionally, Zenodo provides robust metadata capabilities, enabling researchers to enrich their submissions with detailed contextual information. In addition, it allows you to [link your GitHub account](https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content), providing a streamlined way to archive a specific release of your GitHub repository directly into Zenodo. This integration simplifies the process of preserving a snapshot of your project's progress for long-term accessibility and citation.

:::{.callout-exercise}
# Exercise 6: Archive a Project GitHub repo in Zenodo

1. In order to archive your GitHub repos in Zenodo, you will first need to [link your Zenodo and GitHub accounts](https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content).
2. Once your accounts are linked, go to your Zenodo GitHub account settings and turn on the GitHub repository you want to archive.
    ![zenodo_github_link](./images/zenodo_github.png)
3. Creating a Zenodo archive is now as simple as [making a release](https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository) in your GitHub repository. Remember to make a proper tag! **NOTE: If you make a release before enabling the GitHub repository in Zenodo, it will not appear in Zenodo!**
    ![github_release](./images/github_release.png)
4. Zenodo will automatically detect the release and it should appear on your Zenodo upload page.
    ![zenodo_archives](./images/zenodo_archives.png)
5. This archive is assigned a unique Digital Object Identifier (DOI), making it a citable reference for your work.
    ![zenodo_example](./images/zenodo_example.png)

Before submitting your work in a journal, make sure to link your data analysis repository to [Zenodo](https://zenodo.org/), get a DOI, and cite it in your manuscript!
:::

## Wrap up

In this small workshop, we have learned how to improve the FAIRability of your data, as well as organize and structure it in a way that will be much more useful in the future. These advantages do not serve yourself only, but your teammates, group leader, and the general scientific population! We hope that you found this workshop useful. If you would like to leave us some comments or suggestions, feel free to answer this form!

<!--
[**FEEDBACK FORM**](https://forms.office.com/e/BZkpzDKL0L)
-->
